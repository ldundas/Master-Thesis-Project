{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSc. Thesis - Clustering and shape analysis of high dimensional biomedical data.\n",
    "\n",
    "- **Author:** Dundas Lorenzo Agustin, s223288, MSc. Business Analytics, Technical University of Denmark.\n",
    "- **Co-Supervisor:** SebastiÃ¡n Basterrech, Postdoc, Department of Applied Mathematics and Computer Science, Technical University of Denmark.\n",
    "- **Supervisor:** Line Katrine Harder Clemmensen, Professor, Department of Applied Mathematics and Computer Science, Technical University of Denmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import struct\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import hdbscan\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, confusion_matrix, accuracy_score, davies_bouldin_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics import pairwise_distances as sklearn_pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import mode, linregress, norm\n",
    "from array import array\n",
    "from os.path import join\n",
    "from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding, MDS,trustworthiness\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from IPython.display import SVG, Image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from livelossplot.tf_keras import PlotLossesCallback\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Data Loader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Data Loader Class\n",
    "\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath, training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())\n",
    "        \n",
    "        # Convert labels to NumPy array\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        \n",
    "        return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Reading Dataset via MNISTDataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Reading Dataset via MnistDataloader class\n",
    "%matplotlib inline\n",
    "\n",
    "# Set file paths of MNIST Datasets\n",
    "input_path = 'C:/Users/Lorenzo/OneDrive/Documents/DTU/Python/2024 Fall/MSc Thesis'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "# Helper function to show a list of images with their relating titles\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n",
    "\n",
    "# Load MINST dataset\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "# Show some random training and test images \n",
    "images_2_show = []\n",
    "titles_2_show = []\n",
    "for i in range(0, 10):\n",
    "    r = random.randint(1, 60000)\n",
    "    images_2_show.append(x_train[r])\n",
    "    titles_2_show.append('training image [' + str(r) + '] = ' + str(y_train[r]))    \n",
    "\n",
    "for i in range(0, 5):\n",
    "    r = random.randint(1, 10000)\n",
    "    images_2_show.append(x_test[r])        \n",
    "    titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test[r]))    \n",
    "\n",
    "show_images(images_2_show, titles_2_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of images to a NumPy array\n",
    "x_train_array = np.array(x_train)\n",
    "x_test_array = np.array(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Option 1) Normalization done by Standarization (zero mean and unit variance). Recommended for DR techniques and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Flatten images if needed\n",
    "x_train_flattened1 = x_train_array.reshape(x_train_array.shape[0], -1)\n",
    "x_test_flattened1 = x_test_array.reshape(x_test_array.shape[0], -1)\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "x_train_standardized1 = scaler.fit_transform(x_train_flattened1)\n",
    "x_test_standardized1 = scaler.transform(x_test_flattened1)\n",
    "\n",
    "print(\"Data standardized: Mean =\", x_train_standardized1.mean(), \"Std Dev =\", x_train_standardized1.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Option 2) Normalizaiton done by Scailing reshaping the images scaling to [0,1]. However more in use for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the 28x28 images into 784-dimensional vectors\n",
    "x_train_flattened = np.array([img.flatten() for img in x_train_array])\n",
    "x_test_flattened = np.array([img.flatten() for img in x_test_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing by 255 scales the pixel intensity values to the [0, 1] range.\n",
    "# Hhelps improve performance and consistency in clustering and dimensionality reduction algorithms. \n",
    "# Making it a common practice in image-based data processing.\n",
    "\n",
    "x_train_normalized = x_train_flattened / 255.0\n",
    "x_test_normalized = x_test_flattened / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check normalization for x_train_normalized\n",
    "train_mean = x_train_normalized.mean(axis=0)  # Mean for each feature\n",
    "train_std = x_train_normalized.std(axis=0)    # Standard deviation for each feature\n",
    "\n",
    "# Check normalization for x_test_normalized\n",
    "test_mean = x_test_normalized.mean(axis=0)  # Mean for each feature\n",
    "test_std = x_test_normalized.std(axis=0)    # Standard deviation for each feature\n",
    "\n",
    "# Print results\n",
    "print(\"Train Data - Mean (per feature):\")\n",
    "print(train_mean)\n",
    "print(\"Train Data - Standard Deviation (per feature):\")\n",
    "print(train_std)\n",
    "\n",
    "print(\"\\nTest Data - Mean (per feature):\")\n",
    "print(test_mean)\n",
    "print(\"Test Data - Standard Deviation (per feature):\")\n",
    "print(test_std)\n",
    "\n",
    "# Verify if data is normalized\n",
    "if np.allclose(train_mean, 0, atol=1e-2) and np.allclose(train_std, 1, atol=1e-2):\n",
    "    print(\"\\nx_train_normalized is properly normalized (zero mean, unit variance).\")\n",
    "else:\n",
    "    print(\"\\nx_train_normalized is NOT properly normalized.\")\n",
    "\n",
    "if np.allclose(test_mean, 0, atol=1e-2) and np.allclose(test_std, 1, atol=1e-2):\n",
    "    print(\"x_test_normalized is properly normalized (zero mean, unit variance).\")\n",
    "else:\n",
    "    print(\"x_test_normalized is NOT properly normalized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and comparison of multiple algorithms - MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_components=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "x_train_pca_c2 = pca.fit_transform(x_train_standardized1)\n",
    "x_test_pca_c2 = pca.transform(x_test_standardized1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_pca_c2 = np.vstack([x_train_pca_c2, x_test_pca_c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_pca_c2 = kmeans.fit_predict(x_full_pca_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_pca_c2 = adjusted_rand_score(y_full, cluster_labels_pca_c2)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_pca_c2}\")\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette_pca_c2 = silhouette_score(x_full_pca_c2, cluster_labels_pca_c2)\n",
    "print(silhouette_pca_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracies_pca_c2 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_pca_c2, y_train)\n",
    "    knn_accuracy = knn.score(x_test_pca_c2, y_test)\n",
    "    knn_accuracies_pca_c2[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_pca_c2, y_train)\n",
    "y_test_pred_pca_c2= svm_clf.predict(x_test_pca_c2)\n",
    "svm_accuracy_pca_c2 = accuracy_score(y_test, y_test_pred_pca_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-Fold Cross-Validation Accuracy\n",
    "cv_scores_pca_c2 = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_pca_c2, y_train, cv=10)\n",
    "cv_accuracy_pca_c2 = cv_scores_pca_c2.mean()\n",
    "cv_std_pca_c2 = cv_scores_pca_c2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for PCA\n",
    "results_pca_c2 = {\n",
    "    'ARI': ari_pca_c2,\n",
    "    'Silhouette Score': silhouette_pca_c2,\n",
    "    'SVM Accuracy': svm_accuracy_pca_c2,\n",
    "    'k-NN Accuracy': knn_accuracies_pca_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_pca_c2, cv_std_pca_c2)\n",
    "}\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(results_pca_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_pca_c2.npy\", x_train_pca_c2)  # pca-reduced training data\n",
    "np.save(\"x_test_pca_c2.npy\", x_test_pca_c2)    # pca-reduced test data\n",
    "np.save(\"y_test_pred_pca_c2.npy\", y_test_pred_pca_c2)  # SVM predictions\n",
    "np.save(\"cv_scores_pca_c2.npy\", cv_scores_pca_c2)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracies_pca_c2.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracies_pca_c2, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_pca_c2_serializable = convert_to_serializable({\n",
    "    'ARI': ari_pca_c2,\n",
    "    'Silhouette Score': silhouette_pca_c2,\n",
    "    'SVM Accuracy': svm_accuracy_pca_c2,\n",
    "    'k-NN Accuracy': knn_accuracies_pca_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_pca_c2, cv_std_pca_c2)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"pca_c2_results.json\", \"w\") as file:\n",
    "    json.dump(results_pca_c2_serializable, file, indent=4)\n",
    "\n",
    "print(\"PCA results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2D projection with cluster labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=x_train_pca_c2[:, 0], y=x_train_pca_c2[:, 1], hue=y_train, palette='tab10', s=10, legend='full')\n",
    "plt.title(\"2D Scatter Plot of PCA-reduced Data\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA n_components=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=50)\n",
    "x_train_pca_c50 = pca.fit_transform(x_train_standardized1)\n",
    "x_test_pca_c50 = pca.transform(x_test_standardized1)\n",
    "\n",
    "np.save(\"x_train_pca_c50.npy\", x_train_pca_c50)\n",
    "np.save(\"x_test_pca_c50.npy\", x_test_pca_c50)\n",
    "\n",
    "print(f\"Original number of features: {x_train_standardized1.shape[1]}\")\n",
    "print(f\"Reduced number of features: {x_train_pca_c50.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_pca_c50 = np.vstack([x_train_pca_c50, x_test_pca_c50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_pca50 = kmeans.fit_predict(x_full_pca_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_pca_c50 = adjusted_rand_score(y_full, cluster_labels_pca50)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_pca_c50}\")\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette_pca_c50 = silhouette_score(x_full_pca_c50, cluster_labels_pca50)\n",
    "print(silhouette_pca_c50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracies_pca_c50 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_pca_c50, y_train)\n",
    "    knn_accuracy = knn.score(x_test_pca_c50, y_test)\n",
    "    knn_accuracies_pca_c50[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_pca_c50, y_train)\n",
    "y_test_pred_pca_c50= svm_clf.predict(x_test_pca_c50)\n",
    "svm_accuracy_pca_c50 = accuracy_score(y_test, y_test_pred_pca_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-Fold Cross-Validation Accuracy\n",
    "cv_scores_pca_c50 = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_pca_c50, y_train, cv=10)\n",
    "cv_accuracy_pca_c50 = cv_scores_pca_c50.mean()\n",
    "cv_std_pca_c50 = cv_scores_pca_c50.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for PCA\n",
    "results_pca_c50 = {\n",
    "    'ARI': ari_pca_c50,\n",
    "    'Silhouette Score': silhouette_pca_c50,\n",
    "    'SVM Accuracy': svm_accuracy_pca_c50,\n",
    "    'k-NN Accuracy': knn_accuracies_pca_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_pca_c50, cv_std_pca_c50)\n",
    "}\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(results_pca_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_pca_c50.npy\", x_train_pca_c50)  # pca-reduced training data\n",
    "np.save(\"x_test_pca_c50.npy\", x_test_pca_c50)    # pca-reduced test data\n",
    "np.save(\"y_test_pred_pca_c50.npy\", y_test_pred_pca_c50)  # SVM predictions\n",
    "np.save(\"cv_scores_pca_c50.npy\", cv_scores_pca_c50)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracies_pca_c50.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracies_pca_c50, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_pca_c50_serializable = convert_to_serializable({\n",
    "    'ARI': ari_pca_c50,\n",
    "    'Silhouette Score': silhouette_pca_c50,\n",
    "    'SVM Accuracy': svm_accuracy_pca_c50,\n",
    "    'k-NN Accuracy': knn_accuracies_pca_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_pca_c50, cv_std_pca_c50)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"pca_c50_results.json\", \"w\") as file:\n",
    "    json.dump(results_pca_c50_serializable, file, indent=4)\n",
    "\n",
    "print(\"PCA results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply PCA\n",
    "pca = PCA(0.95)\n",
    "x_train_pca_95 = pca.fit_transform(x_train_standardized1)\n",
    "x_test_pca_95 = pca.transform(x_test_standardized1)\n",
    "\n",
    "np.save(\"x_train_pca_95.npy\", x_train_pca_95)\n",
    "np.save(\"x_test_pca_95.npy\", x_test_pca_95)\n",
    "\n",
    "print(f\"Original number of features: {x_train_standardized1.shape[1]}\")\n",
    "print(f\"Reduced number of features: {x_train_pca_95.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pca_95= np.load(\"x_train_pca_95.npy\")\n",
    "x_test_pca_95= np.load(\"x_test_pca_95.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_pca95 = kmeans.fit_predict(x_test_pca_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_pca_95 = adjusted_rand_score(y_test, cluster_labels_pca95)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_pca_95}\")\n",
    "# ari_pca_95 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(x_train_pca_95, y_train).predict(x_test_pca_95))\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette_pca_95 = silhouette_score(x_test_pca_95, cluster_labels_pca95)\n",
    "print(silhouette_pca_95)\n",
    "# silhouette_pca_95 = silhouette_score(x_test_pca_95, KNeighborsClassifier(n_neighbors=1).fit(x_train_pca_95, y_train).predict(x_test_pca_95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_pca_95, y_train)\n",
    "y_test_pred_pca_95= svm_clf.predict(x_test_pca_95)\n",
    "svm_accuracy_pca = accuracy_score(y_test, y_test_pred_pca_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracies_pca = {}\n",
    "for k in [1, 5, 10]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_pca_95, y_train)\n",
    "    knn_accuracy = knn.score(x_test_pca_95, y_test)\n",
    "    knn_accuracies_pca[k] = knn_accuracy\n",
    "\n",
    "# 10-Fold Cross-Validation Accuracy\n",
    "cv_scores_pca = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_pca_95, y_train, cv=10)\n",
    "cv_accuracy_pca = cv_scores_pca.mean()\n",
    "cv_std_pca = cv_scores_pca.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for PCA\n",
    "results_pca = {\n",
    "    'ARI': ari_pca_95,\n",
    "    'Silhouette Score': silhouette_pca_95,\n",
    "    'SVM Accuracy': svm_accuracy_pca,\n",
    "    'k-NN Accuracy': knn_accuracies_pca,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_pca, cv_std_pca)\n",
    "}\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(results_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2D projection with cluster labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=x_train_pca_95[:, 0], y=x_train_pca_95[:, 1], hue=y_train, palette='tab10', s=10, legend='full')\n",
    "plt.title(\"2D Scatter Plot of PCA-reduced Data\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save intermediate data (PCA embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_pca_95.npy\", x_train_pca_95)\n",
    "np.save(\"x_test_pca_95.npy\", x_test_pca_95)\n",
    "np.save(\"y_test_pred_pca_95.npy\", y_test_pred_pca_95)  # Save SVM predictions\n",
    "np.save(\"cv_scores_pca.npy\", cv_scores_pca)      # Save cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies\n",
    "with open(\"knn_accuracies_pca.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracies_pca, file, indent=4)\n",
    "\n",
    "# Save PCA Results\n",
    "results_pca_95 = {\n",
    "    'ARI': ari_pca_95,\n",
    "    'Silhouette Score': silhouette_pca_95,\n",
    "    'SVM Accuracy': svm_accuracy_pca,\n",
    "    'k-NN Accuracy': knn_accuracies_pca,\n",
    "    '10-Fold CV Accuracy': {\n",
    "        'Mean': cv_accuracy_pca,\n",
    "        'StdDev': cv_std_pca\n",
    "    },\n",
    "    'Filepaths': {\n",
    "        'x_train_pca_95': \"x_train_pca_95.npy\",\n",
    "        'x_test_pca_95': \"x_test_pca_95.npy\",\n",
    "        'y_test_pred_pca_95': \"y_test_pred_pca_95.npy\",\n",
    "        'cv_scores_pca': \"cv_scores_pca.npy\",\n",
    "        'knn_accuracies_pca': \"knn_accuracies_pca.json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open(\"pca_95_results.json\", \"w\") as file:\n",
    "    json.dump(results_pca_95, file, indent=4)\n",
    "\n",
    "print(\"PCA 95% results and all intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE n_components=2 (not possible with 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and testing sets for t-SNE (unsupervised embedding)\n",
    "x_full = np.vstack([x_train_standardized1, x_test_standardized1])  # Combine normalized train and test data\n",
    "y_full = np.hstack([y_train, y_test])  # Combine train and test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=15)\n",
    "x_full_tsne = tsne.fit_transform(x_train_standardized1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_tsne= np.load('x_full_tsne.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the embeddings back into train and test sets\n",
    "x_train_tsne_c2 = x_full_tsne[:x_train_standardized1.shape[0], :]  # Train embeddings\n",
    "x_test_tsne_c2 = x_full_tsne[x_train_standardized1.shape[0]:, :]  # Test embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data (tsne embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_tsne_c2.npy\", x_train_tsne_c2)\n",
    "np.save(\"x_test_tsne_c2.npy\", x_test_tsne_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the TSNE-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_tsne_c2 = kmeans.fit_predict(x_full_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_tsne_c2 = adjusted_rand_score(y_full, cluster_labels_tsne_c2)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_tsne_c2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_tsne_c2 = silhouette_score(x_full_tsne, cluster_labels_tsne_c2)\n",
    "print(silhouette_tsne_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_tsne_c2 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_tsne_c2, y_train)\n",
    "    knn_accuracy = knn.score(x_test_tsne_c2, y_test)\n",
    "    knn_accuracy_tsne_c2[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on umap embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_tsne_c2, y_train)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm_test = svm_clf.predict(x_test_tsne_c2)  # Predict on test embeddings\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_tsne_c2 = accuracy_score(y_test, y_pred_svm_test)\n",
    "print(f\"SVM Accuracy (Test): {svm_accuracy_tsne_c2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_tsne_c2 = cross_val_score(svm_clf, x_train_tsne_c2, y_train, cv=10)\n",
    "cv_accuracy_tsne_c2 = cv_scores_tsne_c2.mean()\n",
    "cv_std_tsne_c2 = cv_scores_tsne_c2.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_tsne_c2:.4f} Â± {cv_std_tsne_c2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for umap\n",
    "results_tsne_c2 = {\n",
    "    'ARI': ari_tsne_c2,\n",
    "    'Silhouette Score': silhouette_tsne_c2,\n",
    "    'SVM Accuracy': svm_accuracy_tsne_c2,\n",
    "    'k-NN Accuracy': knn_accuracy_tsne_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_tsne_c2, cv_std_tsne_c2)\n",
    "}\n",
    "\n",
    "print(\"umap Results:\")\n",
    "print(results_tsne_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_tsne_c2.npy\", x_train_tsne_c2)  # umap-reduced training data\n",
    "np.save(\"x_test_tsne_c2.npy\", x_test_tsne_c2)    # umap-reduced test data\n",
    "np.save(\"y_test_pred_tsne_c2.npy\", y_pred_svm_test)  # SVM predictions\n",
    "np.save(\"cv_scores_tsne_c2.npy\", cv_scores_tsne_c2)      # Cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_tsne_c2.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_tsne_c2, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_tsne_c2_serializable = convert_to_serializable({\n",
    "    'ARI': ari_tsne_c2,\n",
    "    'Silhouette Score': silhouette_tsne_c2,\n",
    "    'SVM Accuracy': svm_accuracy_tsne_c2,\n",
    "    'k-NN Accuracy': knn_accuracy_tsne_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_tsne_c2, cv_std_tsne_c2)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"tsne_c50_results.json\", \"w\") as file:\n",
    "    json.dump(results_tsne_c2_serializable, file, indent=4)\n",
    "\n",
    "print(\"tsne results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISOMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISOMAP n_components=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_mnist_consistent(x_data, y_labels, sample_fraction=0.35):\n",
    "    \"\"\"\n",
    "    Downsample the dataset consistently, returning indices to ensure\n",
    "    the same points are selected in both spaces.\n",
    "    \"\"\"\n",
    "    sampled_indices = []\n",
    "    unique_labels = np.unique(y_labels)\n",
    "    for label in unique_labels:\n",
    "        # Select indices for the current label\n",
    "        label_indices = np.where(y_labels == label)[0]\n",
    "        # Sample a fraction of points for this label\n",
    "        sampled_indices_label = resample(\n",
    "            label_indices, n_samples=int(len(label_indices) * sample_fraction), replace=False, random_state=42\n",
    "        )\n",
    "        sampled_indices.extend(sampled_indices_label)\n",
    "    return np.array(sampled_indices)\n",
    "\n",
    "# Downsample training data\n",
    "sampled_indices_train = downsample_mnist_consistent(x_train_standardized1, y_train, sample_fraction=0.35)\n",
    "x_train_sampled = x_train_standardized1[sampled_indices_train]\n",
    "y_train_sampled = y_train[sampled_indices_train]\n",
    "\n",
    "# Downsample test data\n",
    "sampled_indices_test = downsample_mnist_consistent(x_test_standardized1, y_test, sample_fraction=0.35)\n",
    "x_test_sampled = x_test_standardized1[sampled_indices_test]\n",
    "y_test_sampled = y_test[sampled_indices_test]\n",
    "\n",
    "print(f\"Training set reduced to {len(x_train_sampled)} samples.\")\n",
    "print(f\"Test set reduced to {len(x_test_sampled)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sampled indices\n",
    "np.save(\"sampled_indices_train.npy\", sampled_indices_train)\n",
    "np.save(\"sampled_indices_test.npy\", sampled_indices_test)\n",
    "\n",
    "# Save the downsampled dataset\n",
    "np.save(\"x_train_sampled.npy\", x_train_sampled)\n",
    "np.save(\"y_train_sampled.npy\", y_train_sampled)\n",
    "np.save(\"x_test_sampled.npy\", x_test_sampled)\n",
    "np.save(\"y_test_sampled.npy\", y_test_sampled)\n",
    "\n",
    "print(\"Downsampling saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sampled indices\n",
    "sampled_indices_train= np.load(\"sampled_indices_train.npy\")\n",
    "sampled_indices_test= np.load(\"sampled_indices_test.npy\")\n",
    "\n",
    "# load the downsampled dataset\n",
    "x_train_sampled= np.load(\"x_train_sampled.npy\")\n",
    "y_train_sampled= np.load(\"y_train_sampled.npy\")\n",
    "x_test_sampled= np.load(\"x_test_sampled.npy\")\n",
    "y_test_sampled= np.load(\"y_test_sampled.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Isomap\n",
    "isomap = Isomap(n_components=2, n_neighbors=15)\n",
    "x_train_isomap_c2 = isomap.fit_transform(x_train_sampled)\n",
    "x_test_isomap_c2 = isomap.transform(x_test_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_train_isomap_c2.npy',x_train_isomap_c2)\n",
    "np.save('x_test_isomap_c2.npy',x_test_isomap_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_isomap_c2 = kmeans.fit_predict(x_train_isomap_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_isomap_c2 = adjusted_rand_score(y_train_sampled, cluster_labels_isomap_c2)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_isomap_c2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_isomap_c2 = silhouette_score(x_train_isomap_c2, cluster_labels_isomap_c2)\n",
    "print(silhouette_isomap_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_isomap_c2 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_isomap_c2, y_train_sampled)\n",
    "    knn_accuracy = knn.score(x_train_isomap_c2, y_train_sampled)\n",
    "    knn_accuracy_isomap_c2[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on Isomap embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_isomap_c2, y_train_sampled)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm = svm_clf.predict(x_train_isomap_c2)\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_isomap_c2 = accuracy_score(y_train_sampled, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {svm_accuracy_isomap_c2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_isomap_c2 = cross_val_score(svm_clf, x_train_isomap_c2, y_train_sampled, cv=10)\n",
    "cv_accuracy_isomap_c2 = cv_scores_isomap_c2.mean()\n",
    "cv_std_isomap_c2 = cv_scores_isomap_c2.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_isomap_c2:.4f} Â± {cv_std_isomap_c2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for Isomap\n",
    "results_isomap_c2 = {\n",
    "    'ARI': ari_isomap_c2,\n",
    "    'Silhouette Score': silhouette_isomap_c2,\n",
    "    'SVM Accuracy': svm_accuracy_isomap_c2,\n",
    "    'k-NN Accuracy': knn_accuracy_isomap_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_isomap_c2, cv_std_isomap_c2)\n",
    "}\n",
    "\n",
    "print(\"Isomap Results:\")\n",
    "print(results_isomap_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_isomap_c2.npy\", x_train_isomap_c2)  # ISOMAP-reduced training data\n",
    "np.save(\"x_test_isomap_c2.npy\", x_test_isomap_c2)    # ISOMAP-reduced test data\n",
    "np.save(\"y_test_pred_isomap_c2.npy\", y_pred_svm)  # SVM predictions\n",
    "np.save(\"cv_scores_isomap_c2.npy\", cv_scores_isomap_c2)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_isomap_c2.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_isomap_c2, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_isomap_c2_serializable = convert_to_serializable({\n",
    "    'ARI': ari_isomap_c2,\n",
    "    'Silhouette Score': silhouette_isomap_c2,\n",
    "    'SVM Accuracy': svm_accuracy_isomap_c2,\n",
    "    'k-NN Accuracy': knn_accuracy_isomap_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_isomap_c2, cv_std_isomap_c2)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"isomap_c2_results.json\", \"w\") as file:\n",
    "    json.dump(results_isomap_c2_serializable, file, indent=4)\n",
    "\n",
    "print(\"ISOMAP results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISOMAP n_component=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Isomap\n",
    "isomap = Isomap(n_components=50, n_neighbors=15)\n",
    "x_train_isomap_c50 = isomap.fit_transform(x_train_sampled)\n",
    "x_test_isomap_c50 = isomap.transform(x_test_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_isomap_c50 = kmeans.fit_predict(x_train_isomap_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_isomap_c50 = adjusted_rand_score(y_train_sampled, cluster_labels_isomap_c50)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_isomap_c50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_isomap_c50 = silhouette_score(x_train_isomap_c50, cluster_labels_isomap_c50)\n",
    "print(silhouette_isomap_c50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_isomap_c50 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_isomap_c50, y_train_sampled)\n",
    "    knn_accuracy = knn.score(x_train_isomap_c50, y_train_sampled)\n",
    "    knn_accuracy_isomap_c50[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on Isomap embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_isomap_c50, y_train_sampled)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm = svm_clf.predict(x_train_isomap_c50)\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_isomap_c50 = accuracy_score(y_train_sampled, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {svm_accuracy_isomap_c50:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_isomap_c50 = cross_val_score(svm_clf, x_train_isomap_c50, y_train_sampled, cv=10)\n",
    "cv_accuracy_isomap_c50 = cv_scores_isomap_c50.mean()\n",
    "cv_std_isomap_c50 = cv_scores_isomap_c50.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_isomap_c50:.4f} Â± {cv_std_isomap_c50:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for Isomap\n",
    "results_isomap_c50 = {\n",
    "    'ARI': ari_isomap_c50,\n",
    "    'Silhouette Score': silhouette_isomap_c50,\n",
    "    'SVM Accuracy': svm_accuracy_isomap_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_isomap_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_isomap_c50, cv_std_isomap_c50)\n",
    "}\n",
    "\n",
    "print(\"Isomap Results:\")\n",
    "print(results_isomap_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_isomap_c50.npy\", x_train_isomap_c50)  # ISOMAP-reduced training data\n",
    "np.save(\"x_test_isomap_c50.npy\", x_test_isomap_c50)    # ISOMAP-reduced test data\n",
    "np.save(\"y_test_pred_isomap_c50.npy\", y_pred_svm)  # SVM predictions\n",
    "np.save(\"cv_scores_isomap_c50.npy\", cv_scores_isomap_c50)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_isomap_c50.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_isomap_c50, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_isomap_c50_serializable = convert_to_serializable({\n",
    "    'ARI': ari_isomap_c50,\n",
    "    'Silhouette Score': silhouette_isomap_c50,\n",
    "    'SVM Accuracy': svm_accuracy_isomap_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_isomap_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_isomap_c50, cv_std_isomap_c50)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"isomap_c50_results.json\", \"w\") as file:\n",
    "    json.dump(results_isomap_c50_serializable, file, indent=4)\n",
    "\n",
    "print(\"ISOMAP results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLE n_components=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LLE\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=15, method='standard')\n",
    "x_train_lle_c2 = lle.fit_transform(x_train_sampled)\n",
    "x_test_lle_c2 = lle.transform(x_test_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the LLE-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_lle_c2 = kmeans.fit_predict(x_test_lle_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_lle_c2= np.load('x_test_lle_c2.npy')\n",
    "x_train_lle_c2= np.load('x_train_lle_c2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_lle_c2 = adjusted_rand_score(y_test_sampled, cluster_labels_lle_c2)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_lle_c2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_lle_c2 = silhouette_score(x_test_lle_c2, cluster_labels_lle_c2)\n",
    "print(silhouette_lle_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_lle_c2 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_lle_c2, y_train_sampled)\n",
    "    knn_accuracy = knn.score(x_train_lle_c2, y_train_sampled)\n",
    "    knn_accuracy_lle_c2[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on lle embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_lle_c2, y_train_sampled)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm = svm_clf.predict(x_train_lle_c2)\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_lle_c2 = accuracy_score(y_train_sampled, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {svm_accuracy_lle_c2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_lle_c2 = cross_val_score(svm_clf, x_train_lle_c2, y_train_sampled, cv=10)\n",
    "cv_accuracy_lle_c2 = cv_scores_lle_c2.mean()\n",
    "cv_std_lle_c2 = cv_scores_lle_c2.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_lle_c2:.4f} Â± {cv_std_lle_c2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for lle\n",
    "results_lle_c2 = {\n",
    "    'ARI': ari_lle_c2,\n",
    "    'Silhouette Score': silhouette_lle_c2,\n",
    "    'SVM Accuracy': svm_accuracy_lle_c2,\n",
    "    'k-NN Accuracy': knn_accuracy_lle_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_lle_c2, cv_std_lle_c2)\n",
    "}\n",
    "\n",
    "print(\"lle Results:\")\n",
    "print(results_lle_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_lle_c2.npy\", x_train_lle_c2)  # lle-reduced training data\n",
    "np.save(\"x_test_lle_c2.npy\", x_test_lle_c2)    # lle-reduced test data\n",
    "np.save(\"y_test_pred_lle_c2.npy\", y_pred_svm)  # SVM predictions\n",
    "np.save(\"cv_scores_lle_c2.npy\", cv_scores_lle_c2)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_lle_c2.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_lle_c2, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_lle_c2_serializable = convert_to_serializable({\n",
    "    'ARI': ari_lle_c2,\n",
    "    'Silhouette Score': silhouette_lle_c2,\n",
    "    'SVM Accuracy': svm_accuracy_lle_c2,\n",
    "    'k-NN Accuracy': knn_accuracy_lle_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_lle_c2, cv_std_lle_c2)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"lle_c2_results.json\", \"w\") as file:\n",
    "    json.dump(results_lle_c2_serializable, file, indent=4)\n",
    "\n",
    "print(\"lle results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLE n_component=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LLE\n",
    "lle = LocallyLinearEmbedding(n_components=50, n_neighbors=15, method='standard')\n",
    "x_train_lle_c50 = lle.fit_transform(x_train_sampled)\n",
    "x_test_lle_c50 = lle.transform(x_test_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_lle_c50= np.load('x_train_lle_c50.npy')\n",
    "x_test_lle_c50= np.load('x_test_lle_c50.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_lle_c50 = np.vstack([x_train_lle_c50, x_test_lle_c50])\n",
    "y_full_lle_c50 = np.hstack([y_train_sampled, y_test_sampled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clsutering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_lle_c50 = kmeans.fit_predict(x_full_lle_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_lle_c50 = adjusted_rand_score(y_full_lle_c50, cluster_labels_lle_c50)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_lle_c50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_lle_c50 = silhouette_score(x_full_lle_c50, cluster_labels_lle_c50)\n",
    "print(silhouette_lle_c50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_lle_c50 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_lle_c50, y_train_sampled)\n",
    "    knn_accuracy = knn.score(x_train_lle_c50, y_train_sampled)\n",
    "    knn_accuracy_lle_c50[k] = knn_accuracy\n",
    "\n",
    "# {1: 1.0, 5: 0.9598475827577995, 10: 0.9524172422005239}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on lle embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_lle_c50, y_train_sampled)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm = svm_clf.predict(x_train_lle_c50)\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_lle_c50 = accuracy_score(y_train_sampled, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {svm_accuracy_lle_c50:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_lle_c50 = cross_val_score(svm_clf, x_train_lle_c50, y_train_sampled, cv=10)\n",
    "cv_accuracy_lle_c50 = cv_scores_lle_c50.mean()\n",
    "cv_std_lle_c50 = cv_scores_lle_c50.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_lle_c50:.4f} Â± {cv_std_lle_c50:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for lle\n",
    "results_lle_c50 = {\n",
    "    'ARI': ari_lle_c50,\n",
    "    'Silhouette Score': silhouette_lle_c50,\n",
    "    'SVM Accuracy': svm_accuracy_lle_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_lle_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_lle_c50, cv_std_lle_c50)\n",
    "}\n",
    "\n",
    "print(\"lle Results:\")\n",
    "print(results_lle_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_lle_c50.npy\", x_train_lle_c50)  # lle-reduced training data\n",
    "np.save(\"x_test_lle_c50.npy\", x_test_lle_c50)    # lle-reduced test data\n",
    "np.save(\"y_test_pred_lle_c50.npy\", y_pred_svm)  # SVM predictions\n",
    "np.save(\"cv_scores_lle_c50.npy\", cv_scores_lle_c50)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_lle_c50.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_lle_c50, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_lle_c50_serializable = convert_to_serializable({\n",
    "    'ARI': ari_lle_c50,\n",
    "    'Silhouette Score': silhouette_lle_c50,\n",
    "    'SVM Accuracy': svm_accuracy_lle_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_lle_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_lle_c50, cv_std_lle_c50)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"lle_c50_results.json\", \"w\") as file:\n",
    "    json.dump(results_lle_c50_serializable, file, indent=4)\n",
    "\n",
    "print(\"lle results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_components=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP\n",
    "umap = UMAP(n_components=2, n_neighbors=15, random_state=42)\n",
    "x_train_umap_c2_std = umap.fit_transform(x_train_standardized1)\n",
    "x_test_umap_c2_std = umap.transform(x_test_standardized1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data (umap embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_umap_c2_std.npy\", x_train_umap_c2_std)\n",
    "np.save(\"x_test_umap_c2_std.npy\", x_test_umap_c2_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_umap_c2_std = np.vstack([x_train_umap_c2_std, x_test_umap_c2_std])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_umap_c2_std = kmeans.fit_predict(x_full_umap_c2_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_umap_c2_std = adjusted_rand_score(y_full, cluster_labels_umap_c2_std)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_umap_c2_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_umap_c2_std = silhouette_score(x_full_umap_c2_std, cluster_labels_umap_c2_std)\n",
    "print(silhouette_umap_c2_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_umap_c2_std = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_umap_c2_std, y_train)\n",
    "    knn_accuracy = knn.score(x_test_umap_c2_std, y_test)\n",
    "    knn_accuracy_umap_c2_std[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on umap embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_umap_c2_std, y_train)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm_test = svm_clf.predict(x_test_umap_c2_std)  # Predict on test embeddings\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_umap_c2_std = accuracy_score(y_test, y_pred_svm_test)\n",
    "print(f\"SVM Accuracy (Test): {svm_accuracy_umap_c2_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_umap_c2_std = cross_val_score(svm_clf, x_train_umap_c2_std, y_train, cv=10)\n",
    "cv_accuracy_umap_c2_std = cv_scores_umap_c2_std.mean()\n",
    "cv_std_umap_c2_std = cv_scores_umap_c2_std.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_umap_c2_std:.4f} Â± {cv_std_umap_c2_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for umap\n",
    "results_umap_c2_std = {\n",
    "    'ARI': ari_umap_c2_std,\n",
    "    'Silhouette Score': silhouette_umap_c2_std,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c2_std,\n",
    "    'k-NN Accuracy': knn_accuracy_umap_c2_std,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c2_std, cv_std_umap_c2_std)\n",
    "}\n",
    "\n",
    "print(\"umap Results:\")\n",
    "print(results_umap_c2_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_components=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "from umap import UMAP\n",
    "\n",
    "# Apply UMAP\n",
    "umap = UMAP(n_components=50, n_neighbors=15, random_state=42)\n",
    "x_train_umap_c50 = umap.fit_transform(x_train_standardized1)\n",
    "x_test_umap_c50 = umap.transform(x_test_standardized1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data (umap embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_umap_c50.npy\", x_train_umap_c50)\n",
    "np.save(\"x_test_umap_c50.npy\", x_test_umap_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_umap_c50 = np.vstack([x_train_umap_c50, x_test_umap_c50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full = np.hstack([y_train, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_umap_c50 = kmeans.fit_predict(x_full_umap_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_umap_c50 = adjusted_rand_score(y_full, cluster_labels_umap_c50)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_umap_c50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_umap_c50 = silhouette_score(x_full_umap_c50, cluster_labels_umap_c50)\n",
    "print(silhouette_umap_c50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_umap_c50 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_umap_c50, y_train)\n",
    "    knn_accuracy = knn.score(x_test_umap_c50, y_test)\n",
    "    knn_accuracy_umap_c50[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on umap embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_umap_c50, y_train)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm_test = svm_clf.predict(x_test_umap_c50)  # Predict on test embeddings\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_umap_c50 = accuracy_score(y_test, y_pred_svm_test)\n",
    "print(f\"SVM Accuracy (Test): {svm_accuracy_umap_c50:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_umap_c50= np.load('cv_scores_umap_c50.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_umap_c50 = cross_val_score(svm_clf, x_train_umap_c50, y_train, cv=10)\n",
    "cv_accuracy_umap_c50 = cv_scores_umap_c50.mean()\n",
    "cv_std_umap_c50 = cv_scores_umap_c50.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_umap_c50:.4f} Â± {cv_std_umap_c50:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for umap\n",
    "results_umap_c50 = {\n",
    "    'ARI': ari_umap_c50,\n",
    "    'Silhouette Score': silhouette_umap_c50,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_umap_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c50, cv_std_umap_c50)\n",
    "}\n",
    "\n",
    "print(\"umap Results:\")\n",
    "print(results_umap_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_umap_c50.npy\", x_train_umap_c50)  # umap-reduced training data\n",
    "np.save(\"x_test_umap_c50.npy\", x_test_umap_c50)    # umap-reduced test data\n",
    "np.save(\"y_test_pred_umap_c50.npy\", y_pred_svm)  # SVM predictions\n",
    "np.save(\"cv_scores_umap_c50.npy\", cv_scores_umap_c50)      # Cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intermediate data\n",
    "x_train_umap_c50= np.load(\"x_train_umap_c50.npy\") \n",
    "x_test_umap_c50= np.load(\"x_test_umap_c50.npy\") \n",
    "y_pred_svm= np.load(\"y_test_pred_umap_c50.npy\") \n",
    "cv_scores_umap_c50= np.load(\"cv_scores_umap_c50.npy\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_umap_c50.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_umap_c50, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_umap_c50_serializable = convert_to_serializable({\n",
    "    'ARI': ari_umap_c50,\n",
    "    'Silhouette Score': silhouette_umap_c50,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_umap_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c50, cv_std_umap_c50)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"umap_c50_results.json\", \"w\") as file:\n",
    "    json.dump(results_umap_c50_serializable, file, indent=4)\n",
    "\n",
    "print(\"umap results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from JSON file\n",
    "with open(\"umap_c50_results.json\", \"r\") as file:\n",
    "    results_umap_c50 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_umap_c50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_components=50 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "from umap import UMAP\n",
    "\n",
    "# Apply UMAP\n",
    "umap = UMAP(n_components=50, n_neighbors=15, random_state=42)\n",
    "x_train_umap_c50_norm = umap.fit_transform(x_train_normalized)\n",
    "x_test_umap_c50_norm = umap.transform(x_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data (umap embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_umap_c50_norm.npy\", x_train_umap_c50_norm)\n",
    "np.save(\"x_test_umap_c50_norm.npy\", x_test_umap_c50_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_umap_c50_norm = np.vstack([x_train_umap_c50_norm, x_test_umap_c50_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full = np.hstack([y_train, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_umap_c50_norm = kmeans.fit_predict(x_full_umap_c50_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_umap_c50_norm = adjusted_rand_score(y_full, cluster_labels_umap_c50_norm)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_umap_c50_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_umap_c50_norm = silhouette_score(x_full_umap_c50_norm, cluster_labels_umap_c50_norm)\n",
    "print(silhouette_umap_c50_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_umap_c50_norm = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_umap_c50_norm, y_train)\n",
    "    knn_accuracy = knn.score(x_test_umap_c50_norm, y_test)\n",
    "    knn_accuracy_umap_c50_norm[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on umap embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_umap_c50_norm, y_train)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm_test = svm_clf.predict(x_test_umap_c50_norm)  # Predict on test embeddings\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_umap_c50_norm = accuracy_score(y_test, y_pred_svm_test)\n",
    "print(f\"SVM Accuracy (Test): {svm_accuracy_umap_c50_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('cv_scores_umap_c50_norm.npy', cv_scores_umap_c50_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_umap_c50_norm= np.load('cv_scores_umap_c50_norm.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_umap_c50_norm = cross_val_score(svm_clf, x_train_umap_c50_norm, y_train, cv=10)\n",
    "cv_accuracy_umap_c50_norm = cv_scores_umap_c50_norm.mean()\n",
    "cv_std_umap_c50_norm = cv_scores_umap_c50_norm.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_umap_c50_norm:.4f} Â± {cv_std_umap_c50_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for umap\n",
    "results_umap_c50_norm = {\n",
    "    'ARI': ari_umap_c50_norm,\n",
    "    'Silhouette Score': silhouette_umap_c50_norm,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c50_norm,\n",
    "    'k-NN Accuracy': knn_accuracy_umap_c50_norm,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c50_norm, cv_std_umap_c50_norm)\n",
    "}\n",
    "\n",
    "print(\"umap Results:\")\n",
    "print(results_umap_c50_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_umap_c50_norm.npy\", x_train_umap_c50_norm)  # umap-reduced training data\n",
    "np.save(\"x_test_umap_c50_norm.npy\", x_test_umap_c50_norm)    # umap-reduced test data\n",
    "np.save(\"y_test_pred_umap_c50_norm.npy\", y_pred_svm)  # SVM predictions\n",
    "np.save(\"cv_scores_umap_c50_norm.npy\", cv_scores_umap_c50_norm)      # Cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intermediate data\n",
    "x_train_umap_c50_norm= np.load(\"x_train_umap_c50_norm.npy\") \n",
    "x_test_umap_c50_norm= np.load(\"x_test_umap_c50_norm.npy\") \n",
    "y_pred_svm= np.load(\"y_test_pred_umap_c50_norm.npy\") \n",
    "cv_scores_umap_c50_norm= np.load(\"cv_scores_umap_c50_norm.npy\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_umap_c50_norm.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_umap_c50_norm, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_umap_c50_norm_serializable = convert_to_serializable({\n",
    "    'ARI': ari_umap_c50_norm,\n",
    "    'Silhouette Score': silhouette_umap_c50_norm,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c50_norm,\n",
    "    'k-NN Accuracy': knn_accuracy_umap_c50_norm,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c50_norm, cv_std_umap_c50_norm)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"umap_c50_norm_results.json\", \"w\") as file:\n",
    "    json.dump(results_umap_c50_norm_serializable, file, indent=4)\n",
    "\n",
    "print(\"umap results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from JSON file\n",
    "with open(\"umap_c50_norm_results.json\", \"r\") as file:\n",
    "    results_umap_c50_norm = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_umap__c2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_components=2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "from umap import UMAP\n",
    "\n",
    "# Apply UMAP\n",
    "umap = UMAP(n_components=2, n_neighbors=15, random_state=42)\n",
    "x_train_umap_c2_norm = umap.fit_transform(x_train_normalized)\n",
    "x_test_umap_c2_norm = umap.transform(x_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data (umap embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_umap_c2_norm.npy\", x_train_umap_c2_norm)\n",
    "np.save(\"x_test_umap_c2_norm.npy\", x_test_umap_c2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_umap_c2_norm = np.vstack([x_train_umap_c2_norm, x_test_umap_c2_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full = np.hstack([y_train, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_umap_c2_norm = kmeans.fit_predict(x_full_umap_c2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_umap_c2_norm = adjusted_rand_score(y_full, cluster_labels_umap_c2_norm)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_umap_c2_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_umap_c2_norm = silhouette_score(x_full_umap_c2_norm, cluster_labels_umap_c2_norm)\n",
    "print(silhouette_umap_c2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_umap_c2_norm = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_umap_c2_norm, y_train)\n",
    "    knn_accuracy = knn.score(x_test_umap_c2_norm, y_test)\n",
    "    knn_accuracy_umap_c2_norm[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on umap embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_umap_c2_norm, y_train)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm_test = svm_clf.predict(x_test_umap_c2_norm)  # Predict on test embeddings\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_umap_c2_norm = accuracy_score(y_test, y_pred_svm_test)\n",
    "print(f\"SVM Accuracy (Test): {svm_accuracy_umap_c2_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('cv_scores_umap_c2_norm.npy', cv_scores_umap_c2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_umap_c2_norm = cross_val_score(svm_clf, x_train_umap_c2_norm, y_train, cv=10)\n",
    "cv_accuracy_umap_c2_norm = cv_scores_umap_c2_norm.mean()\n",
    "cv_std_umap_c2_norm = cv_scores_umap_c2_norm.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_umap_c2_norm:.4f} Â± {cv_std_umap_c2_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for umap\n",
    "results_umap_c2_norm = {\n",
    "    'ARI': ari_umap_c2_norm,\n",
    "    'Silhouette Score': silhouette_umap_c2_norm,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c2_norm,\n",
    "    'k-NN Accuracy': knn_accuracy_umap_c2_norm,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c2_norm, cv_std_umap_c2_norm)\n",
    "}\n",
    "\n",
    "print(\"umap Results:\")\n",
    "print(results_umap_c2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_umap_c2_norm.npy\", x_train_umap_c2_norm)  # umap-reduced training data\n",
    "np.save(\"x_test_umap_c2_norm.npy\", x_test_umap_c2_norm)    # umap-reduced test data\n",
    "np.save(\"y_test_pred_umap_c2_norm.npy\", y_pred_svm_test)  # SVM predictions\n",
    "np.save(\"cv_scores_umap_c2_norm.npy\", cv_scores_umap_c2_norm)      # Cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intermediate data\n",
    "x_train_umap_c2_norm= np.load(\"x_train_umap_c2_norm.npy\") \n",
    "x_test_umap_c2_norm= np.load(\"x_test_umap_c2_norm.npy\") \n",
    "y_pred_svm= np.load(\"y_test_pred_umap_c2_norm.npy\") \n",
    "cv_scores_umap_c2_norm= np.load(\"cv_scores_umap_c2_norm.npy\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_umap_c2_norm.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_umap_c2_norm, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_umap_c2_norm_serializable = convert_to_serializable({\n",
    "    'ARI': ari_umap_c2_norm,\n",
    "    'Silhouette Score': silhouette_umap_c2_norm,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c2_norm,\n",
    "    'k-NN Accuracy': knn_accuracy_umap_c2_norm,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c2_norm, cv_std_umap_c2_norm)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"umap_c2_norm_results.json\", \"w\") as file:\n",
    "    json.dump(results_umap_c2_norm_serializable, file, indent=4)\n",
    "\n",
    "print(\"umap results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from JSON file\n",
    "with open(\"umap_c2_norm_results.json\", \"r\") as file:\n",
    "    results_umap_c2_norm = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_umap_c2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Visualize the UMAP Results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_train_umap_c2_norm[:, 0], x_train_umap_c2_norm[:, 1], c=y_train, cmap=\"tab10\", s=5, alpha=0.8)\n",
    "plt.title(\"UMAP Projection of MNIST Dataset\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "plt.colorbar(label=\"MNIST Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS n_components= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_mnist_consistent(x_data, y_labels, sample_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Downsample the dataset consistently, returning indices to ensure\n",
    "    the same points are selected in both spaces.\n",
    "    \"\"\"\n",
    "    sampled_indices = []\n",
    "    unique_labels = np.unique(y_labels)\n",
    "    for label in unique_labels:\n",
    "        # Select indices for the current label\n",
    "        label_indices = np.where(y_labels == label)[0]\n",
    "        # Sample a fraction of points for this label\n",
    "        sampled_indices_label = resample(\n",
    "            label_indices, n_samples=int(len(label_indices) * sample_fraction), replace=False, random_state=42\n",
    "        )\n",
    "        sampled_indices.extend(sampled_indices_label)\n",
    "    return np.array(sampled_indices)\n",
    "\n",
    "# Downsample training data\n",
    "sampled_indices_train_mds = downsample_mnist_consistent(x_train_standardized1, y_train, sample_fraction=0.1)\n",
    "x_train_sampled_mds = x_train_standardized1[sampled_indices_train_mds]\n",
    "y_train_sampled_mds = y_train[sampled_indices_train_mds]\n",
    "\n",
    "# Downsample test data\n",
    "sampled_indices_test_mds = downsample_mnist_consistent(x_test_standardized1, y_test, sample_fraction=0.1)\n",
    "x_test_sampled_mds = x_test_standardized1[sampled_indices_test_mds]\n",
    "y_test_sampled_mds = y_test[sampled_indices_test_mds]\n",
    "\n",
    "print(f\"Training set reduced to {len(x_train_sampled_mds)} samples.\")\n",
    "print(f\"Test set reduced to {len(x_test_sampled_mds)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sampled indices\n",
    "np.save(\"sampled_indices_train_mds.npy\", sampled_indices_train_mds)\n",
    "np.save(\"sampled_indices_test_mds.npy\", sampled_indices_test_mds)\n",
    "\n",
    "# Save the downsampled dataset\n",
    "np.save(\"x_train_sampled_mds.npy\", x_train_sampled_mds)\n",
    "np.save(\"y_train_sampled_mds.npy\", y_train_sampled_mds)\n",
    "np.save(\"x_test_sampled_mds.npy\", x_test_sampled_mds)\n",
    "np.save(\"y_test_sampled_mds.npy\", y_test_sampled_mds)\n",
    "\n",
    "print(\"Downsampling saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sampled indices\n",
    "sampled_indices_train_mds= np.load(\"sampled_indices_train_mds.npy\")\n",
    "sampled_indices_test_mds= np.load(\"sampled_indices_test_mds.npy\")\n",
    "\n",
    "# Load downsampled dataset\n",
    "x_train_sampled_mds= np.load(\"x_train_sampled_mds.npy\")\n",
    "y_train_sampled_mds= np.load(\"y_train_sampled_mds.npy\")\n",
    "x_test_sampled_mds= np.load(\"x_test_sampled_mds.npy\")\n",
    "y_test_sampled_mds= np.load(\"y_test_sampled_mds.npy\")\n",
    "\n",
    "# Load \n",
    "x_train_mds_c2= np.load(\"x_train_mds_c2.npy\")\n",
    "x_test_mds_c2= np.load(\"x_test_mds_c2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MDS\n",
    "mds = MDS(n_components=2, random_state=42, n_jobs=-1)\n",
    "x_train_mds_c2 = mds.fit_transform(x_train_sampled_mds)\n",
    "x_test_mds_c2 = mds.fit_transform(x_test_sampled_mds)  # MDS needs to be run separately for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the downsampled dataset\n",
    "np.save(\"x_train_mds_c2.npy\", x_train_mds_c2)\n",
    "np.save(\"x_test_mds_c2.npy\", x_test_mds_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_mds_c2 = kmeans.fit_predict(x_test_mds_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_mds_c2 = adjusted_rand_score(y_test_sampled_mds, cluster_labels_mds_c2)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_mds_c2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_mds_c2 = silhouette_score(x_test_mds_c2, cluster_labels_mds_c2)\n",
    "print(silhouette_mds_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_accuracy_mds_c2 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_mds_c2, y_train_sampled_mds)\n",
    "    knn_accuracy = knn.score(x_test_mds_c2, y_test_sampled_mds)\n",
    "    knn_accuracy_mds_c2[k] = knn_accuracy\n",
    "\n",
    "print(f\"k-NN Accuracy: {knn_accuracy_mds_c2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_mds_c2, y_train_sampled_mds)\n",
    "y_test_pred_mds_c2 = svm_clf.predict(x_test_mds_c2)\n",
    "svm_accuracy_mds_c2 = accuracy_score(y_test_sampled_mds, y_test_pred_mds_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-Fold Cross-Validation Accuracy\n",
    "cv_scores_mds_c2 = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_mds_c2, y_train_sampled_mds, cv=10)\n",
    "cv_accuracy_mds_c2 = cv_scores_mds_c2.mean()\n",
    "cv_std_mds_c2 = cv_scores_mds_c2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for MDS\n",
    "results_mds_c2 = {\n",
    "    'ARI': ari_mds_c2,\n",
    "    'Silhouette Score': silhouette_mds_c2,\n",
    "    'SVM Accuracy': svm_accuracy_mds_c2,\n",
    "    'k-NN Accuracy': knn_accuracies_mds_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_mds_c2, cv_std_mds_c2)\n",
    "}\n",
    "\n",
    "print(\"MDS Results:\")\n",
    "print(results_mds_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_mds_c2.npy\", x_train_mds_c2)  # MDS-reduced training data\n",
    "np.save(\"x_test_mds_c2.npy\", x_test_mds_c2)    # MDS-reduced test data\n",
    "np.save(\"y_test_pred_mds_c2.npy\", y_test_pred_mds_c2)  # SVM predictions\n",
    "np.save(\"cv_scores_mds_c2.npy\", cv_scores_mds_c2)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracies_mds_c2.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracies_mds_c2, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_mds_c2_serializable = convert_to_serializable({\n",
    "    'ARI': ari_mds_c2,\n",
    "    'Silhouette Score': silhouette_mds_c2,\n",
    "    'SVM Accuracy': svm_accuracy_mds_c2,\n",
    "    'k-NN Accuracy': knn_accuracies_mds_c2,\n",
    "    '10-Fold CV Accuracy': {\n",
    "        'Mean': cv_accuracy_mds_c2,\n",
    "        'StdDev': cv_std_mds_c2\n",
    "    }\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"mds_c2_results.json\", \"w\") as file:\n",
    "    json.dump(results_mds_c2_serializable, file, indent=4)\n",
    "\n",
    "print(\"MDS results and intermediate data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2D projection with cluster labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=x_train_mds_c2[:, 0], y=x_train_mds_c2[:, 1], hue=y_train_sampled_mds, palette='tab10', s=10, legend='full')\n",
    "plt.title(\"2D Scatter Plot of PCA-reduced Data\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS n_components= 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and test sets\n",
    "x_full_mds = np.vstack([x_train_sampled_mds, x_test_sampled_mds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MDS once\n",
    "mds = MDS(n_components=50, random_state=42, n_jobs=-1)\n",
    "x_full_mds_c50 = mds.fit_transform(x_full_mds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the embeddings back into train and test sets\n",
    "x_train_mds_c50 = x_full_mds_c50[:len(y_train_sampled_mds)]\n",
    "x_test_mds_c50 = x_full_mds_c50[len(y_train_sampled_mds):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_full_mds_c50.npy', x_full_mds_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_train_mds_c50.npy', x_train_mds_c50)\n",
    "np.save('x_test_mds_c50.npy', x_test_mds_c50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_mds_c50 = kmeans.fit_predict(x_test_mds_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_mds_c50 = adjusted_rand_score(y_test_sampled_mds, cluster_labels_mds_c50)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_mds_c50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_mds_c50 = silhouette_score(x_test_mds_c50, cluster_labels_mds_c50)\n",
    "print(silhouette_mds_c50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_accuracy_mds_c50 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_mds_c50, y_train_sampled_mds)\n",
    "    knn_accuracy = knn.score(x_test_mds_c50, y_test_sampled_mds)\n",
    "    knn_accuracy_mds_c50[k] = knn_accuracy\n",
    "\n",
    "print(f\"k-NN Accuracy: {knn_accuracy_mds_c50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_mds_c50, y_train_sampled_mds)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_svm = svm_clf.predict(x_test_mds_c50)\n",
    "\n",
    "# Compute accuracy\n",
    "svm_accuracy_mds_c50 = accuracy_score(y_test_sampled_mds, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {svm_accuracy_mds_c50:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 10-fold CV\n",
    "cv_scores_mds_c50 = cross_val_score(svm_clf, x_train_mds_c50, y_train_sampled_mds, cv=10)\n",
    "cv_accuracy_mds_c50 = cv_scores_mds_c50.mean()\n",
    "cv_std_mds_c50 = cv_scores_mds_c50.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_mds_c50:.4f} Â± {cv_std_mds_c50:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for mds\n",
    "results_mds_c50 = {\n",
    "    'ARI': ari_mds_c50,\n",
    "    'Silhouette Score': silhouette_mds_c50,\n",
    "    'SVM Accuracy': svm_accuracy_mds_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_mds_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_mds_c50, cv_std_mds_c50)\n",
    "}\n",
    "\n",
    "print(\"mds Results:\")\n",
    "print(results_mds_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_mds_c50.npy\", x_train_mds_c50)  # mds-reduced training data\n",
    "np.save(\"x_test_mds_c50.npy\", x_test_mds_c50)    # mds-reduced test data\n",
    "np.save(\"y_test_pred_mds_c50.npy\", y_pred_svm)  # SVM predictions\n",
    "np.save(\"cv_scores_mds_c50.npy\", cv_scores_mds_c50)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_mds_c50.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_mds_c50, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_mds_c50_serializable = convert_to_serializable({\n",
    "    'ARI': ari_mds_c50,\n",
    "    'Silhouette Score': silhouette_mds_c50,\n",
    "    'SVM Accuracy': svm_accuracy_mds_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_mds_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_mds_c50, cv_std_mds_c50)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"mds_c50_results.json\", \"w\") as file:\n",
    "    json.dump(results_mds_c50_serializable, file, indent=4)\n",
    "\n",
    "print(\"mds results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings and cluster labels\n",
    "x_train_pca_c2 = np.load(\"x_train_pca_c2.npy\")\n",
    "x_test_pca_c2 = np.load(\"x_test_pca_c2.npy\")\n",
    "\n",
    "x_train_tsne_c2 = np.load(\"x_train_tsne_c2.npy\")\n",
    "x_test_tsne_c2 = np.load(\"x_test_tsne_c2.npy\")\n",
    "\n",
    "x_train_isomap_c2 = np.load(\"x_train_isomap_c2.npy\")\n",
    "x_test_isomap_c2 = np.load(\"x_test_isomap_c2.npy\")\n",
    "y_test_sampled = np.load(\"y_test_sampled.npy\")\n",
    "y_train_sampled = np.load(\"y_train_sampled.npy\")\n",
    "\n",
    "x_train_lle_c2 = np.load(\"x_train_lle_c2.npy\")\n",
    "x_test_lle_c2 = np.load(\"x_test_lle_c2.npy\")\n",
    "\n",
    "x_train_umap = np.load('x_train_umap_c2.npy')\n",
    "x_test_umap_c2_norm= np.load('x_test_umap_c2_norm.npy')\n",
    "\n",
    "x_train_mds_c2 = np.load(\"x_train_mds_c2.npy\")\n",
    "x_test_mds_c2 = np.load(\"x_test_mds_c2.npy\")\n",
    "y_train_sampled_mds = np.load(\"y_train_sampled_mds.npy\")\n",
    "y_test_sampled_mds = np.load(\"y_test_sampled_mds.npy\")\n",
    "\n",
    "# Use test embeddings and labels for visualization\n",
    "methods = {\n",
    "    'PCA': (x_test_pca_c2, y_test),\n",
    "    'Isomap': (x_test_isomap_c2, y_test_sampled),\n",
    "    'LLE': (x_test_lle_c2, y_test_sampled),\n",
    "    'MDS': (x_test_mds_c2, y_test_sampled_mds),\n",
    "    't-SNE': (x_test_tsne_c2, y_test),\n",
    "    'UMAP': (x_test_umap_c2_norm, y_test)\n",
    "}\n",
    "\n",
    "# Create a grid of subplots with two columns and three rows\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))  # Two columns, three rows\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)  # Adjust spacing between plots\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define the label names (digits 0-9)\n",
    "label_names = [f\"Digit {i}\" for i in range(10)]\n",
    "\n",
    "for ax, (method, (embedding, labels)) in zip(axes, methods.items()):\n",
    "    scatter = sns.scatterplot(\n",
    "        x=embedding[:, 0], \n",
    "        y=embedding[:, 1], \n",
    "        hue=labels.astype(str),  # Ensure labels are strings\n",
    "        palette='tab10',  # Use a standard tab10 palette for each plot\n",
    "        s=40,  # Larger markers for better visibility\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f'{method} Embeddings', fontsize=14, pad=10, loc='center')  # Larger font size\n",
    "\n",
    "    # Hide x and y axis ticks and labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    # Set equal aspect ratio for symmetry\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Add a legend for each plot\n",
    "    handles, labels = scatter.get_legend_handles_labels()\n",
    "    ax.legend(\n",
    "        handles=handles, labels=label_names, title=\"Cluster\", fontsize=10, loc='upper right',\n",
    "        frameon=True, edgecolor=\"black\"\n",
    "    )\n",
    "\n",
    "# Ensure the layout updates properly\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show all plots\n",
    "plt.show()\n",
    "\n",
    "# Verification: Ensure colors and digits are correctly matched\n",
    "print(\"\\nVerifying colors and digits for each plot:\")\n",
    "for method, (embedding, labels) in methods.items():\n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    \n",
    "    # Generate scatter plot to extract handles and colors\n",
    "    scatter = sns.scatterplot(\n",
    "        x=embedding[:, 0],\n",
    "        y=embedding[:, 1],\n",
    "        hue=labels.astype(str),\n",
    "        palette='tab10',\n",
    "        s=10,\n",
    "        legend=True  # Ensure legend is generated\n",
    "    )\n",
    "    legend = scatter.get_legend()\n",
    "    handles = legend.legendHandles  # Get handles from the legend\n",
    "    plt.close()  # Close the plot since we only need the handles\n",
    "\n",
    "    # Ensure there are exactly 10 handles for digits 0-9\n",
    "    if len(handles) != 10:\n",
    "        print(f\"Error: Expected 10 clusters but got {len(handles)} for {method}.\")\n",
    "        continue\n",
    "\n",
    "    # Check colors for each digit\n",
    "    for digit in range(10):\n",
    "        # Extract color from the plot handle and normalize to (R, G, B)\n",
    "        color_in_plot = tuple(handles[digit].get_facecolor()[0][:3])  # Normalize to a tuple\n",
    "        expected_color = tuple(sns.color_palette('tab10', 10)[digit])  # Also as a tuple\n",
    "        \n",
    "        # Compare RGB components\n",
    "        match = color_in_plot == expected_color\n",
    "        print(\n",
    "            f\"Digit {digit}: Color in plot {color_in_plot} | Expected color {expected_color} | Match: {match}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"x_train_umap_c50.npy\", x_train_umap_c50)\n",
    "np.save(\"x_test_umap_c50.npy\", x_test_umap_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings and cluster labels\n",
    "x_train_pca_c50 = np.load(\"x_train_pca_c50.npy\")\n",
    "x_test_pca_c50 = np.load(\"x_test_pca_c50.npy\")\n",
    "\n",
    "x_train_tsne_c2 = np.load(\"x_train_tsne_c2.npy\")\n",
    "x_test_tsne_c2 = np.load(\"x_test_tsne_c2.npy\")\n",
    "\n",
    "x_train_isomap_c50 = np.load(\"x_train_isomap_c50.npy\")\n",
    "x_test_isomap_c50 = np.load(\"x_test_isomap_c50.npy\")\n",
    "y_test_sampled = np.load(\"y_test_sampled.npy\")\n",
    "y_train_sampled = np.load(\"y_train_sampled.npy\")\n",
    "\n",
    "x_train_lle_c50 = np.load(\"x_train_lle_c50.npy\")\n",
    "x_test_lle_c50 = np.load(\"x_test_lle_c50.npy\")\n",
    "\n",
    "x_train_umap_c50 = np.load('x_train_umap_c50.npy')\n",
    "x_test_umap_c50 = np.load('x_test_umap_c50.npy')\n",
    "\n",
    "\n",
    "\n",
    "x_train_mds_c50 = np.load(\"x_train_mds_c50.npy\")\n",
    "x_test_mds_c50 = np.load(\"x_test_mds_c50.npy\")\n",
    "y_train_sampled_mds = np.load(\"y_train_sampled_mds.npy\")\n",
    "y_test_sampled_mds = np.load(\"y_test_sampled_mds.npy\")\n",
    "\n",
    "# Use test embeddings and labels for visualization\n",
    "methods = {\n",
    "    'PCA': (x_test_pca_c50, y_test),\n",
    "    'Isomap': (x_test_isomap_c50, y_test_sampled),\n",
    "    'LLE': (x_test_lle_c50, y_test_sampled),\n",
    "    'MDS': (x_test_mds_c50, y_test_sampled_mds),\n",
    "    't-SNE': (x_test_tsne_c2, y_test),\n",
    "    'UMAP': (x_test_umap_c50, y_test)\n",
    "}\n",
    "\n",
    "# Create a grid of subplots with two columns and three rows\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))  # Two columns, three rows\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)  # Adjust spacing between plots\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define the label names (digits 0-9)\n",
    "label_names = [f\"Digit {i}\" for i in range(10)]\n",
    "\n",
    "for ax, (method, (embedding, labels)) in zip(axes, methods.items()):\n",
    "    scatter = sns.scatterplot(\n",
    "        x=embedding[:, 0], \n",
    "        y=embedding[:, 1], \n",
    "        hue=labels.astype(str),  # Ensure labels are strings\n",
    "        palette='tab10',  # Use a standard tab10 palette for each plot\n",
    "        s=40,  # Larger markers for better visibility\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f'{method} Embeddings', fontsize=14, pad=10, loc='center')  # Larger font size\n",
    "\n",
    "    # Hide x and y axis ticks and labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    # Set equal aspect ratio for symmetry\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Add a legend for each plot\n",
    "    handles, labels = scatter.get_legend_handles_labels()\n",
    "    ax.legend(\n",
    "        handles=handles, labels=label_names, title=\"Cluster\", fontsize=10, loc='upper right',\n",
    "        frameon=True, edgecolor=\"black\"\n",
    "    )\n",
    "\n",
    "# Ensure the layout updates properly\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show all plots\n",
    "plt.show()\n",
    "\n",
    "# Verification: Ensure colors and digits are correctly matched\n",
    "print(\"\\nVerifying colors and digits for each plot:\")\n",
    "for method, (embedding, labels) in methods.items():\n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    \n",
    "    # Generate scatter plot to extract handles and colors\n",
    "    scatter = sns.scatterplot(\n",
    "        x=embedding[:, 0],\n",
    "        y=embedding[:, 1],\n",
    "        hue=labels.astype(str),\n",
    "        palette='tab10',\n",
    "        s=10,\n",
    "        legend=True  # Ensure legend is generated\n",
    "    )\n",
    "    legend = scatter.get_legend()\n",
    "    handles = legend.legendHandles  # Get handles from the legend\n",
    "    plt.close()  # Close the plot since we only need the handles\n",
    "\n",
    "    # Ensure there are exactly 10 handles for digits 0-9\n",
    "    if len(handles) != 10:\n",
    "        print(f\"Error: Expected 10 clusters but got {len(handles)} for {method}.\")\n",
    "        continue\n",
    "\n",
    "    # Check colors for each digit\n",
    "    for digit in range(10):\n",
    "        # Extract color from the plot handle and normalize to (R, G, B)\n",
    "        color_in_plot = tuple(handles[digit].get_facecolor()[0][:3])  # Normalize to a tuple\n",
    "        expected_color = tuple(sns.color_palette('tab10', 10)[digit])  # Also as a tuple\n",
    "        \n",
    "        # Compare RGB components\n",
    "        match = color_in_plot == expected_color\n",
    "        print(\n",
    "            f\"Digit {digit}: Color in plot {color_in_plot} | Expected color {expected_color} | Match: {match}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General UMAP - Hyperparameters experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the MNIST dataset to a flattened format suitable for UMAP\n",
    "x_train_flattened = np.array([np.array(img).flatten() for img in x_train])\n",
    "x_test_flattened = np.array([np.array(img).flatten() for img in x_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=5 min_dist=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_5_01_35= np.load(f'umap_projections_5_01_35.npy')\n",
    "mean_projection_5_01_35= np.load(f'mean_projection_5_01_35.npy')\n",
    "std_projection_5_01_35= np.load(f'std_projection_5_01_35.npy')\n",
    "lower_limit_intconf_matrix_5_01_35= np.load(f'lower_limit_intconf_matrix_5_01_35.npy')\n",
    "upper_limit_intconf_matrix_5_01_35= np.load(f'upper_limit_intconf_matrix_5_01_35.npy')\n",
    "distance_matrices_5_01_35=np.load(f'distance_matrices_neighbors_5_01_35.npy')\n",
    "mean_distance_matrix_5_01_35=np.load(f'mean_distance_matrix_neighbors_5_01_35.npy')\n",
    "norm_lower_limit_intconf_matrix_5_01_35=np.load(f'norm_lower_limit_intconf_matrix_5_01_35.npy')\n",
    "norm_upper_limit_intconf_matrix_5_01_35=np.load(f'norm_upper_limit_intconf_matrix_5_01_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 5\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_5_01_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_5_01_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_5_01_35 = np.array(umap_projections_5_01_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_5_01_35 = np.mean(umap_projections_5_01_35, axis=0)\n",
    "std_projection_5_01_35 = np.std(umap_projections_5_01_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_5_01_35.npy', umap_projections_5_01_35)\n",
    "np.save('mean_projection_5_01_35.npy', mean_projection_5_01_35)\n",
    "np.save('std_projection_5_01_35.npy', std_projection_5_01_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_5_01_35'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_5_01_35 = np.sqrt(np.sum((umap_projections_5_01_35 - mean_projection_5_01_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_5_01_35, axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"y_train type: {type(y_train)}\")  # Should now be <class 'numpy.ndarray'>\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_5_01_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_5_01 = np.zeros((n_runs, n_clusters, umap_projections_5_01_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_5_01_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_5_01[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_5_01 = np.zeros(10)\n",
    "std_dev_y_5_01 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_5_01[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_5_01[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_5_01[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_5_01[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_5_01)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_5_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_5[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_5[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_5[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_5_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{5_01}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_5_01_35[run]  # Shape: (n_samples, 2)\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_5_01_35 = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_5_01_35.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_5_01_35 = np.array(distance_matrices_5_01_35)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_5_01_35 = np.mean(distance_matrices_5_01_35, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_5_01_35 = (mean_distance_matrix_5_01_35 - np.min(mean_distance_matrix_5_01_35)) / (np.max(mean_distance_matrix_5_01_35) - np.min(mean_distance_matrix_5_01_35))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=5)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_5_01_35.npy', distance_matrices_5_01_35)\n",
    "np.save('mean_distance_matrix_neighbors_5_01_35.npy', mean_distance_matrix_5_01_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_5_01_35}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_5_01_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_5_01_35,3))\n",
    "np.save('G_5_01_35.npy',G_5_01_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_5_01_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_5_01_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_5_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_5_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_5_01_35 = nx.minimum_spanning_tree(G_5_01_35)\n",
    "np.save('mst_5_01_35.npy', mst_5_01_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_5_01_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_5_01_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_5_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_5_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=5, min_dist=0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_5_01_35 = np.std(distance_matrices_5_01_35, axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_5_01_35.npy\", distance_matrix_std_5_01_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_01_35):\\n\", distance_matrix_std_5_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_5_01_35 = distance_matrix_std_5_01_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_5_01_35 = z_score * sem_matrix_5_01_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_5_01_35 = mean_distance_matrix_5_01_35 - margin_of_error_matrix_5_01_35\n",
    "upper_limit_intconf_matrix_5_01_35 = mean_distance_matrix_5_01_35 + margin_of_error_matrix_5_01_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_5_01_35 = np.maximum(lower_limit_intconf_matrix_5_01_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_5_01_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_5_01_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_5_01_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_5_01_35.npy', lower_limit_intconf_matrix_5_01_35)\n",
    "np.save('upper_limit_intconf_matrix_5_01_35.npy', upper_limit_intconf_matrix_5_01_35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interval of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_5_01_35 = normalize_matrix(lower_limit_intconf_matrix_5_01_35)\n",
    "norm_upper_limit_intconf_matrix_5_01_35 = normalize_matrix(upper_limit_intconf_matrix_5_01_35)\n",
    "np.save('norm_lower_limit_intconf_matrix_5_01_35.npy', norm_lower_limit_intconf_matrix_5_01_35)\n",
    "np.save('norm_upper_limit_intconf_matrix_5_01_35.npy', norm_upper_limit_intconf_matrix_5_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_5_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=5, min_dist=0.1)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=5, min_dist=0.1)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_5_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=5, min_dist=0.1)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_5_01_35, \"UMAP MST - Mean Distances - n_neighbors=5 min_dist = 0.1\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_5_01_35, \"UMAP MST - Lower Limit - n_neighbors=5 min_dist = 0.1\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_5_01_35, \"UMAP MST - Upper Limit - n_neighbors=5 min_dist = 0.1\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptable Radius Final Version & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cluster metrics\n",
    "def calculate_cluster_metrics(umap_projections, y_labels, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Calculate average cluster radii and neighbor counts for each cluster over all runs.\n",
    "    \"\"\"\n",
    "    n_runs = len(umap_projections)  # Number of runs\n",
    "    cluster_centers_full = []\n",
    "    \n",
    "    # Step 1: Calculabte cluster centers for each run\n",
    "    for run_idx, x_umap in enumerate(umap_projections):\n",
    "        cluster_centers_run = []\n",
    "        for label in np.unique(y_labels):\n",
    "            cluster_points = x_umap[y_labels == label]\n",
    "            if len(cluster_points) > 0:\n",
    "                cluster_center = np.mean(cluster_points, axis=0)\n",
    "                cluster_centers_run.append(cluster_center)\n",
    "        cluster_centers_full.append(np.array(cluster_centers_run))\n",
    "    \n",
    "    cluster_centers_full = np.array(cluster_centers_full)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "    # Step 2: Calculate average radii for each cluster\n",
    "    radii_per_cluster = []\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        radii_cluster = []\n",
    "        for run_idx, x_umap in enumerate(umap_projections):\n",
    "            cluster_center = cluster_centers_full[run_idx][cluster_idx]\n",
    "            cluster_points = x_umap[y_labels == cluster_idx]\n",
    "            if len(cluster_points) > 0:\n",
    "                distances_to_center = np.linalg.norm(cluster_points - cluster_center, axis=1)\n",
    "                dynamic_radius = np.mean(distances_to_center)  # Mean distance to center\n",
    "                radii_cluster.append(dynamic_radius)\n",
    "        radii_per_cluster.append(np.mean(radii_cluster))  # Average radius across runs\n",
    "\n",
    "    # Step 3: Calculate neighbor counts for each cluster\n",
    "    neighbor_counts_full = []\n",
    "    for run_idx, x_umap in enumerate(umap_projections):\n",
    "        counts_run = []\n",
    "        for cluster_idx, cluster_center in enumerate(cluster_centers_full[run_idx]):\n",
    "            radius = radii_per_cluster[cluster_idx]  # Use the average radius\n",
    "            distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "            count = np.sum(distances_to_center <= radius)  # Count points within the radius\n",
    "            counts_run.append(count)\n",
    "        neighbor_counts_full.append(counts_run)\n",
    "\n",
    "    neighbor_counts_full = np.array(neighbor_counts_full)  # Shape: (n_runs, n_clusters)\n",
    "    average_neighbor_counts = np.mean(neighbor_counts_full, axis=0)  # Average across runs\n",
    "\n",
    "    return radii_per_cluster, average_neighbor_counts\n",
    "\n",
    "# Define n_neighbors values\n",
    "n_neighbors_values = [5, 10, 20, 30, 50, 100]\n",
    "results = []\n",
    "\n",
    "# Iterate over each n_neighbors value\n",
    "for n_neighbors in n_neighbors_values:\n",
    "    if n_neighbors == 5:\n",
    "        umap_projections = umap_projections_5_01_35\n",
    "    elif n_neighbors == 10:\n",
    "        umap_projections = umap_projections_10_01_35\n",
    "    elif n_neighbors == 20:\n",
    "        umap_projections = umap_projections_20_01_35\n",
    "    elif n_neighbors == 30:\n",
    "        umap_projections = umap_projections_30_01_35\n",
    "    elif n_neighbors == 50:\n",
    "        umap_projections = umap_projections_50_01_35\n",
    "    elif n_neighbors == 100:\n",
    "        umap_projections = umap_projections_100_01_35\n",
    "\n",
    "    # Calculate metrics\n",
    "    radii_per_cluster, average_neighbor_counts = calculate_cluster_metrics(umap_projections, y_train)\n",
    "\n",
    "    # Store results\n",
    "    for cluster_idx in range(len(radii_per_cluster)):\n",
    "        results.append({\n",
    "            \"N\": n_neighbors,\n",
    "            \"Cluster\": cluster_idx,\n",
    "            \"Radius\": np.round(radii_per_cluster[cluster_idx], 3),\n",
    "            \"Number of Neighbors\": np.round(average_neighbor_counts[cluster_idx], 0)\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save results for later use\n",
    "df_results.to_csv(\"radius_neighbor_analysis_merged_MinDist_01.csv\", index=False)\n",
    "\n",
    "# Pivot table for easy visualization\n",
    "pivot_table = df_results.pivot(index=\"Cluster\", columns=\"N\", values=[\"Radius\", \"Number of Neighbors\"])\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_neighbor_counts_across_runs(umap_projections_list, n_neighbors_values, y_labels, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Plot mean number of neighbors across runs for different n_neighbors values.\n",
    "    \"\"\"\n",
    "    neighbor_counts_avg_runs = []\n",
    "\n",
    "    for umap_projections in umap_projections_list:\n",
    "        # Calculate neighbor counts for each run\n",
    "        neighbor_counts_per_run = []\n",
    "        for run_idx, x_umap in enumerate(umap_projections):\n",
    "            cluster_centers = []\n",
    "            radii_per_cluster = []\n",
    "            for cluster_idx in range(n_clusters):\n",
    "                # Compute cluster center\n",
    "                cluster_points = x_umap[y_labels == cluster_idx]\n",
    "                if len(cluster_points) > 0:\n",
    "                    cluster_center = np.mean(cluster_points, axis=0)\n",
    "                    cluster_centers.append(cluster_center)\n",
    "\n",
    "                    # Compute dynamic radius for this cluster\n",
    "                    distances_to_center = np.linalg.norm(cluster_points - cluster_center, axis=1)\n",
    "                    dynamic_radius = np.mean(distances_to_center)\n",
    "                    radii_per_cluster.append(dynamic_radius)\n",
    "                else:\n",
    "                    radii_per_cluster.append(0)\n",
    "                    cluster_centers.append(np.array([0, 0]))\n",
    "\n",
    "            # Compute number of neighbors within radius for each cluster\n",
    "            neighbor_counts = []\n",
    "            for cluster_idx, cluster_center in enumerate(cluster_centers):\n",
    "                if radii_per_cluster[cluster_idx] > 0:  # Avoid empty clusters\n",
    "                    distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "                    neighbor_count = np.sum(distances_to_center <= radii_per_cluster[cluster_idx])\n",
    "                    neighbor_counts.append(neighbor_count)\n",
    "\n",
    "            # Store the mean neighbor count for this run\n",
    "            neighbor_counts_per_run.append(np.mean(neighbor_counts))\n",
    "        \n",
    "        neighbor_counts_avg_runs.append(neighbor_counts_per_run)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, counts in enumerate(neighbor_counts_avg_runs):\n",
    "        plt.plot(range(1, len(counts) + 1), counts, label=f'n_neighbors={n_neighbors_values[i]}', marker='o')\n",
    "\n",
    "    plt.xlabel(\"Run Index\")\n",
    "    plt.ylabel(\"Mean Number of Points\")\n",
    "    plt.title(\"Mean Number of Points Across Runs for min_dist = 0.1\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))  # Adjust legend position\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_neighbor_counts_across_runs(\n",
    "    umap_projections_list=[\n",
    "        umap_projections_5_01_35,\n",
    "        umap_projections_10_01_35,\n",
    "        umap_projections_20_01_35,\n",
    "        umap_projections_30_01_35,\n",
    "        umap_projections_50_01_35,\n",
    "        umap_projections_100_01_35\n",
    "    ],\n",
    "    n_neighbors_values=n_neighbors_values,\n",
    "    y_labels=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=10 min_dist=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_10_01_35= np.load(f'umap_projections_10_01_35.npy')\n",
    "mean_projection_10_01_35= np.load(f'mean_projection_10_01_35.npy')\n",
    "std_projection_10_01_35= np.load(f'std_projection_10_01_35.npy')\n",
    "lower_limit_intconf_matrix_10_01_35= np.load(f'lower_limit_intconf_matrix_10_01_35.npy')\n",
    "upper_limit_intconf_matrix_10_01_35= np.load(f'upper_limit_intconf_matrix_10_01_35.npy')\n",
    "distance_matrices_10_01_35=np.load(f'distance_matrices_neighbors_10_01_35.npy')\n",
    "mean_distance_matrix_10_01_35=np.load(f'mean_distance_matrix_neighbors_10_01_35.npy')\n",
    "norm_lower_limit_intconf_matrix_10_01_35=np.load(f'norm_lower_limit_intconf_matrix_10_01_35.npy')\n",
    "norm_upper_limit_intconf_matrix_10_01_35=np.load(f'norm_upper_limit_intconf_matrix_10_01_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_10_01_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_10_01_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_10_01_35 = np.array(umap_projections_10_01_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_10_01_35 = np.mean(umap_projections_10_01_35, axis=0)\n",
    "std_projection_10_01_35 = np.std(umap_projections_10_01_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_10_01_35.npy', umap_projections_10_01_35)\n",
    "np.save('mean_projection_10_01_35.npy', mean_projection_10_01_35)\n",
    "np.save('std_projection_10_01_35.npy', std_projection_10_01_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_10_01_35'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_10_01_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_10_01 = np.zeros((n_runs, n_clusters, umap_projections_10_01_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_10_01_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_10_01[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_10_01 = np.zeros(10)\n",
    "std_dev_y_10_01 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_10_01[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_10_01[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_10_01[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_10_01[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_10_01)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_10_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_10[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_10[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_10[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_10_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{10_01}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_10_01_35[run]  # Shape: (n_samples, 2)\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_10_01_35 = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_10_01_35.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_10_01_35 = np.array(distance_matrices_10_01_35)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_10_01_35 = np.mean(distance_matrices_10_01_35, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_10_01_35 = (mean_distance_matrix_10_01_35 - np.min(mean_distance_matrix_10_01_35)) / (np.max(mean_distance_matrix_10_01_35) - np.min(mean_distance_matrix_10_01_35))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=10)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_10_01_35.npy', distance_matrices_10_01_35)\n",
    "np.save('mean_distance_matrix_neighbors_10_01_35.npy', mean_distance_matrix_10_01_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_10_01_35}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spaning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_10_01_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_10_01_35,3))\n",
    "np.save('G_10_01_35.npy',G_10_01_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_10_01_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_10_01_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_10_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_10_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total weight of the MST\n",
    "total_weight = sum(nx.get_edge_attributes(mst_10_01_35, 'weight').values())\n",
    "\n",
    "# Print the total weight\n",
    "print(f\"Total weight of the MST: {total_weight}\")\n",
    "\n",
    "# Compute the minimum spanning tree of the graph\n",
    "mst_10_01_35 = nx.minimum_spanning_tree(G_10_01_35)\n",
    "np.save('mst_10_01_35.npy', mst_10_01_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_10_01_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_10_01_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_10_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_10_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=10, min_dist=0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree (MST) of the graph\n",
    "mst_10_01_35 = nx.minimum_spanning_tree(G_10_01_35)\n",
    "\n",
    "# Save the MST for later use\n",
    "np.save('mst_10_01_35.npy', mst_10_01_35)\n",
    "\n",
    "# Define positions for all nodes in the MST using a spring layout\n",
    "pos = nx.spring_layout(mst_10_01_35, seed=42)\n",
    "\n",
    "# Increase figure size for better visibility\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Draw the MST with larger nodes, thicker edges, and a larger font for labels\n",
    "nx.draw(\n",
    "    mst_10_01_35,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_color='lightblue',\n",
    "    edge_color='red',\n",
    "    node_size=1000,  # Larger node size for better visibility\n",
    "    font_size=12,    # Larger font size for node labels\n",
    "    width=3          # Thicker edge lines\n",
    ")\n",
    "\n",
    "# Get edge weights and format them to 2 decimal places for clarity\n",
    "edge_labels = nx.get_edge_attributes(mst_10_01_35, 'weight')\n",
    "formatted_edge_labels = {k: f\"{v:.2f}\" for k, v in edge_labels.items()}\n",
    "\n",
    "# Draw edge labels with formatted weights\n",
    "nx.draw_networkx_edge_labels(\n",
    "    mst_10_01_35,\n",
    "    pos,\n",
    "    edge_labels=formatted_edge_labels,\n",
    "    font_size=20,    # Font size for edge labels\n",
    "    label_pos=0.5    # Position edge labels at the center of edges\n",
    ")\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title(\"MST UMAP - n_neighbors=10, min_dist=0.1\", fontsize=16)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_10_01_35 = np.std(distance_matrices_10_01_35, axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_10_01_35.npy\", distance_matrix_std_10_01_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (10_01_35):\\n\", distance_matrix_std_10_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_10_01_35 = distance_matrix_std_10_01_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_10_01_35 = z_score * sem_matrix_10_01_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_10_01_35 = mean_distance_matrix_10_01_35 - margin_of_error_matrix_10_01_35\n",
    "upper_limit_intconf_matrix_10_01_35 = mean_distance_matrix_10_01_35 + margin_of_error_matrix_10_01_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_10_01_35 = np.maximum(lower_limit_intconf_matrix_10_01_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_10_01_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_10_01_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_10_01_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_10_01_35.npy', lower_limit_intconf_matrix_10_01_35)\n",
    "np.save('upper_limit_intconf_matrix_10_01_35.npy', upper_limit_intconf_matrix_10_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_10_01_35 = normalize_matrix(lower_limit_intconf_matrix_10_01_35)\n",
    "norm_upper_limit_intconf_matrix_10_01_35 = normalize_matrix(upper_limit_intconf_matrix_10_01_35)\n",
    "np.save('norm_lower_limit_intconf_matrix_10_01_35.npy', norm_lower_limit_intconf_matrix_10_01_35)\n",
    "np.save('norm_upper_limit_intconf_matrix_10_01_35.npy', norm_upper_limit_intconf_matrix_10_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_10_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=10, min_dist=0.1)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=10, min_dist=0.1)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_10_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=10, min_dist=0.1)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_10_01_35, \"UMAP MST - Mean Distances - n_neighbors=10, min_dist=0.1\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_10_01_35, \"UMAP MST - Lower Limit- n_neighbors=10, min_dist=0.1\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_10_01_35, \"UMAP MST - Upper Limit- n_neighbors=10, min_dist=0.1\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=20 min_dist= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_20_01_35= np.load(f'umap_projections_20_01_35.npy')\n",
    "mean_projection_20_01_35= np.load(f'mean_projection_20_01_35.npy')\n",
    "std_projection_20_01_35= np.load(f'std_projection_20_01_35.npy')\n",
    "lower_limit_intconf_matrix_20_01_35= np.load(f'lower_limit_intconf_matrix_20_01_35.npy')\n",
    "upper_limit_intconf_matrix_20_01_35= np.load(f'upper_limit_intconf_matrix_20_01_35.npy')\n",
    "distance_matrices_20_01_35=np.load(f'distance_matrices_neighbors_20_01_35.npy')\n",
    "mean_distance_matrix_20_01_35=np.load(f'mean_distance_matrix_neighbors_20_01_35.npy')\n",
    "norm_lower_limit_intconf_matrix_20_01_35=np.load(f'norm_lower_limit_intconf_matrix_20_01_35.npy')\n",
    "norm_upper_limit_intconf_matrix_20_01_35=np.load(f'norm_upper_limit_intconf_matrix_20_01_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 20\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_20_01_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_20_01_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_20_01_35 = np.array(umap_projections_20_01_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_20_01_35 = np.mean(umap_projections_20_01_35, axis=0)\n",
    "std_projection_20_01_35 = np.std(umap_projections_20_01_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_20_01_35.npy', umap_projections_20_01_35)\n",
    "np.save('mean_projection_20_01_35.npy', mean_projection_20_01_35)\n",
    "np.save('std_projection_20_01_35.npy', std_projection_20_01_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_20_01_35'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_20_01_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_20_01 = np.zeros((n_runs, n_clusters, umap_projections_20_01_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_20_01_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_20_01[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_20_01 = np.zeros(10)\n",
    "std_dev_y_20_01 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_20_01[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_20_01[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_20_01[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_20_01[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_20_01)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_20_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_20[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_20[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_20[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_20_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{20_01}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_20_01_35[run]  # Shape: (n_samples, 2)\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_20_01_35 = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_20_01_35.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_20_01_35 = np.array(distance_matrices_20_01_35)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_20_01_35 = np.mean(distance_matrices_20_01_35, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_20_01_35 = (mean_distance_matrix_20_01_35 - np.min(mean_distance_matrix_20_01_35)) / (np.max(mean_distance_matrix_20_01_35) - np.min(mean_distance_matrix_20_01_35))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_20_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=20, min_dist=0.1)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_20_01_35.npy', distance_matrices_20_01_35)\n",
    "np.save('mean_distance_matrix_neighbors_20_01_35.npy', mean_distance_matrix_20_01_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_20_01_35}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_20_01_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_20_01_35,3))\n",
    "np.save('G_20_01_35.npy',G_20_01_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_20_01_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_20_01_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_20_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_20_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_20_01_35 = nx.minimum_spanning_tree(G_20_01_35)\n",
    "np.save('mst_20_01_35.npy', mst_20_01_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_20_01_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_20_01_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_20_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_20_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=20, min_dist=0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_20_01_35 = np.std(distance_matrices_20_01_35, axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_20_01_35.npy\", distance_matrix_std_20_01_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (20_01_35):\\n\", distance_matrix_std_20_01_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_20_01_35 = distance_matrix_std_20_01_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_20_01_35 = z_score * sem_matrix_20_01_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_20_01_35 = mean_distance_matrix_20_01_35 - margin_of_error_matrix_20_01_35\n",
    "upper_limit_intconf_matrix_20_01_35 = mean_distance_matrix_20_01_35 + margin_of_error_matrix_20_01_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_20_01_35 = np.maximum(lower_limit_intconf_matrix_20_01_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_20_01_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_20_01_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_20_01_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_20_01_35.npy', lower_limit_intconf_matrix_20_01_35)\n",
    "np.save('upper_limit_intconf_matrix_20_01_35.npy', upper_limit_intconf_matrix_20_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_20_01_35 = normalize_matrix(lower_limit_intconf_matrix_20_01_35)\n",
    "norm_upper_limit_intconf_matrix_20_01_35 = normalize_matrix(upper_limit_intconf_matrix_20_01_35)\n",
    "np.save('norm_lower_limit_intconf_matrix_20_01_35.npy', norm_lower_limit_intconf_matrix_20_01_35)\n",
    "np.save('norm_upper_limit_intconf_matrix_20_01_35.npy', norm_upper_limit_intconf_matrix_20_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_20_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=20, min_dist=0.1)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_20_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=20, min_dist=0.1)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_20_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=20, min_dist=0.1)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_20_01_35, \"UMAP MST - Mean Distances - n_neighbors=20, min_dist=0.1\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_20_01_35, \"UMAP MST - Lower Limit - n_neighbors=20, min_dist=0.1\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_20_01_35, \"UMAP MST - Upper Limit - n_neighbors=20, min_dist=0.1\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=30 min_dist=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_30_01_35= np.load(f'umap_projections_30_01_35.npy')\n",
    "mean_projection_30_01_35= np.load(f'mean_projection_30_01_35.npy')\n",
    "std_projection_30_01_35= np.load(f'std_projection_30_01_35.npy')\n",
    "lower_limit_intconf_matrix_30_01_35= np.load(f'lower_limit_intconf_matrix_30_01_35.npy')\n",
    "upper_limit_intconf_matrix_30_01_35= np.load(f'upper_limit_intconf_matrix_30_01_35.npy')\n",
    "distance_matrices_30_01_35=np.load(f'distance_matrices_neighbors_30_01_35.npy')\n",
    "mean_distance_matrix_30_01_35=np.load(f'mean_distance_matrix_neighbors_30_01_35.npy')\n",
    "norm_lower_limit_intconf_matrix_30_01_35=np.load(f'norm_lower_limit_intconf_matrix_30_01_35.npy')\n",
    "norm_upper_limit_intconf_matrix_30_01_35=np.load(f'norm_upper_limit_intconf_matrix_30_01_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 30\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_30_01_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_30_01_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_30_01_35 = np.array(umap_projections_30_01_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_30_01_35 = np.mean(umap_projections_30_01_35, axis=0)\n",
    "std_projection_30_01_35 = np.std(umap_projections_30_01_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_30_01_35.npy', umap_projections_30_01_35)\n",
    "np.save('mean_projection_30_01_35.npy', mean_projection_30_01_35)\n",
    "np.save('std_projection_30_01_35.npy', std_projection_30_01_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_30_01_35'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_30_01_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_30_01 = np.zeros((n_runs, n_clusters, umap_projections_30_01_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_30_01_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_30_01[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_30_01 = np.zeros(10)\n",
    "std_dev_y_30_01 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_30_01[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_30_01[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_30_01[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_30_01[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_30_01)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_30_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_30[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_30[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_30[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_20_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{30_01}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_30_01_35[run]  # Shape: (n_samples, 2)\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_30_01_35 = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_30_01_35.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_30_01_35 = np.array(distance_matrices_30_01_35)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_30_01_35 = np.mean(distance_matrices_30_01_35, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_30_01_35 = (mean_distance_matrix_30_01_35 - np.min(mean_distance_matrix_30_01_35)) / (np.max(mean_distance_matrix_30_01_35) - np.min(mean_distance_matrix_30_01_35))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_30_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10,  n_neighbors=30, min_dist=0.1)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_30_01_35.npy', distance_matrices_30_01_35)\n",
    "np.save('mean_distance_matrix_neighbors_30_01_35.npy', mean_distance_matrix_30_01_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_30_01_35}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_30_01_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_30_01_35,3))\n",
    "np.save('G_30_01_35.npy',G_30_01_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_30_01_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_30_01_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_30_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_30_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_30_01_35 = nx.minimum_spanning_tree(G_30_01_35)\n",
    "np.save('mst_30_01_35.npy', mst_30_01_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_30_01_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_30_01_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_30_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_30_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=30, min_dist=0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_30_01_35 = np.std(distance_matrices_30_01_35, axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_30_01_35.npy\", distance_matrix_std_30_01_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (30_01_35):\\n\", distance_matrix_std_30_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_30_01_35 = distance_matrix_std_30_01_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_30_01_35 = z_score * sem_matrix_30_01_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_30_01_35 = mean_distance_matrix_30_01_35 - margin_of_error_matrix_30_01_35\n",
    "upper_limit_intconf_matrix_30_01_35 = mean_distance_matrix_30_01_35 + margin_of_error_matrix_30_01_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_30_01_35 = np.maximum(lower_limit_intconf_matrix_30_01_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_30_01_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_30_01_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_30_01_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_30_01_35.npy', lower_limit_intconf_matrix_30_01_35)\n",
    "np.save('upper_limit_intconf_matrix_30_01_35.npy', upper_limit_intconf_matrix_30_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_30_01_35 = normalize_matrix(lower_limit_intconf_matrix_30_01_35)\n",
    "norm_upper_limit_intconf_matrix_30_01_35 = normalize_matrix(upper_limit_intconf_matrix_30_01_35)\n",
    "np.save('norm_lower_limit_intconf_matrix_30_01_35.npy', norm_lower_limit_intconf_matrix_30_01_35)\n",
    "np.save('norm_upper_limit_intconf_matrix_30_01_35.npy', norm_upper_limit_intconf_matrix_30_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_30_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=30, min_dist=0.1)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_30_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=30, min_dist=0.1)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_30_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=30, min_dist=0.1)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_30_01_35, \"UMAP MST - Mean Distances - n_neighbors=30, min_dist=0.1\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_30_01_35, \"UMAP MST - Lower Limit - n_neighbors=30, min_dist=0.1\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_30_01_35, \"UMAP MST - Upper Limit - n_neighbors=30, min_dist=0.1\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=50 min_dist=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_50_01_35= np.load(f'umap_projections_50_01_35.npy')\n",
    "mean_projection_50_01_35= np.load(f'mean_projection_50_01_35.npy')\n",
    "std_projection_50_01_35= np.load(f'std_projection_50_01_35.npy')\n",
    "lower_limit_intconf_matrix_50_01_35= np.load(f'lower_limit_intconf_matrix_50_01_35.npy')\n",
    "upper_limit_intconf_matrix_50_01_35= np.load(f'upper_limit_intconf_matrix_50_01_35.npy')\n",
    "distance_matrices_50_01_35=np.load(f'distance_matrices_neighbors_50_01_35.npy')\n",
    "mean_distance_matrix_50_01_35=np.load(f'mean_distance_matrix_neighbors_50_01_35.npy')\n",
    "norm_lower_limit_intconf_matrix_50_01_35=np.load(f'norm_lower_limit_intconf_matrix_50_01_35.npy')\n",
    "norm_upper_limit_intconf_matrix_50_01_35=np.load(f'norm_upper_limit_intconf_matrix_50_01_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 50\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_50_01_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_50_01_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_50_01_35 = np.array(umap_projections_50_01_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_50_01_35 = np.mean(umap_projections_50_01_35, axis=0)\n",
    "std_projection_50_01_35 = np.std(umap_projections_50_01_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_50_01_35.npy', umap_projections_50_01_35)\n",
    "np.save('mean_projection_50_01_35.npy', mean_projection_50_01_35)\n",
    "np.save('std_projection_50_01_35.npy', std_projection_50_01_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_50_01_35'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_50_01_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_50_01 = np.zeros((n_runs, n_clusters, umap_projections_50_01_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_50_01_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_50_01[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_50_01 = np.zeros(10)\n",
    "std_dev_y_50_01 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_50_01[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_50_01[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_50_01[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_50_01[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_50_01)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_50_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_50[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_50[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_50[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_50_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{50_01}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_50_01_35[run]  # Shape: (n_samples, 2)\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_50_01_35 = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_50_01_35.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_50_01_35 = np.array(distance_matrices_50_01_35)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_50_01_35 = np.mean(distance_matrices_50_01_35, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_50_01_35 = (mean_distance_matrix_50_01_35 - np.min(mean_distance_matrix_50_01_35)) / (np.max(mean_distance_matrix_50_01_35) - np.min(mean_distance_matrix_50_01_35))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_50_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=50, min_dist=0.1)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_50_01_35.npy', distance_matrices_50_01_35)\n",
    "np.save('mean_distance_matrix_neighbors_50_01_35.npy', mean_distance_matrix_50_01_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_50_01_35}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_50_01_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_50_01_35,3))\n",
    "np.save('G_50_01_35.npy',G_50_01_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_50_01_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_50_01_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_50_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_50_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_50_01_35 = (mean_distance_matrix_50_01_35 - np.min(mean_distance_matrix_50_01_35)) / (np.max(mean_distance_matrix_50_01_35) - np.min(mean_distance_matrix_50_01_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total weight of the MST\n",
    "total_weight_50 = sum(nx.get_edge_attributes(mst_50_01_35, 'weight').values())\n",
    "\n",
    "# Print the total weight\n",
    "print(f\"Total weight of the MST: {total_weight_50}\")\n",
    "\n",
    "# Compute the minimum spanning tree of the graph\n",
    "mst_50_01_35 = nx.minimum_spanning_tree(G_50_01_35)\n",
    "np.save('mst_50_01_35.npy', mst_50_01_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_50_01_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_50_01_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_50_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_50_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=50, min_dist=0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_50_01_35 = np.std(distance_matrices_50_01_35, axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_50_01_35.npy\", distance_matrix_std_50_01_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (50_01_35):\\n\", distance_matrix_std_50_01_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_50_01_35 = distance_matrix_std_50_01_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_50_01_35 = z_score * sem_matrix_50_01_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_50_01_35 = mean_distance_matrix_50_01_35 - margin_of_error_matrix_50_01_35\n",
    "upper_limit_intconf_matrix_50_01_35 = mean_distance_matrix_50_01_35 + margin_of_error_matrix_50_01_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_50_01_35 = np.maximum(lower_limit_intconf_matrix_50_01_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_50_01_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_50_01_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_50_01_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_50_01_35.npy', lower_limit_intconf_matrix_50_01_35)\n",
    "np.save('upper_limit_intconf_matrix_50_01_35.npy', upper_limit_intconf_matrix_50_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_50_01_35 = normalize_matrix(lower_limit_intconf_matrix_50_01_35)\n",
    "norm_upper_limit_intconf_matrix_50_01_35 = normalize_matrix(upper_limit_intconf_matrix_50_01_35)\n",
    "np.save('norm_lower_limit_intconf_matrix_50_01_35.npy', norm_lower_limit_intconf_matrix_50_01_35)\n",
    "np.save('norm_upper_limit_intconf_matrix_50_01_35.npy', norm_upper_limit_intconf_matrix_50_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_50_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=50, min_dist=0.1)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_50_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=50, min_dist=0.1)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_50_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=50, min_dist=0.1)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_50_01_35, \"UMAP MST - Mean Distances - n_neighbors=50, min_dist=0.1\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_50_01_35, \"UMAP MST - Lower Limit - n_neighbors=50, min_dist=0.1\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_50_01_35, \"UMAP MST - Upper Limit - n_neighbors=50, min_dist=0.1\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=100 min_dist=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_100_01_35= np.load(f'umap_projections_100_01_35.npy')\n",
    "mean_projection_100_01_35= np.load(f'mean_projection_100_01_35.npy')\n",
    "std_projection_100_01_35= np.load(f'std_projection_100_01_35.npy')\n",
    "lower_limit_intconf_matrix_100_01_35= np.load(f'lower_limit_intconf_matrix_100_01_35.npy')\n",
    "upper_limit_intconf_matrix_100_01_35= np.load(f'upper_limit_intconf_matrix_100_01_35.npy')\n",
    "distance_matrices_100_01_35=np.load(f'distance_matrices_neighbors_100_01_35.npy')\n",
    "mean_distance_matrix_100_01_35=np.load(f'mean_distance_matrix_neighbors_100_01_35.npy')\n",
    "norm_lower_limit_intconf_matrix_100_01_35=np.load(f'norm_lower_limit_intconf_matrix_100_01_35.npy')\n",
    "norm_upper_limit_intconf_matrix_100_01_35=np.load(f'norm_upper_limit_intconf_matrix_100_01_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 100\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_100_01_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_100_01_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_100_01_35 = np.array(umap_projections_100_01_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_100_01_35 = np.mean(umap_projections_100_01_35, axis=0)\n",
    "std_projection_100_01_35 = np.std(umap_projections_100_01_35, axis=0)\n",
    "                                                                        \n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_100_01_35.npy', umap_projections_100_01_35)\n",
    "np.save('mean_projection_100_01_35.npy', mean_projection_100_01_35)\n",
    "np.save('std_projection_100_01_35.npy', std_projection_100_01_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_100_01_35'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_100_01_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_100_01 = np.zeros((n_runs, n_clusters, umap_projections_100_01_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_100_01_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_100_01[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_100_01 = np.zeros(10)\n",
    "std_dev_y_100_01 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_100_01[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_100_01[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_100_01[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_100_01[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_100_01)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_100_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_100[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_100[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_100[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_100_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{100_01}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance MAtrix Calacualtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_100_01_35[run]  # Shape: (n_samples, 2)\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_100_01_35 = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_100_01_35.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_100_01_35 = np.array(distance_matrices_100_01_35)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_100_01_35 = np.mean(distance_matrices_100_01_35, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_100_01_35 = (mean_distance_matrix_100_01_35 - np.min(mean_distance_matrix_100_01_35)) / (np.max(mean_distance_matrix_100_01_35) - np.min(mean_distance_matrix_100_01_35))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=100, min_dist=0.1)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_100_01_35.npy', distance_matrices_100_01_35)\n",
    "np.save('mean_distance_matrix_neighbors_100_01_35.npy', mean_distance_matrix_100_01_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_100_01_35}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_100_01_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_100_01_35,3))\n",
    "np.save('G_100_01_35.npy',G_100_01_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_100_01_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_100_01_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_100_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_100_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total weight of the MST\n",
    "total_weight_100 = sum(nx.get_edge_attributes(mst_100_01_35, 'weight').values())\n",
    "\n",
    "# Print the total weight\n",
    "print(f\"Total weight of the MST: {total_weight_100}\")\n",
    "\n",
    "# Compute the minimum spanning tree of the graph\n",
    "mst_100_01_35 = nx.minimum_spanning_tree(G_100_01_35)\n",
    "np.save('mst_100_01_35.npy', mst_100_01_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_100_01_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_100_01_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_100_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_100_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=100, min_dist=0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_100_01_35 = np.std(distance_matrices_100_01_35, axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_100_01_35.npy\", distance_matrix_std_100_01_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (100_01_35):\\n\", distance_matrix_std_100_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_100_01_35 = distance_matrix_std_100_01_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_100_01_35 = z_score * sem_matrix_100_01_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_100_01_35 = mean_distance_matrix_100_01_35 - margin_of_error_matrix_100_01_35\n",
    "upper_limit_intconf_matrix_100_01_35 = mean_distance_matrix_100_01_35 + margin_of_error_matrix_100_01_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_100_01_35 = np.maximum(lower_limit_intconf_matrix_100_01_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_100_01_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_100_01_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_100_01_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_100_01_35.npy', lower_limit_intconf_matrix_100_01_35)\n",
    "np.save('upper_limit_intconf_matrix_100_01_35.npy', upper_limit_intconf_matrix_100_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_100_01_35 = normalize_matrix(lower_limit_intconf_matrix_100_01_35)\n",
    "norm_upper_limit_intconf_matrix_100_01_35 = normalize_matrix(upper_limit_intconf_matrix_100_01_35)\n",
    "np.save('norm_lower_limit_intconf_matrix_100_01_35.npy', norm_lower_limit_intconf_matrix_100_01_35)\n",
    "np.save('norm_upper_limit_intconf_matrix_100_01_35.npy', norm_upper_limit_intconf_matrix_100_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_100_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=100, min_dist=0.1)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=100, min_dist=0.1)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_100_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=100, min_dist=0.1)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_100_01_35, \"UMAP MST - Mean Distances - n_neighbors=100, min_dist=0.1\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_100_01_35, \"UMAP MST - Lower Limit - n_neighbors=100, min_dist=0.1\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_100_01_35, \"UMAP MST - Upper Limit - n_neighbors=100, min_dist=0.1\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=5, min_dist=0.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_5_00125_35= np.load('umap_projections_5_00125_35.npy')\n",
    "mean_umap_projection_5_00125_35= np.load('mean_projection_5_00125_35.npy')\n",
    "std_projection_umap_5_00125_35= np.load('std_projection_5_00125_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 5\n",
    "min_dist = 0.0125\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_5_00125_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_5_00125_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_5_00125_35 = np.array(umap_projections_5_00125_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_5_00125_35 = np.mean(umap_projections_5_00125_35, axis=0)\n",
    "std_projection_5_00125_35 = np.std(umap_projections_5_00125_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_5_00125_35.npy', umap_projections_5_00125_35)\n",
    "np.save('mean_projection_5_00125_35.npy', mean_projection_5_00125_35)\n",
    "np.save('std_projection_5_00125_35.npy', std_projection_5_00125_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_5_00125_35'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_5_00125_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_5_00125 = np.zeros((n_runs, n_clusters, umap_projections_5_00125_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_5_00125_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_5_00125[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_5_00125 = np.zeros(10)\n",
    "std_dev_y_5_00125 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_5_00125[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_5_00125[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_5_00125[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_5_00125[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_5_00125)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_5_00125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_5_00125[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_5_00125[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_5_00125[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_5_00125_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{5_00125}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_5_00125_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_5_00125_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_5_00125_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_5_00125_35  = np.array(distance_matrices_5_00125_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_5_00125_35  = np.mean(distance_matrices_5_00125_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_5_00125_35  = (mean_distance_matrix_5_00125_35  - np.min(mean_distance_matrix_5_00125_35 )) / (np.max(mean_distance_matrix_5_00125_35 ) - np.min(mean_distance_matrix_5_00125_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=5, , min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_5_00125_35 .npy', distance_matrices_5_00125_35)\n",
    "np.save('mean_distance_matrix_neighbors_5_00125_35 .npy', mean_distance_matrix_5_00125_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_5_00125_35 }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_5_00125_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_5_00125_35 ,3))\n",
    "np.save('G_5_00125_35 .npy',G_5_00125_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_5_00125_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_5_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_5_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_5_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_5_00125_35  = nx.minimum_spanning_tree(G_5_00125_35 )\n",
    "np.save('mst_5_00125_35 .npy', mst_5_00125_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_5_00125_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_5_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_5_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_5_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=5, min_dist=0.0125\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_5_00125_35  = np.std(distance_matrices_5_00125_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_5_00125_35 .npy\", distance_matrix_std_5_00125_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_00125_35):\\n\", distance_matrix_std_5_00125_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_5_00125_35  = distance_matrix_std_5_00125_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_5_00125_35  = z_score * sem_matrix_5_00125_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_5_00125_35  = mean_distance_matrix_5_00125_35  - margin_of_error_matrix_5_00125_35 \n",
    "upper_limit_intconf_matrix_5_00125_35  = mean_distance_matrix_5_00125_35  + margin_of_error_matrix_5_00125_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_5_00125_35  = np.maximum(lower_limit_intconf_matrix_5_00125_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_5_00125_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_5_00125_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_5_00125_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_5_00125_35 .npy', lower_limit_intconf_matrix_5_00125_35 )\n",
    "np.save('upper_limit_intconf_matrix_5_00125_35 .npy', upper_limit_intconf_matrix_5_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_5_00125_35  = normalize_matrix(lower_limit_intconf_matrix_5_00125_35 )\n",
    "norm_upper_limit_intconf_matrix_5_00125_35  = normalize_matrix(upper_limit_intconf_matrix_5_00125_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_5_00125_35.npy', norm_lower_limit_intconf_matrix_5_00125_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_5_00125_35.npy', norm_upper_limit_intconf_matrix_5_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_5_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=5, min_dist=0.0125\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=5, min_dist=0.0125\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_5_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=5, min_dist=0.0125\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_5_00125_35 , \"UMAP MST - Mean Distances - n_neighbors=5, min_dist=0.0125\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_5_00125_35 , \"UMAP MST - Lower Limit - n_neighbors=5, min_dist=0.0125\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_5_00125_35 , \"UMAP MST - Upper Limit - n_neighbors=5, min_dist=0.0125\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=10, min_dist=0.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_10_00125_35= np.load('umap_projections_10_00125_35.npy')\n",
    "mean_umap_projection_10_00125_35= np.load('mean_projection_10_00125_35.npy')\n",
    "std_projection_umap_10_00125_35= np.load('std_projection_10_00125_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.0125\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_10_00125_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_10_00125_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_10_00125_35 = np.array(umap_projections_10_00125_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_10_00125_35 = np.mean(umap_projections_10_00125_35, axis=0)\n",
    "std_projection_10_00125_35 = np.std(umap_projections_10_00125_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_10_00125_35.npy', umap_projections_10_00125_35)\n",
    "np.save('mean_projection_10_00125_35.npy', mean_projection_10_00125_35)\n",
    "np.save('std_projection_10_00125_35.npy', std_projection_10_00125_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_10_00125_35'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_10_00125_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_10_00125 = np.zeros((n_runs, n_clusters, umap_projections_10_00125_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_10_00125_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_10_00125[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_10_00125 = np.zeros(10)\n",
    "std_dev_y_10_00125 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_10_00125[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_10_00125[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_10_00125[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_10_00125[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_10_00125)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_10_00125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_10_00125[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_10_00125[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_10_00125[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_10_00125_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{10_00125}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_10_00125_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_10_00125_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_10_00125_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_10_00125_35  = np.array(distance_matrices_10_00125_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_10_00125_35  = np.mean(distance_matrices_10_00125_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_10_00125_35  = (mean_distance_matrix_10_00125_35  - np.min(mean_distance_matrix_10_00125_35 )) / (np.max(mean_distance_matrix_10_00125_35 ) - np.min(mean_distance_matrix_10_00125_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=10, , min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_10_00125_35 .npy', distance_matrices_10_00125_35)\n",
    "np.save('mean_distance_matrix_neighbors_10_00125_35 .npy', mean_distance_matrix_10_00125_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_10_00125_35 }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_10_00125_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_10_00125_35 ,3))\n",
    "np.save('G_10_00125_35 .npy',G_10_00125_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_10_00125_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_10_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_10_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_10_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_10_00125_35  = nx.minimum_spanning_tree(G_10_00125_35 )\n",
    "np.save('mst_10_00125_35 .npy', mst_10_00125_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_10_00125_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_10_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_10_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_10_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=10, min_dist=0.0125\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_10_00125_35  = np.std(distance_matrices_10_00125_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_10_00125_35 .npy\", distance_matrix_std_10_00125_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_00125_35):\\n\", distance_matrix_std_10_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_10_00125_35  = distance_matrix_std_10_00125_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_10_00125_35  = z_score * sem_matrix_10_00125_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_10_00125_35  = mean_distance_matrix_10_00125_35  - margin_of_error_matrix_10_00125_35 \n",
    "upper_limit_intconf_matrix_10_00125_35  = mean_distance_matrix_10_00125_35  + margin_of_error_matrix_10_00125_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_10_00125_35  = np.maximum(lower_limit_intconf_matrix_10_00125_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_10_00125_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_10_00125_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_10_00125_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_10_00125_35 .npy', lower_limit_intconf_matrix_10_00125_35 )\n",
    "np.save('upper_limit_intconf_matrix_10_00125_35 .npy', upper_limit_intconf_matrix_10_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_10_00125_35  = normalize_matrix(lower_limit_intconf_matrix_10_00125_35 )\n",
    "norm_upper_limit_intconf_matrix_10_00125_35  = normalize_matrix(upper_limit_intconf_matrix_10_00125_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_10_00125_35.npy', norm_lower_limit_intconf_matrix_10_00125_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_10_00125_35.npy', norm_upper_limit_intconf_matrix_10_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_10_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=10, min_dist=0.0125\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=10, min_dist=0.0125\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_10_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=10, min_dist=0.0125\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_10_00125_35 , \"UMAP MST - Mean Distances - n_neighbors=10, min_dist=0.0125\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_10_00125_35 , \"UMAP MST - Lower Limit - n_neighbors=10, min_dist=0.0125\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_10_00125_35 , \"UMAP MST - Upper Limit - n_neighbors=10, min_dist=0.0125\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=20, min_dist=0.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_20_00125_35= np.load('umap_projections_20_00125_35.npy')\n",
    "mean_umap_projection_20_00125_35= np.load('mean_projection_20_00125_35.npy')\n",
    "std_projection_umap_20_00125_35= np.load('std_projection_20_00125_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 20\n",
    "min_dist = 0.0125\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_20_00125_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_20_00125_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_20_00125_35 = np.array(umap_projections_20_00125_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_20_00125_35 = np.mean(umap_projections_20_00125_35, axis=0)\n",
    "std_projection_20_00125_35 = np.std(umap_projections_20_00125_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_20_00125_35.npy', umap_projections_20_00125_35)\n",
    "np.save('mean_projection_20_00125_35.npy', mean_projection_20_00125_35)\n",
    "np.save('std_projection_20_00125_35.npy', std_projection_20_00125_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_20_00125_35'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_20_00125_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_20_00125 = np.zeros((n_runs, n_clusters, umap_projections_20_00125_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_20_00125_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_20_00125[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_20_00125 = np.zeros(10)\n",
    "std_dev_y_20_00125 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_20_00125[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_20_00125[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_20_00125[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_20_00125[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_20_00125)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_20_00125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_20_00125[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_20_00125[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_20_00125[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_20_00125_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{20_00125}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_20_00125_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_20_00125_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_20_00125_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_20_00125_35  = np.array(distance_matrices_20_00125_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_20_00125_35  = np.mean(distance_matrices_20_00125_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_20_00125_35  = (mean_distance_matrix_20_00125_35  - np.min(mean_distance_matrix_20_00125_35 )) / (np.max(mean_distance_matrix_20_00125_35 ) - np.min(mean_distance_matrix_20_00125_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_20_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=10, , min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_20_00125_35 .npy', distance_matrices_20_00125_35)\n",
    "np.save('mean_distance_matrix_neighbors_20_00125_35 .npy', mean_distance_matrix_20_00125_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_20_00125_35 }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_20_00125_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_20_00125_35 ,3))\n",
    "np.save('G_20_00125_35 .npy',G_20_00125_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_20_00125_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_20_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_20_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_20_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_20_00125_35  = nx.minimum_spanning_tree(G_20_00125_35 )\n",
    "np.save('mst_20_00125_35 .npy', mst_20_00125_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_20_00125_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_20_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_20_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_20_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=20, min_dist=0.0125\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_20_00125_35  = np.std(distance_matrices_20_00125_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_20_00125_35 .npy\", distance_matrix_std_20_00125_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_00125_35):\\n\", distance_matrix_std_20_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_20_00125_35  = distance_matrix_std_20_00125_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_20_00125_35  = z_score * sem_matrix_20_00125_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_20_00125_35  = mean_distance_matrix_20_00125_35  - margin_of_error_matrix_20_00125_35 \n",
    "upper_limit_intconf_matrix_20_00125_35  = mean_distance_matrix_20_00125_35  + margin_of_error_matrix_20_00125_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_20_00125_35  = np.maximum(lower_limit_intconf_matrix_20_00125_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_20_00125_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_20_00125_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_20_00125_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_20_00125_35 .npy', lower_limit_intconf_matrix_20_00125_35 )\n",
    "np.save('upper_limit_intconf_matrix_20_00125_35 .npy', upper_limit_intconf_matrix_20_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_20_00125_35  = normalize_matrix(lower_limit_intconf_matrix_20_00125_35 )\n",
    "norm_upper_limit_intconf_matrix_20_00125_35  = normalize_matrix(upper_limit_intconf_matrix_20_00125_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_20_00125_35.npy', norm_lower_limit_intconf_matrix_20_00125_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_20_00125_35.npy', norm_upper_limit_intconf_matrix_20_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_20_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=20, min_dist=0.0125\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_20_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=20, min_dist=0.0125\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_20_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=20, min_dist=0.0125\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_20_00125_35 , \"UMAP MST - Mean Distances - n_neighbors=20, min_dist=0.0125\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_20_00125_35 , \"UMAP MST - Lower Limit - n_neighbors=20, min_dist=0.0125\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_20_00125_35 , \"UMAP MST - Upper Limit - n_neighbors=20, min_dist=0.0125\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=30, min_dist=0.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_30_00125_35= np.load('umap_projections_30_00125_35.npy')\n",
    "mean_umap_projection_30_00125_35= np.load('mean_projection_30_00125_35.npy')\n",
    "std_projection_umap_30_00125_35= np.load('std_projection_30_00125_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 30\n",
    "min_dist = 0.0125\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_30_00125_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_30_00125_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_30_00125_35 = np.array(umap_projections_30_00125_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_30_00125_35 = np.mean(umap_projections_30_00125_35, axis=0)\n",
    "std_projection_30_00125_35 = np.std(umap_projections_30_00125_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_30_00125_35.npy', umap_projections_30_00125_35)\n",
    "np.save('mean_projection_30_00125_35.npy', mean_projection_30_00125_35)\n",
    "np.save('std_projection_30_00125_35.npy', std_projection_30_00125_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_30_00125_35'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_30_00125_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_30_00125 = np.zeros((n_runs, n_clusters, umap_projections_30_00125_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_30_00125_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_30_00125[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_30_00125 = np.zeros(10)\n",
    "std_dev_y_30_00125 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_30_00125[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_30_00125[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_30_00125[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_30_00125[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_30_00125)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_30_00125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_30_00125[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_30_00125[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_30_00125[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_30_00125_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{30_00125}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_30_00125_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_30_00125_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_30_00125_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_30_00125_35  = np.array(distance_matrices_30_00125_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_30_00125_35  = np.mean(distance_matrices_30_00125_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_30_00125_35  = (mean_distance_matrix_30_00125_35  - np.min(mean_distance_matrix_30_00125_35 )) / (np.max(mean_distance_matrix_30_00125_35 ) - np.min(mean_distance_matrix_30_00125_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_30_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=10, , min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_30_00125_35 .npy', distance_matrices_30_00125_35)\n",
    "np.save('mean_distance_matrix_neighbors_30_00125_35 .npy', mean_distance_matrix_30_00125_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_30_00125_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_30_00125_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_30_00125_35 ,3))\n",
    "np.save('G_30_00125_35 .npy',G_30_00125_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_30_00125_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_30_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_30_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_30_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_30_00125_35  = nx.minimum_spanning_tree(G_30_00125_35 )\n",
    "np.save('mst_30_00125_35 .npy', mst_30_00125_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_30_00125_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_30_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_30_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_30_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=30, min_dist=0.0125\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_30_00125_35  = np.std(distance_matrices_30_00125_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_30_00125_35 .npy\", distance_matrix_std_30_00125_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_00125_35):\\n\", distance_matrix_std_30_00125_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_30_00125_35  = distance_matrix_std_30_00125_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_30_00125_35  = z_score * sem_matrix_30_00125_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_30_00125_35  = mean_distance_matrix_30_00125_35  - margin_of_error_matrix_30_00125_35 \n",
    "upper_limit_intconf_matrix_30_00125_35  = mean_distance_matrix_30_00125_35  + margin_of_error_matrix_30_00125_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_30_00125_35  = np.maximum(lower_limit_intconf_matrix_30_00125_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_30_00125_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_30_00125_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_30_00125_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_30_00125_35 .npy', lower_limit_intconf_matrix_30_00125_35 )\n",
    "np.save('upper_limit_intconf_matrix_30_00125_35 .npy', upper_limit_intconf_matrix_30_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_30_00125_35  = normalize_matrix(lower_limit_intconf_matrix_30_00125_35 )\n",
    "norm_upper_limit_intconf_matrix_30_00125_35  = normalize_matrix(upper_limit_intconf_matrix_30_00125_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_30_00125_35.npy', norm_lower_limit_intconf_matrix_30_00125_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_30_00125_35.npy', norm_upper_limit_intconf_matrix_30_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_30_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=30, min_dist=0.0125\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_30_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=30, min_dist=0.0125\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_30_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=30, min_dist=0.0125\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_30_00125_35 , \"UMAP MST - Mean Distances - n_neighbors=30, min_dist=0.0125\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_30_00125_35 , \"UMAP MST - Lower Limit - n_neighbors=30, min_dist=0.0125\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_30_00125_35 , \"UMAP MST - Upper Limit - n_neighbors=30, min_dist=0.0125\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=50, min_dist=0.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_50_00125_35= np.load('umap_projections_50_00125_35.npy')\n",
    "mean_umap_projection_50_00125_35= np.load('mean_projection_50_00125_35.npy')\n",
    "std_projection_umap_50_00125_35= np.load('std_projection_50_00125_35.npy')\n",
    "distance_matrices_50_00125_35= np.load('distance_matrices_neighbors_50_00125_35 .npy')\n",
    "mean_distance_matrix_50_00125_35= np.load('mean_distance_matrix_neighbors_50_00125_35 .npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 50\n",
    "min_dist = 0.0125\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "    \n",
    "# Store UMAP projections for each run\n",
    "umap_projections_50_00125_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_50_00125_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_50_00125_35 = np.array(umap_projections_50_00125_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_50_00125_35 = np.mean(umap_projections_50_00125_35, axis=0)\n",
    "std_projection_50_00125_35 = np.std(umap_projections_50_00125_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_50_00125_35.npy', umap_projections_50_00125_35)\n",
    "np.save('mean_projection_50_00125_35.npy', mean_projection_50_00125_35)\n",
    "np.save('std_projection_50_00125_35.npy', std_projection_50_00125_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_50_00125_35'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_50_00125_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_50_00125 = np.zeros((n_runs, n_clusters, umap_projections_50_00125_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_50_00125_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_50_00125[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_50_00125 = np.zeros(10)\n",
    "std_dev_y_50_00125 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_50_00125[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_50_00125[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_50_00125[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_50_00125[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_50_00125)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_50_00125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_50_00125[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_50_00125[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_50_00125[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_50_00125_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{50_00125}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_50_00125_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_50_00125_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_50_00125_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_50_00125_35  = np.array(distance_matrices_50_00125_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_50_00125_35  = np.mean(distance_matrices_50_00125_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_50_00125_35  = (mean_distance_matrix_50_00125_35  - np.min(mean_distance_matrix_50_00125_35 )) / (np.max(mean_distance_matrix_50_00125_35 ) - np.min(mean_distance_matrix_50_00125_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_50_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=50, , min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_50_00125_35 .npy', distance_matrices_50_00125_35)\n",
    "np.save('mean_distance_matrix_neighbors_50_00125_35 .npy', mean_distance_matrix_50_00125_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_50_00125_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_50_00125_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_50_00125_35 ,3))\n",
    "np.save('G_50_00125_35 .npy',G_50_00125_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_50_00125_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_50_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_50_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_50_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total weight of the MST\n",
    "total_weight_50_00125 = sum(nx.get_edge_attributes(mst_50_00125_35, 'weight').values())\n",
    "\n",
    "# Print the total weight\n",
    "print(f\"Total weight of the MST: {total_weight_50_00125}\")\n",
    "\n",
    "# Compute the minimum spanning tree of the graph\n",
    "mst_50_00125_35  = nx.minimum_spanning_tree(G_50_00125_35 )\n",
    "np.save('mst_50_00125_35 .npy', mst_50_00125_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_50_00125_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_50_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_50_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_50_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=50, min_dist=0.0125\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_50_00125_35  = np.std(distance_matrices_50_00125_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_50_00125_35 .npy\", distance_matrix_std_50_00125_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_00125_35):\\n\", distance_matrix_std_50_00125_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_50_00125_35  = distance_matrix_std_50_00125_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_50_00125_35  = z_score * sem_matrix_50_00125_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_50_00125_35  = mean_distance_matrix_50_00125_35  - margin_of_error_matrix_50_00125_35 \n",
    "upper_limit_intconf_matrix_50_00125_35  = mean_distance_matrix_50_00125_35  + margin_of_error_matrix_50_00125_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_50_00125_35  = np.maximum(lower_limit_intconf_matrix_50_00125_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_50_00125_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_50_00125_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_50_00125_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_50_00125_35 .npy', lower_limit_intconf_matrix_50_00125_35 )\n",
    "np.save('upper_limit_intconf_matrix_50_00125_35 .npy', upper_limit_intconf_matrix_50_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_50_00125_35  = normalize_matrix(lower_limit_intconf_matrix_50_00125_35 )\n",
    "norm_upper_limit_intconf_matrix_50_00125_35  = normalize_matrix(upper_limit_intconf_matrix_50_00125_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_50_00125_35.npy', norm_lower_limit_intconf_matrix_50_00125_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_50_00125_35.npy', norm_upper_limit_intconf_matrix_50_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_50_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=50, min_dist=0.0125\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_50_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=50, min_dist=0.0125\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_50_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=50, min_dist=0.0125\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_50_00125_35 , \"UMAP MST - Mean Distances - n_neighbors=50, min_dist=0.0125\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_50_00125_35 , \"UMAP MST - Lower Limit - n_neighbors=50, min_dist=0.0125\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_50_00125_35 , \"UMAP MST - Upper Limit - n_neighbors=50, min_dist=0.0125\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=100, min_dist=0.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_100_00125_35= np.load('umap_projections_100_00125_35.npy')\n",
    "mean_umap_projection_100_00125_35= np.load('mean_projection_100_00125_35.npy')\n",
    "std_projection_umap_100_00125_35= np.load('std_projection_100_00125_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 100\n",
    "min_dist = 0.0125\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_100_00125_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_100_00125_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_100_00125_35 = np.array(umap_projections_100_00125_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_100_00125_35 = np.mean(umap_projections_100_00125_35, axis=0)\n",
    "std_projection_100_00125_35 = np.std(umap_projections_100_00125_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_100_00125_35.npy', umap_projections_100_00125_35)\n",
    "np.save('mean_projection_100_00125_35.npy', mean_projection_100_00125_35)\n",
    "np.save('std_projection_100_00125_35.npy', std_projection_100_00125_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_100_00125_35'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs\n",
    "n_runs = umap_projections_100_00125_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_100_00125 = np.zeros((n_runs, n_clusters, umap_projections_100_00125_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_100_00125_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_100_00125[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_100_00125 = np.zeros(10)\n",
    "std_dev_y_100_00125 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_100_00125[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_100_00125[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_100_00125[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_100_00125[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_100_00125)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_100_00125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_100_00125[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_100_00125[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_100_00125[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_100_00125_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{100_00125}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_100_00125_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_100_00125_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_100_00125_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_100_00125_35  = np.array(distance_matrices_100_00125_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_100_00125_35  = np.mean(distance_matrices_100_00125_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_100_00125_35  = (mean_distance_matrix_100_00125_35  - np.min(mean_distance_matrix_100_00125_35 )) / (np.max(mean_distance_matrix_100_00125_35 ) - np.min(mean_distance_matrix_100_00125_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=100, min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_100_00125_35 .npy', distance_matrices_100_00125_35)\n",
    "np.save('mean_distance_matrix_neighbors_100_00125_35 .npy', mean_distance_matrix_100_00125_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_100_00125_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_100_00125_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_100_00125_35 ,3))\n",
    "np.save('G_100_00125_35 .npy',G_100_00125_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_100_00125_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_100_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_100_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_100_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_100_00125_35  = nx.minimum_spanning_tree(G_100_00125_35 )\n",
    "np.save('mst_100_00125_35 .npy', mst_100_00125_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_100_00125_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_100_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_100_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_100_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=100, min_dist=0.0125\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_100_00125_35  = np.std(distance_matrices_100_00125_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_100_00125_35 .npy\", distance_matrix_std_100_00125_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_00125_35):\\n\", distance_matrix_std_100_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_100_00125_35  = distance_matrix_std_100_00125_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_100_00125_35  = z_score * sem_matrix_100_00125_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_100_00125_35  = mean_distance_matrix_100_00125_35  - margin_of_error_matrix_100_00125_35 \n",
    "upper_limit_intconf_matrix_100_00125_35  = mean_distance_matrix_100_00125_35  + margin_of_error_matrix_100_00125_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_100_00125_35  = np.maximum(lower_limit_intconf_matrix_100_00125_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_100_00125_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_100_00125_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_100_00125_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_100_00125_35 .npy', lower_limit_intconf_matrix_100_00125_35 )\n",
    "np.save('upper_limit_intconf_matrix_100_00125_35 .npy', upper_limit_intconf_matrix_100_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_100_00125_35  = normalize_matrix(lower_limit_intconf_matrix_100_00125_35 )\n",
    "norm_upper_limit_intconf_matrix_100_00125_35  = normalize_matrix(upper_limit_intconf_matrix_100_00125_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_100_00125_35.npy', norm_lower_limit_intconf_matrix_100_00125_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_100_00125_35.npy', norm_upper_limit_intconf_matrix_100_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_100_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=100, min_dist=0.0125\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=100, min_dist=0.0125\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_100_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=100, min_dist=0.0125\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_100_00125_35 , \"UMAP MST - Mean Distances - n_neighbors=100, min_dist=0.0125\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_100_00125_35 , \"UMAP MST - Lower Limit - n_neighbors=100, min_dist=0.0125\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_100_00125_35 , \"UMAP MST - Upper Limit - n_neighbors=100, min_dist=0.0125\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptable Radius for min_dist= 0.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR min_dist=0,0125.\n",
    "# Function to calculate cluster metrics\n",
    "def calculate_cluster_metrics(umap_projections, y_labels, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Calculate average cluster radii and neighbor counts for each cluster over all runs.\n",
    "    \"\"\"\n",
    "    n_runs = len(umap_projections)  # Number of runs\n",
    "    cluster_centers_full = []\n",
    "    \n",
    "    # Step 1: Calculate cluster centers for each run\n",
    "    for run_idx, x_umap in enumerate(umap_projections):\n",
    "        cluster_centers_run = []\n",
    "        for label in np.unique(y_labels):\n",
    "            cluster_points = x_umap[y_labels == label]\n",
    "            if len(cluster_points) > 0:\n",
    "                cluster_center = np.mean(cluster_points, axis=0)\n",
    "                cluster_centers_run.append(cluster_center)\n",
    "        cluster_centers_full.append(np.array(cluster_centers_run))\n",
    "    \n",
    "    cluster_centers_full = np.array(cluster_centers_full)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "    # Step 2: Calculate average radii for each cluster\n",
    "    radii_per_cluster = []\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        radii_cluster = []\n",
    "        for run_idx, x_umap in enumerate(umap_projections):\n",
    "            cluster_center = cluster_centers_full[run_idx][cluster_idx]\n",
    "            cluster_points = x_umap[y_labels == cluster_idx]\n",
    "            if len(cluster_points) > 0:\n",
    "                distances_to_center = np.linalg.norm(cluster_points - cluster_center, axis=1)\n",
    "                dynamic_radius = np.mean(distances_to_center)  # Mean distance to center\n",
    "                radii_cluster.append(dynamic_radius)\n",
    "        radii_per_cluster.append(np.mean(radii_cluster))  # Average radius across runs\n",
    "\n",
    "    # Step 3: Calculate neighbor counts for each cluster\n",
    "    neighbor_counts_full = []\n",
    "    for run_idx, x_umap in enumerate(umap_projections):\n",
    "        counts_run = []\n",
    "        for cluster_idx, cluster_center in enumerate(cluster_centers_full[run_idx]):\n",
    "            radius = radii_per_cluster[cluster_idx]  # Use the average radius\n",
    "            distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "            count = np.sum(distances_to_center <= radius)  # Count points within the radius\n",
    "            counts_run.append(count)\n",
    "        neighbor_counts_full.append(counts_run)\n",
    "\n",
    "    neighbor_counts_full = np.array(neighbor_counts_full)  # Shape: (n_runs, n_clusters)\n",
    "    average_neighbor_counts = np.mean(neighbor_counts_full, axis=0)  # Average across runs\n",
    "\n",
    "    return radii_per_cluster, average_neighbor_counts\n",
    "\n",
    "# Define n_neighbors values\n",
    "n_neighbors_values = [5, 10, 20, 30, 50, 100]\n",
    "results = []\n",
    "\n",
    "# Iterate over each n_neighbors value\n",
    "for n_neighbors in n_neighbors_values:\n",
    "    if n_neighbors == 5:\n",
    "        umap_projections = umap_projections_5_00125_35\n",
    "    elif n_neighbors == 10:\n",
    "        umap_projections = umap_projections_10_00125_35\n",
    "    elif n_neighbors == 20:\n",
    "        umap_projections = umap_projections_20_00125_35\n",
    "    elif n_neighbors == 30:\n",
    "        umap_projections = umap_projections_30_00125_35\n",
    "    elif n_neighbors == 50:\n",
    "        umap_projections = umap_projections_50_00125_35\n",
    "    elif n_neighbors == 100:\n",
    "        umap_projections = umap_projections_100_00125_35\n",
    "\n",
    "    # Calculate metrics\n",
    "    radii_per_cluster, average_neighbor_counts = calculate_cluster_metrics(umap_projections, y_train)\n",
    "\n",
    "    # Store results\n",
    "    for cluster_idx in range(len(radii_per_cluster)):\n",
    "        results.append({\n",
    "            \"N\": n_neighbors,\n",
    "            \"Cluster\": cluster_idx,\n",
    "            \"Radius\": np.round(radii_per_cluster[cluster_idx], 3),\n",
    "            \"Number of Neighbors\": np.round(average_neighbor_counts[cluster_idx], 0)\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save results for later use\n",
    "df_results.to_csv(\"radius_neighbor_analysis_merged_MinDist_00125.csv\", index=False)\n",
    "\n",
    "# Pivot table for easy visualization\n",
    "pivot_table = df_results.pivot(index=\"Cluster\", columns=\"N\", values=[\"Radius\", \"Number of Neighbors\"])\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_neighbor_counts_across_runs(umap_projections_list, n_neighbors_values, y_labels, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Plot mean number of neighbors across runs for different n_neighbors values.\n",
    "    \"\"\"\n",
    "    neighbor_counts_avg_runs = []\n",
    "\n",
    "    for umap_projections in umap_projections_list:\n",
    "        # Calculate neighbor counts for each run\n",
    "        neighbor_counts_per_run = []\n",
    "        for run_idx, x_umap in enumerate(umap_projections):\n",
    "            cluster_centers = []\n",
    "            radii_per_cluster = []\n",
    "            for cluster_idx in range(n_clusters):\n",
    "                # Compute cluster center\n",
    "                cluster_points = x_umap[y_labels == cluster_idx]\n",
    "                if len(cluster_points) > 0:\n",
    "                    cluster_center = np.mean(cluster_points, axis=0)\n",
    "                    cluster_centers.append(cluster_center)\n",
    "\n",
    "                    # Compute dynamic radius for this cluster\n",
    "                    distances_to_center = np.linalg.norm(cluster_points - cluster_center, axis=1)\n",
    "                    dynamic_radius = np.mean(distances_to_center)\n",
    "                    radii_per_cluster.append(dynamic_radius)\n",
    "                else:\n",
    "                    radii_per_cluster.append(0)\n",
    "                    cluster_centers.append(np.array([0, 0]))\n",
    "\n",
    "            # Compute number of neighbors within radius for each cluster\n",
    "            neighbor_counts = []\n",
    "            for cluster_idx, cluster_center in enumerate(cluster_centers):\n",
    "                if radii_per_cluster[cluster_idx] > 0:  # Avoid empty clusters\n",
    "                    distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "                    neighbor_count = np.sum(distances_to_center <= radii_per_cluster[cluster_idx])\n",
    "                    neighbor_counts.append(neighbor_count)\n",
    "\n",
    "            # Store the mean neighbor count for this run\n",
    "            neighbor_counts_per_run.append(np.mean(neighbor_counts))\n",
    "        \n",
    "        neighbor_counts_avg_runs.append(neighbor_counts_per_run)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, counts in enumerate(neighbor_counts_avg_runs):\n",
    "        plt.plot(range(1, len(counts) + 1), counts, label=f'n_neighbors={n_neighbors_values[i]}', marker='o')\n",
    "\n",
    "    plt.xlabel(\"Run Index\")\n",
    "    plt.ylabel(\"Mean Number of Points\")\n",
    "    plt.title(\"Mean Number of Points Across Runs for min_dist = 0.0125\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))  # Adjust legend position\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_neighbor_counts_across_runs(\n",
    "    umap_projections_list=[\n",
    "        umap_projections_5_00125_35,\n",
    "        umap_projections_10_00125_35,\n",
    "        umap_projections_20_00125_35,\n",
    "        umap_projections_30_00125_35,\n",
    "        umap_projections_50_00125_35,\n",
    "        umap_projections_100_00125_35\n",
    "    ],\n",
    "    n_neighbors_values=n_neighbors_values,\n",
    "    y_labels=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading .csv \n",
    "df_results_00125 = pd.read_csv('radius_neighbor_analysis_merged_MinDist_00125.csv')\n",
    "\n",
    "# Add a density column to df_results\n",
    "df_results_00125['Density'] = df_results_00125['Number of Neighbors'] / df_results_00125['Radius']\n",
    "\n",
    "# Find the row with the maximum density\n",
    "max_density_row = df_results_00125.loc[df_results_00125['Density'].idxmax()]\n",
    "\n",
    "# Extract the cluster, n_neighbors, and maximum density\n",
    "max_density = max_density_row['Density']\n",
    "max_cluster = max_density_row['Cluster']\n",
    "max_n_neighbors = max_density_row['N']\n",
    "\n",
    "# Print the results\n",
    "print(f\"Highest Density: {max_density:.2f}\")\n",
    "print(f\"Cluster: {int(max_cluster)}\")\n",
    "print(f\"n_neighbors: {int(max_n_neighbors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=5, min_dist=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_5_08_35= np.load('umap_projections_5_08_35.npy')\n",
    "mean_umap_projection_5_08_35= np.load('mean_projection_5_08_35.npy')\n",
    "std_projection_umap_5_08_35= np.load('std_projection_5_08_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 5\n",
    "min_dist = 0.8\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_5_08_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_5_08_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_5_08_35 = np.array(umap_projections_5_08_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_5_08_35 = np.mean(umap_projections_5_08_35, axis=0)\n",
    "std_projection_5_08_35 = np.std(umap_projections_5_08_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_5_08_35.npy', umap_projections_5_08_35)\n",
    "np.save('mean_projection_5_08_35.npy', mean_projection_5_08_35)\n",
    "np.save('std_projection_5_08_35.npy', std_projection_5_08_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_5_08_35'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_5_08_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_5_08 = np.zeros((n_runs, n_clusters, umap_projections_5_08_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_5_08_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_5_08[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_5_08 = np.zeros(10)\n",
    "std_dev_y_5_08 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_5_08[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_5_08[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_5_08[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_5_08[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_5_08)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_5_08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_5_08[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_5_08[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_5_08[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_5_08_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{5_08}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix Calacualtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_5_08_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_5_08_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_5_08_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_5_08_35  = np.array(distance_matrices_5_08_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_5_08_35  = np.mean(distance_matrices_5_08_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_5_08_35  = (mean_distance_matrix_5_08_35  - np.min(mean_distance_matrix_5_08_35 )) / (np.max(mean_distance_matrix_5_08_35 ) - np.min(mean_distance_matrix_5_08_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=100, min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_5_08_35 .npy', distance_matrices_5_08_35)\n",
    "np.save('mean_distance_matrix_neighbors_5_08_35 .npy', mean_distance_matrix_5_08_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_5_08_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_5_08_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_5_08_35 ,3))\n",
    "np.save('G_5_08_35 .npy',G_5_08_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_5_08_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_5_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_5_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_5_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_5_08_35  = nx.minimum_spanning_tree(G_5_08_35 )\n",
    "np.save('mst_5_08_35 .npy', mst_5_08_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_5_08_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_5_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_5_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_5_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=5, min_dist=0.8\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_5_08_35  = np.std(distance_matrices_5_08_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_5_08_35 .npy\", distance_matrix_std_5_08_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_00125_35):\\n\", distance_matrix_std_5_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_5_08_35  = distance_matrix_std_5_08_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_5_08_35  = z_score * sem_matrix_5_08_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_5_08_35  = mean_distance_matrix_5_08_35  - margin_of_error_matrix_5_08_35 \n",
    "upper_limit_intconf_matrix_5_08_35  = mean_distance_matrix_5_08_35  + margin_of_error_matrix_5_08_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_5_08_35  = np.maximum(lower_limit_intconf_matrix_5_08_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_5_08_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_5_08_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_5_08_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_5_08_35 .npy', lower_limit_intconf_matrix_5_08_35 )\n",
    "np.save('upper_limit_intconf_matrix_5_08_35 .npy', upper_limit_intconf_matrix_5_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_5_08_35  = normalize_matrix(lower_limit_intconf_matrix_5_08_35 )\n",
    "norm_upper_limit_intconf_matrix_5_08_35  = normalize_matrix(upper_limit_intconf_matrix_5_08_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_5_08_35.npy', norm_lower_limit_intconf_matrix_5_08_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_5_08_35.npy', norm_upper_limit_intconf_matrix_5_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_5_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=5, min_dist=0.8\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=5, min_dist=0.8\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_5_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=5, min_dist=0.8\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_5_08_35 , \"UMAP MST - Mean Distances - n_neighbors=5, min_dist=0.8\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_5_08_35 , \"UMAP MST - Lower Limit - n_neighbors=5, min_dist=0.8\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_5_08_35 , \"UMAP MST - Upper Limit - n_neighbors=5, min_dist=0.8\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=10, min_dist=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_10_08_35= np.load('umap_projections_10_08_35.npy')\n",
    "mean_umap_projection_10_08_35= np.load('mean_projection_10_08_35.npy')\n",
    "std_projection_umap_10_08_35= np.load('std_projection_10_08_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.8\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_10_08_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_10_08_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_10_08_35 = np.array(umap_projections_10_08_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_10_08_35 = np.mean(umap_projections_10_08_35, axis=0)\n",
    "std_projection_10_08_35 = np.std(umap_projections_10_08_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_10_08_35.npy', umap_projections_10_08_35)\n",
    "np.save('mean_projection_10_08_35.npy', mean_projection_10_08_35)\n",
    "np.save('std_projection_10_08_35.npy', std_projection_10_08_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_10_08_35'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_10_08_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_10_08 = np.zeros((n_runs, n_clusters, umap_projections_10_08_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_10_08_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_10_08[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_10_08 = np.zeros(10)\n",
    "std_dev_y_10_08 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_10_08[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_10_08[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_10_08[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_10_08[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_10_08)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_10_08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_10_08[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_10_08[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_10_08[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_10_08_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{10_08}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix Calacualtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_10_08_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_10_08_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_10_08_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_10_08_35  = np.array(distance_matrices_10_08_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_10_08_35  = np.mean(distance_matrices_10_08_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_10_08_35  = (mean_distance_matrix_10_08_35  - np.min(mean_distance_matrix_10_08_35 )) / (np.max(mean_distance_matrix_10_08_35 ) - np.min(mean_distance_matrix_10_08_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=10, min_dists=0.8)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_10_08_35 .npy', distance_matrices_10_08_35)\n",
    "np.save('mean_distance_matrix_neighbors_10_08_35 .npy', mean_distance_matrix_10_08_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_10_08_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_10_08_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_10_08_35 ,3))\n",
    "np.save('G_10_08_35 .npy',G_10_08_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_10_08_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_10_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_10_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_10_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_10_08_35  = nx.minimum_spanning_tree(G_10_08_35 )\n",
    "np.save('mst_10_08_35 .npy', mst_10_08_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_10_08_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_10_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_10_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_10_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=10, min_dist=0.8\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_10_08_35  = np.std(distance_matrices_10_08_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_10_08_35 .npy\", distance_matrix_std_10_08_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (10_08_35):\\n\", distance_matrix_std_10_08_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_10_08_35  = distance_matrix_std_10_08_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_10_08_35  = z_score * sem_matrix_10_08_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_10_08_35  = mean_distance_matrix_10_08_35  - margin_of_error_matrix_10_08_35 \n",
    "upper_limit_intconf_matrix_10_08_35  = mean_distance_matrix_10_08_35  + margin_of_error_matrix_10_08_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_10_08_35  = np.maximum(lower_limit_intconf_matrix_10_08_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_10_08_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_10_08_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_10_08_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_10_08_35 .npy', lower_limit_intconf_matrix_10_08_35 )\n",
    "np.save('upper_limit_intconf_matrix_10_08_35 .npy', upper_limit_intconf_matrix_10_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_10_08_35  = normalize_matrix(lower_limit_intconf_matrix_10_08_35 )\n",
    "norm_upper_limit_intconf_matrix_10_08_35  = normalize_matrix(upper_limit_intconf_matrix_10_08_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_10_08_35.npy', norm_lower_limit_intconf_matrix_10_08_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_10_08_35.npy', norm_upper_limit_intconf_matrix_10_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_10_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=10, min_dist=0.8\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=10, min_dist=0.8\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_10_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=10, min_dist=0.8\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_10_08_35 , \"UMAP MST - Mean Distances - n_neighbors=10, min_dist=0.8\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_10_08_35 , \"UMAP MST - Lower Limit - n_neighbors=10, min_dist=0.8\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_10_08_35 , \"UMAP MST - Upper Limit - n_neighbors=10, min_dist=0.8\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=20, min_dist=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_20_08_35= np.load('umap_projections_20_08_35.npy')\n",
    "mean_umap_projection_20_08_35= np.load('mean_projection_20_08_35.npy')\n",
    "std_projection_umap_20_08_35= np.load('std_projection_20_08_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 20\n",
    "min_dist = 0.8\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_20_08_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_20_08_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_20_08_35 = np.array(umap_projections_20_08_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_20_08_35 = np.mean(umap_projections_20_08_35, axis=0)\n",
    "std_projection_20_08_35 = np.std(umap_projections_20_08_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_20_08_35.npy', umap_projections_20_08_35)\n",
    "np.save('mean_projection_20_08_35.npy', mean_projection_20_08_35)\n",
    "np.save('std_projection_20_08_35.npy', std_projection_20_08_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_20_08_35'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_20_08_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_20_08 = np.zeros((n_runs, n_clusters, umap_projections_20_08_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_20_08_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_20_08[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_20_08 = np.zeros(10)\n",
    "std_dev_y_20_08 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_20_08[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_20_08[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_20_08[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_20_08[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_20_08)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_20_08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_20_08[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_20_08[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_20_08[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_20_08_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{20_08}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix Calacualtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_20_08_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_20_08_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_20_08_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_20_08_35  = np.array(distance_matrices_20_08_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_20_08_35  = np.mean(distance_matrices_20_08_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_20_08_35  = (mean_distance_matrix_20_08_35  - np.min(mean_distance_matrix_20_08_35 )) / (np.max(mean_distance_matrix_20_08_35 ) - np.min(mean_distance_matrix_20_08_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_20_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=100, min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_20_08_35 .npy', distance_matrices_20_08_35)\n",
    "np.save('mean_distance_matrix_neighbors_20_08_35 .npy', mean_distance_matrix_20_08_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_20_08_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_20_08_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_20_08_35 ,3))\n",
    "np.save('G_20_08_35 .npy',G_20_08_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_20_08_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_20_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_20_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_20_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_20_08_35  = nx.minimum_spanning_tree(G_20_08_35 )\n",
    "np.save('mst_20_08_35 .npy', mst_20_08_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_20_08_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_20_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_20_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_20_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=20, min_dist=0.8\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_20_08_35  = np.std(distance_matrices_20_08_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_20_08_35 .npy\", distance_matrix_std_20_08_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (20_08_35):\\n\", distance_matrix_std_20_08_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_20_08_35  = distance_matrix_std_20_08_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_20_08_35  = z_score * sem_matrix_20_08_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_20_08_35  = mean_distance_matrix_20_08_35  - margin_of_error_matrix_20_08_35 \n",
    "upper_limit_intconf_matrix_20_08_35  = mean_distance_matrix_20_08_35  + margin_of_error_matrix_20_08_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_20_08_35  = np.maximum(lower_limit_intconf_matrix_20_08_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_20_08_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_20_08_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_20_08_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_20_08_35 .npy', lower_limit_intconf_matrix_20_08_35 )\n",
    "np.save('upper_limit_intconf_matrix_20_08_35 .npy', upper_limit_intconf_matrix_20_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_20_08_35  = normalize_matrix(lower_limit_intconf_matrix_20_08_35 )\n",
    "norm_upper_limit_intconf_matrix_20_08_35  = normalize_matrix(upper_limit_intconf_matrix_20_08_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_20_08_35.npy', norm_lower_limit_intconf_matrix_20_08_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_20_08_35.npy', norm_upper_limit_intconf_matrix_20_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_20_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=20, min_dist=0.8\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_20_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=20, min_dist=0.8\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_20_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=20, min_dist=0.8\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_20_08_35 , \"UMAP MST - Mean Distances - n_neighbors=20, min_dist=0.8\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_20_08_35 , \"UMAP MST - Lower Limit - n_neighbors=20, min_dist=0.8\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_20_08_35 , \"UMAP MST - Upper Limit - n_neighbors=20, min_dist=0.8\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=30, min_dist=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_30_08_35= np.load('umap_projections_30_08_35.npy')\n",
    "mean_umap_projection_30_08_35= np.load('mean_projection_30_08_35.npy')\n",
    "std_projection_umap_30_08_35= np.load('std_projection_30_08_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 30\n",
    "min_dist = 0.8\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_30_08_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_30_08_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_30_08_35 = np.array(umap_projections_30_08_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_30_08_35 = np.mean(umap_projections_30_08_35, axis=0)\n",
    "std_projection_30_08_35 = np.std(umap_projections_30_08_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_30_08_35.npy', umap_projections_30_08_35)\n",
    "np.save('mean_projection_30_08_35.npy', mean_projection_30_08_35)\n",
    "np.save('std_projection_30_08_35.npy', std_projection_30_08_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_30_08_35'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_30_08_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_30_08 = np.zeros((n_runs, n_clusters, umap_projections_30_08_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_30_08_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_30_08[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_30_08 = np.zeros(10)\n",
    "std_dev_y_30_08 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_30_08[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_30_08[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_30_08[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_30_08[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_30_08)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_30_08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_30_08[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_30_08[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_30_08[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_30_08_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{30_08}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix Calacualtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_30_08_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_30_08_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_30_08_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_30_08_35  = np.array(distance_matrices_30_08_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_30_08_35  = np.mean(distance_matrices_30_08_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_30_08_35  = (mean_distance_matrix_30_08_35  - np.min(mean_distance_matrix_30_08_35 )) / (np.max(mean_distance_matrix_30_08_35 ) - np.min(mean_distance_matrix_30_08_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_30_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=30, min_dists=0.8)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_30_08_35 .npy', distance_matrices_30_08_35)\n",
    "np.save('mean_distance_matrix_neighbors_30_08_35 .npy', mean_distance_matrix_30_08_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_30_08_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_30_08_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_30_08_35 ,3))\n",
    "np.save('G_30_08_35 .npy',G_30_08_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_30_08_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_30_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_30_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_30_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_30_08_35  = nx.minimum_spanning_tree(G_30_08_35 )\n",
    "np.save('mst_30_08_35 .npy', mst_30_08_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_30_08_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_30_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_30_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_30_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=30, min_dist=0.8\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_30_08_35  = np.std(distance_matrices_30_08_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_30_08_35 .npy\", distance_matrix_std_30_08_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (30_08_35):\\n\", distance_matrix_std_30_08_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_30_08_35  = distance_matrix_std_30_08_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_30_08_35  = z_score * sem_matrix_30_08_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_30_08_35  = mean_distance_matrix_30_08_35  - margin_of_error_matrix_30_08_35 \n",
    "upper_limit_intconf_matrix_30_08_35  = mean_distance_matrix_30_08_35  + margin_of_error_matrix_30_08_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_30_08_35  = np.maximum(lower_limit_intconf_matrix_30_08_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_30_08_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_30_08_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_30_08_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_30_08_35 .npy', lower_limit_intconf_matrix_30_08_35 )\n",
    "np.save('upper_limit_intconf_matrix_30_08_35 .npy', upper_limit_intconf_matrix_30_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_30_08_35  = normalize_matrix(lower_limit_intconf_matrix_30_08_35 )\n",
    "norm_upper_limit_intconf_matrix_30_08_35  = normalize_matrix(upper_limit_intconf_matrix_30_08_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_30_08_35.npy', norm_lower_limit_intconf_matrix_30_08_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_30_08_35.npy', norm_upper_limit_intconf_matrix_30_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_30_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=30, min_dist=0.8\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_30_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=30, min_dist=0.8\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_30_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=30, min_dist=0.8\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_30_08_35 , \"UMAP MST - Mean Distances - n_neighbors=30, min_dist=0.8\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_30_08_35 , \"UMAP MST - Lower Limit - n_neighbors=30, min_dist=0.8\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_30_08_35 , \"UMAP MST - Upper Limit - n_neighbors=30, min_dist=0.8\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=50, min_dist=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_50_08_35= np.load('umap_projections_50_08_35.npy')\n",
    "mean_umap_projection_50_08_35= np.load('mean_projection_50_08_35.npy')\n",
    "std_projection_umap_50_08_35= np.load('std_projection_50_08_35.npy')\n",
    "distance_matrices_50_08_35= np.load('distance_matrices_neighbors_50_08_35 .npy')\n",
    "mean_distance_matrix_50_08_35= np.load('mean_distance_matrix_neighbors_50_08_35 .npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 50\n",
    "min_dist = 0.8\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_50_08_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_50_08_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_50_08_35 = np.array(umap_projections_50_08_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_50_08_35 = np.mean(umap_projections_50_08_35, axis=0)\n",
    "std_projection_50_08_35 = np.std(umap_projections_50_08_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_50_08_35.npy', umap_projections_50_08_35)\n",
    "np.save('mean_projection_50_08_35.npy', mean_projection_50_08_35)\n",
    "np.save('std_projection_50_08_35.npy', std_projection_50_08_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_50_08_35'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_50_08_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_50_08 = np.zeros((n_runs, n_clusters, umap_projections_50_08_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_50_08_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_50_08[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_50_08 = np.zeros(10)\n",
    "std_dev_y_50_08 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_50_08[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_50_08[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_50_08[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_50_08[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_50_08)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_50_08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_50_08[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_50_08[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_50_08[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_50_08_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{50_08}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix Calacualtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_50_08_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_50_08_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_50_08_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_50_08_35  = np.array(distance_matrices_50_08_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_50_08_35  = np.mean(distance_matrices_50_08_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_50_08_35  = (mean_distance_matrix_50_08_35  - np.min(mean_distance_matrix_50_08_35 )) / (np.max(mean_distance_matrix_50_08_35 ) - np.min(mean_distance_matrix_50_08_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_50_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=50, min_dists=0.8)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_50_08_35 .npy', distance_matrices_50_08_35)\n",
    "np.save('mean_distance_matrix_neighbors_50_08_35 .npy', mean_distance_matrix_50_08_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_50_08_35 }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_50_08_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_50_08_35 ,3))\n",
    "np.save('G_50_08_35 .npy',G_50_08_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_50_08_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_50_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_50_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_50_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total weight of the MST\n",
    "total_weight_50_08_35 = sum(nx.get_edge_attributes(mst_50_08_35, 'weight').values())\n",
    "\n",
    "# Print the total weight\n",
    "print(f\"Total weight of the MST: {total_weight_50_08_35}\")\n",
    "\n",
    "# Compute the minimum spanning tree of the graph\n",
    "mst_50_08_35  = nx.minimum_spanning_tree(G_50_08_35 )\n",
    "np.save('mst_50_08_35 .npy', mst_50_08_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_50_08_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_50_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_50_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_50_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=50, min_dist=0.8\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_50_08_35  = np.std(distance_matrices_50_08_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_50_08_35 .npy\", distance_matrix_std_50_08_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (50_08_35):\\n\", distance_matrix_std_50_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_50_08_35  = distance_matrix_std_50_08_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_50_08_35  = z_score * sem_matrix_50_08_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_50_08_35  = mean_distance_matrix_50_08_35  - margin_of_error_matrix_50_08_35 \n",
    "upper_limit_intconf_matrix_50_08_35  = mean_distance_matrix_50_08_35  + margin_of_error_matrix_50_08_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_50_08_35  = np.maximum(lower_limit_intconf_matrix_50_08_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_50_08_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_50_08_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_50_08_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_50_08_35 .npy', lower_limit_intconf_matrix_50_08_35 )\n",
    "np.save('upper_limit_intconf_matrix_50_08_35 .npy', upper_limit_intconf_matrix_50_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_50_08_35  = normalize_matrix(lower_limit_intconf_matrix_50_08_35 )\n",
    "norm_upper_limit_intconf_matrix_50_08_35  = normalize_matrix(upper_limit_intconf_matrix_50_08_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_50_08_35.npy', norm_lower_limit_intconf_matrix_50_08_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_50_08_35.npy', norm_upper_limit_intconf_matrix_50_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_50_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=50, min_dist=0.8\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_50_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=50, min_dist=0.8\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_50_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=50, min_dist=0.8\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_50_08_35 , \"UMAP MST - Mean Distances - n_neighbors=50, min_dist=0.8\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_50_08_35 , \"UMAP MST - Lower Limit - n_neighbors=50, min_dist=0.8\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_50_08_35 , \"UMAP MST - Upper Limit - n_neighbors=50, min_dist=0.8\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_neighbours=100, min_dist=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_100_08_35= np.load('umap_projections_100_08_35.npy')\n",
    "mean_umap_projection_100_08_35= np.load('mean_projection_100_08_35.npy')\n",
    "std_projection_umap_100_08_35= np.load('std_projection_100_08_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 100\n",
    "min_dist = 0.8\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_100_08_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_100_08_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_100_08_35 = np.array(umap_projections_100_08_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_100_08_35 = np.mean(umap_projections_100_08_35, axis=0)\n",
    "std_projection_100_08_35 = np.std(umap_projections_100_08_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_100_08_35.npy', umap_projections_100_08_35)\n",
    "np.save('mean_projection_100_08_35.npy', mean_projection_100_08_35)\n",
    "np.save('std_projection_100_08_35.npy', std_projection_100_08_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_100_08_35'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_100_08_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_100_08 = np.zeros((n_runs, n_clusters, umap_projections_100_08_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_100_08_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_100_08[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_100_08 = np.zeros(10)\n",
    "std_dev_y_100_08 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_100_08[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_100_08[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_100_08[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_100_08[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_100_08)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_100_08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_100_08[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_100_08[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_100_08[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_100_08_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{100_08}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Matrix Calacualtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_100_08_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_100_08_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_100_08_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_100_08_35  = np.array(distance_matrices_100_08_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_100_08_35  = np.mean(distance_matrices_100_08_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_100_08_35  = (mean_distance_matrix_100_08_35  - np.min(mean_distance_matrix_100_08_35 )) / (np.max(mean_distance_matrix_100_08_35 ) - np.min(mean_distance_matrix_100_08_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=100, min_dists=0.8)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_100_08_35 .npy', distance_matrices_100_08_35)\n",
    "np.save('mean_distance_matrix_neighbors_100_08_35 .npy', mean_distance_matrix_100_08_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_100_08_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_100_08_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_100_08_35 ,3))\n",
    "np.save('G_100_08_35 .npy',G_100_08_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_100_08_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_100_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_100_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_100_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Spanning Tree - MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_100_08_35  = nx.minimum_spanning_tree(G_100_08_35 )\n",
    "np.save('mst_100_08_35 .npy', mst_100_08_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_100_08_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_100_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_100_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_100_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=100, min_dist=0.8\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_100_08_35  = np.std(distance_matrices_100_08_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_100_08_35 .npy\", distance_matrix_std_100_08_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (100_08_35):\\n\", distance_matrix_std_100_08_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_100_08_35  = distance_matrix_std_100_08_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_100_08_35  = z_score * sem_matrix_100_08_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_100_08_35  = mean_distance_matrix_100_08_35  - margin_of_error_matrix_100_08_35 \n",
    "upper_limit_intconf_matrix_100_08_35  = mean_distance_matrix_100_08_35  + margin_of_error_matrix_100_08_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_100_08_35  = np.maximum(lower_limit_intconf_matrix_100_08_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_100_08_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_100_08_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_100_08_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_100_08_35 .npy', lower_limit_intconf_matrix_100_08_35 )\n",
    "np.save('upper_limit_intconf_matrix_100_08_35 .npy', upper_limit_intconf_matrix_100_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_100_08_35  = normalize_matrix(lower_limit_intconf_matrix_100_08_35 )\n",
    "norm_upper_limit_intconf_matrix_100_08_35  = normalize_matrix(upper_limit_intconf_matrix_100_08_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_100_08_35.npy', norm_lower_limit_intconf_matrix_100_08_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_100_08_35.npy', norm_upper_limit_intconf_matrix_100_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_100_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=100, min_dist=0.8\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=100, min_dist=0.8\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_100_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=100, min_dist=0.8\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_100_08_35 , \"UMAP MST - Mean Distances - n_neighbors=100, min_dist=0.8\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_100_08_35 , \"UMAP MST - Lower Limit - n_neighbors=100, min_dist=0.8\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_100_08_35 , \"UMAP MST - Upper Limit - n_neighbors=100, min_dist=0.8\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptable Radius for min_dist=0,8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For min_dist=0,8.\n",
    "\n",
    "# Function to calculate cluster metrics\n",
    "def calculate_cluster_metrics(umap_projections, y_labels, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Calculate average cluster radii and neighbor counts for each cluster over all runs.\n",
    "    \"\"\"\n",
    "    n_runs = len(umap_projections)  # Number of runs\n",
    "    cluster_centers_full = []\n",
    "    \n",
    "    # Step 1: Calculate cluster centers for each run\n",
    "    for run_idx, x_umap in enumerate(umap_projections):\n",
    "        cluster_centers_run = []\n",
    "        for label in np.unique(y_labels):\n",
    "            cluster_points = x_umap[y_labels == label]\n",
    "            if len(cluster_points) > 0:\n",
    "                cluster_center = np.mean(cluster_points, axis=0)\n",
    "                cluster_centers_run.append(cluster_center)\n",
    "        cluster_centers_full.append(np.array(cluster_centers_run))\n",
    "    \n",
    "    cluster_centers_full = np.array(cluster_centers_full)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "    # Step 2: Calculate average radii for each cluster\n",
    "    radii_per_cluster = []\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        radii_cluster = []\n",
    "        for run_idx, x_umap in enumerate(umap_projections):\n",
    "            cluster_center = cluster_centers_full[run_idx][cluster_idx]\n",
    "            cluster_points = x_umap[y_labels == cluster_idx]\n",
    "            if len(cluster_points) > 0:\n",
    "                distances_to_center = np.linalg.norm(cluster_points - cluster_center, axis=1)\n",
    "                dynamic_radius = np.mean(distances_to_center)  # Mean distance to center\n",
    "                radii_cluster.append(dynamic_radius)\n",
    "        radii_per_cluster.append(np.mean(radii_cluster))  # Average radius across runs\n",
    "\n",
    "    # Step 3: Calculate neighbor counts for each cluster\n",
    "    neighbor_counts_full = []\n",
    "    for run_idx, x_umap in enumerate(umap_projections):\n",
    "        counts_run = []\n",
    "        for cluster_idx, cluster_center in enumerate(cluster_centers_full[run_idx]):\n",
    "            radius = radii_per_cluster[cluster_idx]  # Use the average radius\n",
    "            distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "            count = np.sum(distances_to_center <= radius)  # Count points within the radius\n",
    "            counts_run.append(count)\n",
    "        neighbor_counts_full.append(counts_run)\n",
    "\n",
    "    neighbor_counts_full = np.array(neighbor_counts_full)  # Shape: (n_runs, n_clusters)\n",
    "    average_neighbor_counts = np.mean(neighbor_counts_full, axis=0)  # Average across runs\n",
    "\n",
    "    return radii_per_cluster, average_neighbor_counts\n",
    "\n",
    "# Define n_neighbors values\n",
    "n_neighbors_values = [5, 10, 20, 30, 50, 100]\n",
    "results = []\n",
    "\n",
    "# Iterate over each n_neighbors value\n",
    "for n_neighbors in n_neighbors_values:\n",
    "    if n_neighbors == 5:\n",
    "        umap_projections = umap_projections_5_08_35\n",
    "    elif n_neighbors == 10:\n",
    "        umap_projections = umap_projections_10_08_35\n",
    "    elif n_neighbors == 20:\n",
    "        umap_projections = umap_projections_20_08_35\n",
    "    elif n_neighbors == 30:\n",
    "        umap_projections = umap_projections_30_08_35\n",
    "    elif n_neighbors == 50:\n",
    "        umap_projections = umap_projections_50_08_35\n",
    "    elif n_neighbors == 100:\n",
    "        umap_projections = umap_projections_100_08_35\n",
    "\n",
    "    # Calculate metrics\n",
    "    radii_per_cluster, average_neighbor_counts = calculate_cluster_metrics(umap_projections, y_train)\n",
    "\n",
    "    # Store results\n",
    "    for cluster_idx in range(len(radii_per_cluster)):\n",
    "        results.append({\n",
    "            \"N\": n_neighbors,\n",
    "            \"Cluster\": cluster_idx,\n",
    "            \"Radius\": np.round(radii_per_cluster[cluster_idx], 3),\n",
    "            \"Number of Neighbors\": np.round(average_neighbor_counts[cluster_idx], 0)\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save results for later use\n",
    "df_results.to_csv(\"radius_neighbor_analysis_merged_MinDist_08.csv\", index=False)\n",
    "\n",
    "# Pivot table for easy visualization\n",
    "pivot_table = df_results.pivot(index=\"Cluster\", columns=\"N\", values=[\"Radius\", \"Number of Neighbors\"])\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_neighbor_counts_across_runs(\n",
    "    umap_projections_list=[\n",
    "        umap_projections_5_08_35,\n",
    "        umap_projections_10_08_35,\n",
    "        umap_projections_20_08_35,\n",
    "        umap_projections_30_08_35,\n",
    "        umap_projections_50_08_35,\n",
    "        umap_projections_100_08_35\n",
    "    ],\n",
    "    n_neighbors_values=n_neighbors_values,\n",
    "    y_labels=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading .csv \n",
    "df_results_08 = pd.read_csv('radius_neighbor_analysis_merged_MinDist_08.csv')\n",
    "\n",
    "# Add a density column to df_results\n",
    "df_results_08['Density'] = df_results_08['Number of Neighbors'] / df_results_08['Radius']\n",
    "\n",
    "# Find the row with the maximum density\n",
    "max_density_row = df_results_08.loc[df_results_08['Density'].idxmax()]\n",
    "\n",
    "# Extract the cluster, n_neighbors, and maximum density\n",
    "max_density = max_density_row['Density']\n",
    "max_cluster = max_density_row['Cluster']\n",
    "max_n_neighbors = max_density_row['N']\n",
    "\n",
    "# Print the results\n",
    "print(f\"Highest Density: {max_density:.2f}\")\n",
    "print(f\"Cluster: {int(max_cluster)}\")\n",
    "print(f\"n_neighbors: {int(max_n_neighbors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General UMAP Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define clusters\n",
    "clusters = np.arange(10)  # Clusters from 0 to 9\n",
    "\n",
    "# Define colors for each n_neighbors\n",
    "colors = {5: \"orange\", 10: \"blue\", 20: \"yellow\", 30: \"grey\", 50: \"green\", 100: \"red\"}\n",
    "\n",
    "# Create a PDF to save all the plots\n",
    "with PdfPages(\"Cluster_Confidence_Intervals.pdf\") as pdf:\n",
    "    # Iterate over each cluster as the base cluster\n",
    "    for base_cluster in clusters:\n",
    "        \n",
    "        # Define the data for each n_neighbors, adjusted for the base cluster\n",
    "        data = {\n",
    "            5: {\n",
    "                \"mean\": np.delete(mean_distance_matrix_5_01_35[base_cluster], base_cluster),  # Distances from base cluster\n",
    "                \"lower\": np.delete(lower_limit_intconf_matrix_5_01_35[base_cluster], base_cluster),  # Lower bounds\n",
    "                \"upper\": np.delete(upper_limit_intconf_matrix_5_01_35[base_cluster], base_cluster)   # Upper bounds\n",
    "            },\n",
    "            10: {\n",
    "                \"mean\": np.delete(mean_distance_matrix_10_01_35[base_cluster], base_cluster),\n",
    "                \"lower\": np.delete(lower_limit_intconf_matrix_10_01_35[base_cluster], base_cluster),\n",
    "                \"upper\": np.delete(upper_limit_intconf_matrix_10_01_35[base_cluster], base_cluster)\n",
    "            },\n",
    "            20: {\n",
    "                \"mean\": np.delete(mean_distance_matrix_20_01_35[base_cluster], base_cluster),\n",
    "                \"lower\": np.delete(lower_limit_intconf_matrix_20_01_35[base_cluster], base_cluster),\n",
    "                \"upper\": np.delete(upper_limit_intconf_matrix_20_01_35[base_cluster], base_cluster)\n",
    "            },\n",
    "            30: {\n",
    "                \"mean\": np.delete(mean_distance_matrix_30_01_35[base_cluster], base_cluster),\n",
    "                \"lower\": np.delete(lower_limit_intconf_matrix_30_01_35[base_cluster], base_cluster),\n",
    "                \"upper\": np.delete(upper_limit_intconf_matrix_30_01_35[base_cluster], base_cluster)\n",
    "            },\n",
    "            50: {\n",
    "                \"mean\": np.delete(mean_distance_matrix_50_01_35[base_cluster], base_cluster),\n",
    "                \"lower\": np.delete(lower_limit_intconf_matrix_50_01_35[base_cluster], base_cluster),\n",
    "                \"upper\": np.delete(upper_limit_intconf_matrix_50_01_35[base_cluster], base_cluster)\n",
    "            },\n",
    "            100: {\n",
    "                \"mean\": np.delete(mean_distance_matrix_100_01_35[base_cluster], base_cluster),\n",
    "                \"lower\": np.delete(lower_limit_intconf_matrix_100_01_35[base_cluster], base_cluster),\n",
    "                \"upper\": np.delete(upper_limit_intconf_matrix_100_01_35[base_cluster], base_cluster)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Define clusters to be compared against (excluding the base cluster)\n",
    "        compare_clusters = np.delete(clusters, base_cluster)\n",
    "\n",
    "        # Plotting\n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "        width = 0.15  # Bar width\n",
    "        x = np.arange(len(compare_clusters))  # X positions for clusters\n",
    "\n",
    "        for idx, (n_neighbors, values) in enumerate(data.items()):\n",
    "            # Calculate positions for the current set of bars\n",
    "            x_positions = x + (idx - len(data) / 2) * width\n",
    "\n",
    "            # Plot bars for the mean distances\n",
    "            ax.bar(\n",
    "                x_positions,\n",
    "                values[\"mean\"],  # Mean distances\n",
    "                yerr=[\n",
    "                    values[\"mean\"] - values[\"lower\"],  # Lower error\n",
    "                    values[\"upper\"] - values[\"mean\"]   # Upper error\n",
    "                ],\n",
    "                width=width,\n",
    "                color=colors[n_neighbors],\n",
    "                alpha=0.7,\n",
    "                label=f\"n={n_neighbors}\",\n",
    "                capsize=5\n",
    "            )\n",
    "\n",
    "        # Add labels, title, and legend\n",
    "        ax.set_xlabel(\"Clusters\", fontsize=14)\n",
    "        ax.set_ylabel(\"Distance\", fontsize=14)\n",
    "        ax.set_title(f\"Confidence Intervals of Distances from Cluster {base_cluster} to Other Clusters\", fontsize=16)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([f\"{i}\" for i in compare_clusters], fontsize=12)\n",
    "        ax.legend(title=\"n_neighbors\", fontsize=10)\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the figure to the PDF\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"PDF with cluster confidence intervals has been successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define clusters\n",
    "clusters = np.arange(10)  # Clusters from 0 to 9\n",
    "\n",
    "# Define colors for each n_neighbors\n",
    "colors = {5: \"orange\", 10: \"blue\", 20: \"yellow\", 30: \"grey\", 50: \"green\", 100: \"red\"}\n",
    "\n",
    "# Iterate over each cluster as the base cluster\n",
    "for base_cluster in clusters:\n",
    "    \n",
    "    # Define the data for each n_neighbors, adjusted for the base cluster\n",
    "    data = {\n",
    "        5: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_5_01_35[base_cluster], base_cluster),  # Distances from base cluster\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_5_01_35[base_cluster], base_cluster),  # Lower bounds\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_5_01_35[base_cluster], base_cluster)   # Upper bounds\n",
    "        },\n",
    "        10: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_10_01_35[base_cluster], base_cluster),\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_10_01_35[base_cluster], base_cluster),\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_10_01_35[base_cluster], base_cluster)\n",
    "        },\n",
    "        20: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_20_01_35[base_cluster], base_cluster),\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_20_01_35[base_cluster], base_cluster),\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_20_01_35[base_cluster], base_cluster)\n",
    "        },\n",
    "        30: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_30_01_35[base_cluster], base_cluster),\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_30_01_35[base_cluster], base_cluster),\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_30_01_35[base_cluster], base_cluster)\n",
    "        },\n",
    "        50: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_50_01_35[base_cluster], base_cluster),\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_50_01_35[base_cluster], base_cluster),\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_50_01_35[base_cluster], base_cluster)\n",
    "        },\n",
    "        100: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_100_01_35[base_cluster], base_cluster),\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_100_01_35[base_cluster], base_cluster),\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_100_01_35[base_cluster], base_cluster)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define clusters to be compared against (excluding the base cluster)\n",
    "    compare_clusters = np.delete(clusters, base_cluster)\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "    width = 0.15  # Bar width\n",
    "    x = np.arange(len(compare_clusters))  # X positions for clusters\n",
    "\n",
    "    for idx, (n_neighbors, values) in enumerate(data.items()):\n",
    "        # Calculate positions for the current set of bars\n",
    "        x_positions = x + (idx - len(data) / 2) * width\n",
    "\n",
    "        # Plot bars for the mean distances\n",
    "        ax.bar(\n",
    "            x_positions,\n",
    "            values[\"mean\"],  # Mean distances\n",
    "            yerr=[\n",
    "                values[\"mean\"] - values[\"lower\"],  # Lower error\n",
    "                values[\"upper\"] - values[\"mean\"]   # Upper error\n",
    "            ],\n",
    "            width=width,\n",
    "            color=colors[n_neighbors],\n",
    "            alpha=0.7,\n",
    "            label=f\"n={n_neighbors}\",\n",
    "            capsize=5\n",
    "        )\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    ax.set_xlabel(\"Clusters\", fontsize=14)\n",
    "    ax.set_ylabel(\"Distance\", fontsize=14)\n",
    "    ax.set_title(f\"Confidence Intervals of Distances from Cluster {base_cluster} to Other Clusters\", fontsize=16)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f\"{i}\" for i in compare_clusters], fontsize=12)\n",
    "    ax.legend(title=\"n_neighbors\", fontsize=10)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Define your matrices for each n_neighbors value\n",
    "matrices = {\n",
    "    5: {\n",
    "        \"mean\": normalized_mean_distance_matrix_5_08_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_5_08_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_5_08_35\n",
    "    },\n",
    "    10: {\n",
    "        \"mean\": normalized_mean_distance_matrix_10_08_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_10_08_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_10_08_35\n",
    "    },\n",
    "    20: {\n",
    "        \"mean\": normalized_mean_distance_matrix_20_08_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_20_08_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_20_08_35\n",
    "    },\n",
    "    30: {\n",
    "        \"mean\": normalized_mean_distance_matrix_30_08_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_30_08_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_30_08_35\n",
    "    },\n",
    "    50: {\n",
    "        \"mean\": normalized_mean_distance_matrix_50_08_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_50_08_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_50_08_35\n",
    "    },\n",
    "    100: {\n",
    "        \"mean\": normalized_mean_distance_matrix_100_08_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_100_08_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_100_08_35\n",
    "    }\n",
    "}\n",
    "\n",
    "# Open a PDF to save the plots\n",
    "with PdfPages('MST_UMAP_Comparisons min_dis=0.8.pdf') as pdf:\n",
    "    for n_neighbors, matrix_set in matrices.items():\n",
    "        # Set up the figure with three subplots\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # Plot MSTs for mean, lower, and upper matrices\n",
    "        plot_mst(matrix_set[\"mean\"], f\"MST UMAP - Mean Distances (n_neighbors={n_neighbors}, min_dis=0.8)\", axes[1], color='red')\n",
    "        plot_mst(matrix_set[\"lower\"], f\"MST UMAP - Lower Limit (n_neighbors={n_neighbors}, min_dis=0.8)\", axes[0], color='blue')\n",
    "        plot_mst(matrix_set[\"upper\"], f\"MST UMAP - Upper Limit (n_neighbors={n_neighbors}, min_dis=0.8)\", axes[2], color='green')\n",
    "        \n",
    "        # Adjust layout for better spacing\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the current figure to the PDF\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"PDF with MST UMAP Comparisons has been successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS vs UMAP Sammon's stress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following analysis was done using the MDS with 2 components indeces and results from Exploration and comparison of multiple algorithms - MNIST and UMAP Projections of min_dist=0.1 and n_neighbours=10, n_neighbours=50 and n_neighbours=100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Downsample the Dataset Consistently\n",
    "def downsample_mnist_consistent(x_data, y_labels, sample_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Downsample the dataset consistently, returning indices to ensure\n",
    "    the same points are selected in both spaces.\n",
    "    \"\"\"\n",
    "    sampled_indices = []\n",
    "    unique_labels = np.unique(y_labels)\n",
    "    for label in unique_labels:\n",
    "        # Select indices for the current label\n",
    "        label_indices = np.where(y_labels == label)[0]\n",
    "        # Sample a fraction of points for this label\n",
    "        sampled_indices_label = resample(\n",
    "            label_indices, n_samples=int(len(label_indices) * sample_fraction), replace=False\n",
    "        )\n",
    "        sampled_indices.extend(sampled_indices_label)\n",
    "    return np.array(sampled_indices)\n",
    "\n",
    "# Get consistent indices for sampling\n",
    "sampled_indices = downsample_mnist_consistent(x_train_flattened, y_train, sample_fraction=0.1)\n",
    "\n",
    "# Step 2: Use the Sampled Indices to Extract Points from Both Spaces\n",
    "# Downsample the high-dimensional original space\n",
    "x_sampled = x_train_flattened[sampled_indices]  # Original high-dimensional space\n",
    "y_sampled = y_train[sampled_indices]            # Corresponding labels\n",
    "\n",
    "# Load the mean projections and downsample\n",
    "umap_projections_downsampled = {\n",
    "    10: np.load(\"mean_projection_10_01_35.npy\")[sampled_indices],  # Mean projection for n_neighbors=10\n",
    "    50: np.load(\"mean_projection_50_01_35.npy\")[sampled_indices],  # Mean projection for n_neighbors=50\n",
    "    100: np.load(\"mean_projection_100_01_35.npy\")[sampled_indices],  # Mean projection for n_neighbors=100\n",
    "}\n",
    "\n",
    "# Output shapes for verification\n",
    "print(f\"x_sampled shape: {x_sampled.shape}\")\n",
    "print(f\"y_sampled shape: {y_sampled.shape}\")\n",
    "for n_neighbors, projection in umap_projections_downsampled.items():\n",
    "    print(f\"UMAP (n_neighbors={n_neighbors}) downsampled shape: {projection.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn's pairwise_distances for better handling of large arrays\n",
    "# \"True\" distances between points in the original/high dimensional space\n",
    "pairwise_distances = sklearn_pairwise_distances(x_sampled, metric='euclidean')\n",
    "\n",
    "# Initialize and fit MDS\n",
    "# Uses MDS to create a reference (ideal or baseline) embedding in 2D while preserving the global structure of pairwise distances\n",
    "mds_model = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "mds_embedding = mds_model.fit_transform(pairwise_distances)\n",
    "\n",
    "def sammons_stress(original_distances, embedding_distances):\n",
    "    \"\"\"\n",
    "    Calculate Sammon's stress/error with normalization.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-9  # Avoid division by zero\n",
    "    original_distances = np.maximum(original_distances, epsilon)\n",
    "    normalization = np.sum(original_distances)  # Sum of all original distances\n",
    "    stress = np.sum(((original_distances - embedding_distances) ** 2) / original_distances)\n",
    "    return stress / normalization  # Normalize by the total sum of original distances\n",
    "\n",
    "# Above eq. quantifies the degree to which the low-dimensional embedding preserves the pairwise distances from the original space. Lower stress indicates better preservation.\n",
    "\n",
    "# Calculate Sammon's stress for UMAP embeddings\n",
    "# Evaluates how well each UMAP embedding preserves global structures compared to the original distances\n",
    "stress_results = {}\n",
    "original_distances = pairwise_distances\n",
    "for n_neighbors, umap_embedding in umap_projections_downsampled.items():\n",
    "    # Compute pairwise distances for the UMAP embedding\n",
    "    umap_distances = sklearn_pairwise_distances(umap_embedding, metric='euclidean')\n",
    "    # Calculate Sammon's stress\n",
    "    stress = sammons_stress(original_distances, umap_distances)\n",
    "    stress_results[n_neighbors] = stress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sampled indices\n",
    "sampled_indices_train_mds= np.load(\"sampled_indices_train_mds.npy\")\n",
    "sampled_indices_test_mds= np.load(\"sampled_indices_test_mds.npy\")\n",
    "\n",
    "# Load downsampled dataset\n",
    "x_train_sampled_mds= np.load(\"x_train_sampled_mds.npy\")\n",
    "y_train_sampled_mds= np.load(\"y_train_sampled_mds.npy\")\n",
    "x_test_sampled_mds= np.load(\"x_test_sampled_mds.npy\")\n",
    "y_test_sampled_mds= np.load(\"y_test_sampled_mds.npy\")\n",
    "\n",
    "# Load MDS embeddings\n",
    "x_train_mds_c2= np.load(\"x_train_mds_c2.npy\")\n",
    "x_test_mds_c2= np.load(\"x_test_mds_c2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mean projections and downsample\n",
    "umap_projections_downsampled = {\n",
    "    10: np.load(\"mean_projection_10_01_35.npy\")[sampled_indices_train_mds],  \n",
    "    50: np.load(\"mean_projection_50_01_35.npy\")[sampled_indices_train_mds],  \n",
    "    100: np.load(\"mean_projection_100_01_35.npy\")[sampled_indices_train_mds],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neighbors, projection in umap_projections_downsampled.items():\n",
    "    print(f\"UMAP (n_neighbors={n_neighbors}) downsampled shape: {projection.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sammon's stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Sammon's stress\n",
    "def sammons_stress(original_distances, embedding_distances):\n",
    "    \"\"\"\n",
    "    Calculate Sammon's stress/error with normalization.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-9  # Avoid division by zero\n",
    "    original_distances = np.maximum(original_distances, epsilon)  # Prevent zero distances\n",
    "    normalization = np.sum(original_distances)  # Sum of all original distances\n",
    "    stress = np.sum(((original_distances - embedding_distances) ** 2) / original_distances)\n",
    "    return stress / normalization  # Normalize by the total sum of original distances\n",
    "\n",
    "# Compute pairwise distances for the original MDS embedding\n",
    "original_distances = sklearn_pairwise_distances(x_train_mds_c2, metric='euclidean')\n",
    "print(f\"Original distances shape: {original_distances.shape}\")  # Should be (5996, 5996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Sammon's stress for UMAP embeddings\n",
    "stress_results = {}\n",
    "for n_neighbors, umap_embedding in umap_projections_downsampled.items():\n",
    "    # Compute pairwise distances for the UMAP embedding\n",
    "    umap_distances = sklearn_pairwise_distances(umap_embedding, metric='euclidean')\n",
    "    print(f\"UMAP (n_neighbors={n_neighbors}) distances shape: {umap_distances.shape}\")  # Should be (5996, 5996)\n",
    "\n",
    "    # Calculate Sammon's stress\n",
    "    stress = sammons_stress(original_distances, umap_distances)\n",
    "    stress_results[n_neighbors] = stress\n",
    "\n",
    "# Print the Sammon's stress results\n",
    "for n_neighbors, stress in stress_results.items():\n",
    "    print(f\"Sammon's stress for UMAP (n_neighbors={n_neighbors}): {stress}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Visualize Results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot MDS Embedding\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(x_train_mds_c2[:, 0], x_train_mds_c2[:, 1], c=y_train_sampled_mds, cmap='Spectral', s=5)\n",
    "plt.title(\"MDS Embedding\")\n",
    "plt.colorbar(label=\"Digit Label\")\n",
    "\n",
    "# Plot UMAP Embeddings for different n_neighbors\n",
    "for idx, n_neighbors in enumerate([10, 50, 100], start=2):\n",
    "    plt.subplot(2, 2, idx)\n",
    "    plt.scatter(\n",
    "        umap_projections_downsampled[n_neighbors][:, 0], \n",
    "        umap_projections_downsampled[n_neighbors][:, 1], \n",
    "        c=y_train_sampled_mds, cmap='Spectral', s=5\n",
    "    )\n",
    "    plt.title(f\"UMAP Embedding (n_neighbors={n_neighbors})\")\n",
    "    plt.colorbar(label=\"Digit Label\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammon's stress and Variability with t-student (due to runs= 10<30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over stress results\n",
    "for n_neighbors, run_stress_values in stress_results.items():\n",
    "    # Ensure run_stress_values is an array\n",
    "    run_stress_values = np.array(run_stress_values)\n",
    "\n",
    "    if run_stress_values.ndim == 0:  # Handle scalar case (not iterable)\n",
    "        run_stress_values = np.array([run_stress_values])  # Convert scalar to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.stats import t\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the UMAP Projections for the First 10 Runs\n",
    "umap_projections_dict = {\n",
    "    10: umap_projections_10_01_35[:10, sampled_indices_train_mds, :],  # First 10 runs for n_neighbors=10\n",
    "    50: umap_projections_50_01_35[:10, sampled_indices_train_mds, :],  # First 10 runs for n_neighbors=50\n",
    "    100: umap_projections_100_01_35[:10, sampled_indices_train_mds, :]  # First 10 runs for n_neighbors=100\n",
    "}\n",
    "\n",
    "# Step 2: Calculate Sammon's Stress for Each Run\n",
    "sammon_results = {}  # Dictionary to store stress results for each n_neighbors\n",
    "for n_neighbors, projections in umap_projections_dict.items():\n",
    "    stress_values = []\n",
    "    for run_number, run_projection in enumerate(projections, start=1):\n",
    "        # Compute pairwise distances for the UMAP embedding\n",
    "        try:\n",
    "            embedding_distances = pairwise_distances(run_projection, metric='euclidean')\n",
    "            # Calculate Sammon's stress\n",
    "            stress = sammons_stress(original_distances, embedding_distances)\n",
    "            stress_values.append((run_number, stress))\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating stress for n_neighbors={n_neighbors}, run={run_number}: {e}\")\n",
    "            continue\n",
    "    sammon_results[n_neighbors] = stress_values\n",
    "\n",
    "# Step 3: Print the Results for the First 10 Runs\n",
    "print(\"Sammon's Stress Results for the First 10 Runs:\")\n",
    "for n_neighbors, stress_values in sammon_results.items():\n",
    "    print(f\"\\nUMAP (n_neighbors={n_neighbors}):\")\n",
    "    for run_number, stress in stress_values:\n",
    "        print(f\"  Run {run_number}: Sammon's Stress = {stress:.6f}\")\n",
    "\n",
    "# Step 4: Update Variability Computation with Multiple Runs\n",
    "final_results = {}\n",
    "run_variability = {}\n",
    "\n",
    "for n_neighbors, stress_values in sammon_results.items():\n",
    "    run_stress_values = [stress for _, stress in stress_values]\n",
    "    mean_stress = np.mean(run_stress_values)\n",
    "    std_stress = np.std(run_stress_values, ddof=1)  # Use ddof=1 for sample standard deviation\n",
    "\n",
    "    # Calculate confidence interval using Student's t-distribution\n",
    "    if len(run_stress_values) > 1:  # Ensure enough data points for CI calculation\n",
    "        t_score = t.ppf(0.975, df=len(run_stress_values) - 1)\n",
    "        margin_of_error = t_score * (std_stress / np.sqrt(len(run_stress_values)))\n",
    "        confidence_interval = (mean_stress - margin_of_error, mean_stress + margin_of_error)\n",
    "    else:\n",
    "        confidence_interval = (mean_stress, mean_stress)\n",
    "\n",
    "    # Store final results\n",
    "    final_results[n_neighbors] = {\n",
    "        \"mean\": mean_stress,\n",
    "        \"std\": std_stress,\n",
    "        \"95% CI\": confidence_interval,\n",
    "        \"run_values\": run_stress_values\n",
    "    }\n",
    "    # Store standard deviation for run-to-run variability\n",
    "    run_variability[n_neighbors] = std_stress\n",
    "\n",
    "# Step 5: Print Final Results with Variability\n",
    "print(\"\\nSammon's Stress Results with Variability (10 Runs):\")\n",
    "for n_neighbors, stats in final_results.items():\n",
    "    print(f\"n_neighbors={n_neighbors}:\")\n",
    "    print(f\"  Mean Stress: {stats['mean']:.4f}\")\n",
    "    print(f\"  Standard Deviation Across Runs: {stats['std']:.4f}\")\n",
    "    print(f\"  95% Confidence Interval: {stats['95% CI']}\")\n",
    "    for run_idx, stress in enumerate(stats['run_values'], start=1):\n",
    "        print(f\"    Run {run_idx}: Stress={stress:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procrustes Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import procrustes\n",
    "\n",
    "# Calculate Procrustes distance between MDS and UMAP embeddings\n",
    "procrustes_results = {}\n",
    "for n_neighbors, umap_embedding in umap_projections_downsampled.items():\n",
    "    # Perform Procrustes analysis\n",
    "    mds_embedding = x_train_mds_c2  # Reference embedding (MDS)\n",
    "    _, umap_aligned, disparity = procrustes(mds_embedding, umap_embedding)\n",
    "    # Store the Procrustes distance (disparity)\n",
    "    procrustes_results[n_neighbors] = disparity\n",
    "\n",
    "# Print results\n",
    "for n_neighbors, distance in procrustes_results.items():\n",
    "    print(f\"Procrustes Distance for UMAP (n_neighbors={n_neighbors}): {np.round(distance,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP Methodology in image-based facial emotion recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FER 2013 Import and images preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fer2013.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 15 random rows from the DataFrame\n",
    "random_samples = df.sample(n=15, random_state=42)\n",
    "random_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the sampled rows to display the images\n",
    "for index, row in random_samples.iterrows():\n",
    "    # Convert the pixel string into a NumPy array and reshape it\n",
    "    pixels_array = np.array(list(map(int, row[\"pixels\"].split())), dtype=np.uint8).reshape(48, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid for 15 images (3 rows x 5 columns)\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "axes = axes.flatten()  # Flatten the grid to easily iterate\n",
    "\n",
    "# Loop through the sampled rows and display the images\n",
    "for ax, (_, row) in zip(axes, random_samples.iterrows()):\n",
    "    # Convert the pixel string into a NumPy array and reshape it\n",
    "    pixels_array = np.array(list(map(int, row[\"pixels\"].split())), dtype=np.uint8).reshape(48, 48)\n",
    "    \n",
    "    # Display the image\n",
    "    ax.imshow(pixels_array, cmap=\"gray\")\n",
    "    # Add a title with relevant information\n",
    "    ax.set_title(f\"{row['Usage']}[{row.name}] = {row['emotion']}\")\n",
    "    ax.axis('off')  # Turn off axis for a cleaner look\n",
    "\n",
    "# Adjust layout to avoid overlapping titles\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets based on 'Usage' column\n",
    "train_fer2013 = df[df.Usage == \"Training\"]\n",
    "test_fer2013 = df[df.Usage != \"Training\"]  # Assuming all non-training rows are for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert 'pixels' column to numerical arrays\n",
    "def convert_pixels(pixels_str):\n",
    "    return np.array([int(pixel) for pixel in pixels_str.split()], dtype=np.uint8)\n",
    "\n",
    "# Apply conversion\n",
    "train_fer2013_pixels = np.vstack(train_fer2013[\"pixels\"].apply(convert_pixels).values)\n",
    "test_fer2013_pixels = np.vstack(test_fer2013[\"pixels\"].apply(convert_pixels).values)\n",
    "\n",
    "# Step 2: Reshape to flatten into 1D vectors (2304 features)\n",
    "x_train_fer2013 = train_fer2013_pixels.reshape(-1, 48 * 48)  # Shape: (28   709, 2304)\n",
    "x_test_fer2013 = test_fer2013_pixels.reshape(-1, 48 * 48)    # Shape: (7178, 2304)\n",
    "\n",
    "# Step 3: Normalize the flattened data\n",
    "scaler = StandardScaler()\n",
    "x_train_fer2013_scaled = scaler.fit_transform(x_train_fer2013)\n",
    "x_test_fer2013_scaled = scaler.transform(x_test_fer2013)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Shape of x_train_scaled:\", x_train_fer2013_scaled.shape)  # Should be (28709, 2304)\n",
    "print(\"Shape of x_test_scaled:\", x_test_fer2013_scaled.shape)    # Should be (7178, 2304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels directly as a 1D array\n",
    "y_train_fer2013 = train_fer2013[\"emotion\"].values  # Shape: (28709,)\n",
    "y_test_fer2013 = test_fer2013[\"emotion\"].values    # Shape: (7178,)\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"Shape of y_train_fer2013:\", y_train_fer2013.shape)\n",
    "print(\"Shape of y_test_fer2013:\", y_test_fer2013.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe flatten and normalized\n",
    "print(\"Shape of x_train_fer2013:\", x_train_fer2013.shape)\n",
    "print(\"Shape of x_test_fer2013:\", x_test_fer2013.shape)\n",
    "print(\"Shape of y_train_fer2013:\", y_train_fer2013.shape)\n",
    "print(\"Shape of y_test_fer2013:\", y_test_fer2013.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised UMAP 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "fer_sup_umap_projections_train_10_01= np.load('fer_sup_umap_projections_train_10_01.npy')\n",
    "fer_mean_sup_umap_projection_train_10_01= np.load('fer_mean_sup_umap_projection_train_10_01.npy')\n",
    "fer_std_sup_umap_projection_train_10_01= np.load('fer_std_sup_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "fer_sup_umap_projections_test_10_01= np.load('fer_sup_umap_projections_test_10_01.npy')\n",
    "fer_mean_sup_umap_projection_test_10_01= np.load('fer_mean_sup_umap_projection_test_10_01.npy')\n",
    "fer_std_sup_umap_projection_test_10_01= np.load('fer_std_sup_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for training and test sets\n",
    "fer_sup_umap_projections_train_10_01 = []\n",
    "fer_sup_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running Supervised UMAP - Iteration {run + 1}/{n_runs}...\")\n",
    "\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, \n",
    "        min_dist=min_dist, \n",
    "        n_components=n_components, \n",
    "        random_state=run\n",
    "    )\n",
    "\n",
    "    # Fit and transform the training data with labels\n",
    "    projection_train = umap_model.fit_transform(x_train_fer2013, y_train_fer2013)\n",
    "    fer_sup_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_fer2013)\n",
    "    fer_sup_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "fer_sup_umap_projections_train_10_01 = np.array(fer_sup_umap_projections_train_10_01)\n",
    "fer_sup_umap_projections_test_10_01 = np.array(fer_sup_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "fer_mean_sup_umap_projection_train_10_01 = np.mean(fer_sup_umap_projections_train_10_01, axis=0)\n",
    "fer_std_sup_umap_projection_train_10_01 = np.std(fer_sup_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "fer_mean_sup_umap_projection_test_10_01 = np.mean(fer_sup_umap_projections_test_10_01, axis=0)\n",
    "fer_std_sup_umap_projection_test_10_01 = np.std(fer_sup_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('fer_sup_umap_projections_train_10_01.npy', fer_sup_umap_projections_train_10_01)\n",
    "np.save('fer_mean_sup_umap_projection_train_10_01.npy', fer_mean_sup_umap_projection_train_10_01)\n",
    "np.save('fer_std_sup_umap_projection_train_10_01.npy', fer_std_sup_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('fer_sup_umap_projections_test_10_01.npy', fer_sup_umap_projections_test_10_01)\n",
    "np.save('fer_mean_sup_umap_projection_test_10_01.npy', fer_mean_sup_umap_projection_test_10_01)\n",
    "np.save('fer_std_sup_umap_projection_test_10_01.npy', fer_std_sup_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"Supervised UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the projections, mean, and standard deviation for the test set\n",
    "fer_sup_umap_projections_train_10_01= np.load('fer_sup_umap_projections_train_10_01.npy')\n",
    "fer_mean_sup_umap_projection_train_10_01= np.load('fer_mean_sup_umap_projection_train_10_01.npy')\n",
    "fer_std_sup_umap_projection_train_10_01= np.load('fer_std_sup_umap_projection_train_10_01.npy')\n",
    "\n",
    "# Load the projections, mean, and standard deviation for the test set\n",
    "fer_sup_umap_projections_test_10_01= np.load('fer_sup_umap_projections_test_10_01.npy')\n",
    "fer_mean_sup_umap_projection_test_10_01= np.load('fer_mean_sup_umap_projection_test_10_01.npy')\n",
    "fer_std_sup_umap_projection_test_10_01= np.load('fer_std_sup_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "unique_labels = np.unique(y_train_fer2013)\n",
    "cmap = plt.cm.get_cmap(\"tab10\", len(unique_labels))\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(fer_mean_sup_umap_projection_train_10_01[:, 0], fer_mean_sup_umap_projection_train_10_01[:, 1], c=y_train_fer2013, cmap=cmap, s=5, alpha=0.8)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Supervised UMAP Projection of FER2013 Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(0, 7))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(0, 7)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_fer_umap_sup_10_01 = adjusted_rand_score(y_test_fer2013, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_sup_umap_projection_train_10_01, y_train_fer2013).predict(fer_mean_sup_umap_projection_test_10_01)) # second argument is y_test_pred_pca\n",
    "print(f\"ARI: {ari_fer_umap_sup_10_01:.4f}\")\n",
    "# Silhouette Score\n",
    "silhouette_fer_umap_sup_10_01 = silhouette_score(fer_mean_sup_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_sup_umap_projection_train_10_01, y_train_fer2013).predict(fer_mean_sup_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_fer_umap_sup_10_01:.2f}\")\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train_fer2013))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(fer_mean_sup_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "fer_sup_umap_projection_10_01_db_score = davies_bouldin_score(\n",
    "    fer_mean_sup_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {fer_sup_umap_projection_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the saved mean UMAP projections\n",
    "mean_projection = np.load('mean_sup_projection_10_01_35.npy')  # Shape: (n_samples, 2)\n",
    "\n",
    "# Step 2: Separate the mean projection by class\n",
    "classes = np.unique(y_train_fer2013)\n",
    "class_gaussians = {}\n",
    "\n",
    "# Calculate the mean and covariance for each class\n",
    "for c in classes:\n",
    "    class_points = mean_projection[y_train_fer2013 == c]  # Filter by class\n",
    "    mean = np.mean(class_points, axis=0)\n",
    "    cov = np.cov(class_points, rowvar=False)\n",
    "    class_gaussians[c] = {\"mean\": mean, \"cov\": cov}\n",
    "\n",
    "# Step 3: Visualize Gaussian distributions\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot UMAP embeddings for each class\n",
    "for c in classes:\n",
    "    class_points = mean_projection[y_train_fer2013 == c]\n",
    "    plt.scatter(class_points[:, 0], class_points[:, 1], label=f\"Class {c}\", alpha=0.5, s=10)\n",
    "\n",
    "    # Plot Gaussian contours\n",
    "    mean = class_gaussians[c][\"mean\"]\n",
    "    cov = class_gaussians[c][\"cov\"]\n",
    "    x, y = np.meshgrid(\n",
    "        np.linspace(mean[0] - 3, mean[0] + 3, 100), \n",
    "        np.linspace(mean[1] - 3, mean[1] + 3, 100)\n",
    "    )\n",
    "    pos = np.dstack((x, y))\n",
    "    rv = multivariate_normal(mean, cov)\n",
    "    plt.contour(x, y, rv.pdf(pos), levels=5, alpha=0.8)\n",
    "\n",
    "plt.title(\"UMAP Mean Projections with Gaussian Distributions per Class\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Evaluate likelihood for a random point\n",
    "random_point = np.array([0, 0])  # Example point in UMAP space\n",
    "likelihoods = {c: multivariate_normal(class_gaussians[c][\"mean\"], class_gaussians[c][\"cov\"]).pdf(random_point)\n",
    "               for c in classes}\n",
    "\n",
    "print(\"Likelihoods for Random Point:\", likelihoods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised UMAP 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "fer_unsup_umap_projections_train_10_01= np.load('fer_unsup_umap_projections_train_10_01.npy')\n",
    "fer_mean_unsup_umap_projection_train_10_01= np.load('fer_mean_unsup_umap_projection_train_10_01.npy')\n",
    "fer_std_unsup_umap_projection_train_10_01= np.load('fer_std_unsup_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "fer_unsup_umap_projections_test_10_01= np.load('fer_unsup_umap_projections_test_10_01.npy')\n",
    "fer_mean_unsup_umap_projection_test_10_01= np.load('fer_mean_unsup_umap_projection_test_10_01.npy')\n",
    "fer_std_unsup_umap_projection_test_10_01= np.load('fer_std_unsup_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "fer_unsup_umap_projections_train_10_01 = []\n",
    "fer_unsup_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_train_fer2013)\n",
    "    fer_unsup_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_fer2013)\n",
    "    fer_unsup_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "fer_unsup_umap_projections_train_10_01 = np.array(fer_unsup_umap_projections_train_10_01)\n",
    "fer_unsup_umap_projections_test_10_01 = np.array(fer_unsup_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "fer_mean_unsup_umap_projection_train_10_01 = np.mean(fer_unsup_umap_projections_train_10_01, axis=0)\n",
    "fer_std_unsup_umap_projection_train_10_01 = np.std(fer_unsup_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "fer_mean_unsup_umap_projection_test_10_01 = np.mean(fer_unsup_umap_projections_test_10_01, axis=0)\n",
    "fer_std_unsup_umap_projection_test_10_01 = np.std(fer_unsup_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('fer_unsup_umap_projections_train_10_01.npy', fer_unsup_umap_projections_train_10_01)\n",
    "np.save('fer_mean_unsup_umap_projection_train_10_01.npy', fer_mean_unsup_umap_projection_train_10_01)\n",
    "np.save('fer_std_unsup_umap_projection_train_10_01.npy', fer_std_unsup_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('fer_unsup_umap_projections_test_10_01.npy', fer_unsup_umap_projections_test_10_01)\n",
    "np.save('fer_mean_unsup_umap_projection_test_10_01.npy', fer_mean_unsup_umap_projection_test_10_01)\n",
    "np.save('fer_std_unsup_umap_projection_test_10_01.npy', fer_std_unsup_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "unique_labels = np.unique(y_train_fer2013)\n",
    "cmap = plt.cm.get_cmap(\"tab10\", len(unique_labels))\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    fer_mean_unsup_umap_projection_train_10_01[:, 0],\n",
    "    fer_mean_unsup_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train_fer2013, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Unsupervised UMAP Projection of FER2013 Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(0, 7))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(0, 7)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_fer_umap_unsup_10_01 = adjusted_rand_score(y_test_fer2013, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_unsup_umap_projection_train_10_01, y_train_fer2013).predict(fer_mean_unsup_umap_projection_test_10_01)) # second argument is y_test_pred_pca\n",
    "print(f\"ARI: {ari_fer_umap_unsup_10_01:.4f}\")\n",
    "# Silhouette Score\n",
    "silhouette_fer_umap_unsup_10_01 = silhouette_score(fer_mean_unsup_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_unsup_umap_projection_train_10_01, y_train_fer2013).predict(fer_mean_unsup_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_fer_umap_unsup_10_01:.2f}\")\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train_fer2013))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(fer_mean_unsup_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "fer_unsup_umap_projection_10_01_db_score = davies_bouldin_score(\n",
    "    fer_mean_unsup_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {fer_unsup_umap_projection_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA + UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply PCA\n",
    "pca = PCA(0.95)\n",
    "x_train_fer2013_pca_emotions = pca.fit_transform(x_train_fer2013)\n",
    "x_test_fer2013_pca_emotions = pca.transform(x_test_fer2013)\n",
    "\n",
    "print(f\"Original number of features: {x_train_fer2013.shape[1]}\")\n",
    "print(f\"Reduced number of features: {x_train_fer2013_pca_emotions.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the projections, mean, and standard deviation\n",
    "np.save('x_train_fer2013_pca_emotions.npy', x_train_fer2013_pca_emotions)\n",
    "np.save('x_test_fer2013_pca_emotions.npy', x_test_fer2013_pca_emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA + UMAP Unsupervised 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "fer_unsup_pca_umap_projections_train_10_01= np.load('fer_unsup_pca_umap_projections_train_10_01.npy')\n",
    "fer_mean_unsup_pca_umap_projection_train_10_01= np.load('fer_mean_unsup_pca_umap_projection_train_10_01.npy')\n",
    "fer_std_unsup_pca_umap_projection_train_10_01= np.load('fer_std_unsup_pca_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "fer_unsup_pca_umap_projections_test_10_01= np.load('fer_unsup_pca_umap_projections_test_10_01.npy')\n",
    "fer_mean_unsup_pca_umap_projection_test_10_01= np.load('fer_mean_unsup_pca_umap_projection_test_10_01.npy')\n",
    "fer_std_unsup_pca_umap_projection_test_10_01= np.load('fer_std_unsup_pca_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "fer_unsup_pca_umap_projections_train_10_01 = []\n",
    "fer_unsup_pca_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_train_fer2013_pca_emotions)\n",
    "    fer_unsup_pca_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_fer2013_pca_emotions)\n",
    "    fer_unsup_pca_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "fer_unsup_pca_umap_projections_train_10_01 = np.array(fer_unsup_pca_umap_projections_train_10_01)\n",
    "fer_unsup_pca_umap_projections_test_10_01 = np.array(fer_unsup_pca_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "fer_mean_unsup_pca_umap_projection_train_10_01 = np.mean(fer_unsup_pca_umap_projections_train_10_01, axis=0)\n",
    "fer_std_unsup_pca_umap_projection_train_10_01 = np.std(fer_unsup_pca_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "fer_mean_unsup_pca_umap_projection_test_10_01 = np.mean(fer_unsup_pca_umap_projections_test_10_01, axis=0)\n",
    "fer_std_unsup_pca_umap_projection_test_10_01 = np.std(fer_unsup_pca_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('fer_unsup_pca_umap_projections_train_10_01.npy', fer_unsup_pca_umap_projections_train_10_01)\n",
    "np.save('fer_mean_unsup_pca_umap_projection_train_10_01.npy', fer_mean_unsup_pca_umap_projection_train_10_01)\n",
    "np.save('fer_std_unsup_pca_umap_projection_train_10_01.npy', fer_std_unsup_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('fer_unsup_pca_umap_projections_test_10_01.npy', fer_unsup_pca_umap_projections_test_10_01)\n",
    "np.save('fer_mean_unsup_pca_umap_projection_test_10_01.npy', fer_mean_unsup_pca_umap_projection_test_10_01)\n",
    "np.save('fer_std_unsup_pca_umap_projection_test_10_01.npy', fer_std_unsup_pca_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "unique_labels = np.unique(y_train_fer2013)\n",
    "cmap = plt.cm.get_cmap(\"tab10\", len(unique_labels))\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    fer_mean_unsup_pca_umap_projection_train_10_01[:, 0],\n",
    "    fer_mean_unsup_pca_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train_fer2013, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"PCA + Unsupervised UMAP Projection of FER2013 Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(0, 7))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(0, 7)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_fer_pca_umap_unsup_10_01 = adjusted_rand_score(y_test_fer2013, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_unsup_pca_umap_projection_train_10_01, y_train_fer2013).predict(fer_mean_unsup_pca_umap_projection_test_10_01))\n",
    "print(f\"ARI: {ari_fer_pca_umap_unsup_10_01:.4f}\")\n",
    "# Silhouette Score\n",
    "silhouette_fer_pca_umap_unsup_10_01 = silhouette_score(fer_mean_unsup_pca_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_unsup_pca_umap_projection_train_10_01, y_train_fer2013).predict(fer_mean_unsup_pca_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_fer_pca_umap_unsup_10_01:.2f}\")\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(fer_mean_unsup_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "fer_unsup_pca_projection_10_01_db_score = davies_bouldin_score(\n",
    "    fer_mean_unsup_pca_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {fer_unsup_pca_projection_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA + UMAP Supervised 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "fer_sup_pca_umap_projections_train_10_01= np.load('fer_sup_pca_umap_projections_train_10_01.npy')\n",
    "fer_mean_sup_pca_umap_projection_train_10_01= np.load('fer_mean_sup_pca_umap_projection_train_10_01.npy')\n",
    "fer_std_sup_pca_umap_projection_train_10_01= np.load('fer_std_sup_pca_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "fer_sup_pca_umap_projections_test_10_01= np.load('fer_sup_pca_umap_projections_test_10_01.npy')\n",
    "fer_mean_sup_pca_umap_projection_test_10_01= np.load('fer_mean_sup_pca_umap_projection_test_10_01.npy')\n",
    "fer_std_sup_pca_umap_projection_test_10_01= np.load('fer_std_sup_pca_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "fer_sup_pca_umap_projections_train_10_01 = []\n",
    "fer_sup_pca_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_train_fer2013_pca_emotions,y_train_fer2013)\n",
    "    fer_sup_pca_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_fer2013_pca_emotions)\n",
    "    fer_sup_pca_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "fer_sup_pca_umap_projections_train_10_01 = np.array(fer_sup_pca_umap_projections_train_10_01)\n",
    "fer_sup_pca_umap_projections_test_10_01 = np.array(fer_sup_pca_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "fer_mean_sup_pca_umap_projection_train_10_01 = np.mean(fer_sup_pca_umap_projections_train_10_01, axis=0)\n",
    "fer_std_sup_pca_umap_projection_train_10_01 = np.std(fer_sup_pca_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "fer_mean_sup_pca_umap_projection_test_10_01 = np.mean(fer_sup_pca_umap_projections_test_10_01, axis=0)\n",
    "fer_std_sup_pca_umap_projection_test_10_01 = np.std(fer_sup_pca_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('fer_sup_pca_umap_projections_train_10_01.npy', fer_sup_pca_umap_projections_train_10_01)\n",
    "np.save('fer_mean_sup_pca_umap_projection_train_10_01.npy', fer_mean_sup_pca_umap_projection_train_10_01)\n",
    "np.save('fer_std_sup_pca_umap_projection_train_10_01.npy', fer_std_sup_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('fer_sup_pca_umap_projections_test_10_01.npy', fer_sup_pca_umap_projections_test_10_01)\n",
    "np.save('fer_mean_sup_pca_umap_projection_test_10_01.npy', fer_mean_sup_pca_umap_projection_test_10_01)\n",
    "np.save('fer_std_sup_pca_umap_projection_test_10_01.npy', fer_std_sup_pca_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "unique_labels = np.unique(y_train_fer2013)\n",
    "cmap = plt.cm.get_cmap(\"tab10\", len(unique_labels))\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    fer_mean_sup_pca_umap_projection_train_10_01[:, 0],\n",
    "    fer_mean_sup_pca_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train_fer2013, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"PCA + Supervised UMAP Projection of FER2013 Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(0, 7))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(0, 7)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_fer_pca_umap_sup_10_01 = adjusted_rand_score(y_test_fer2013, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_sup_pca_umap_projection_train_10_01, y_train_fer2013).predict(fer_mean_sup_pca_umap_projection_test_10_01))\n",
    "print(f\"ARI: {ari_fer_pca_umap_sup_10_01:.4f}\")\n",
    "# Silhouette Score\n",
    "silhouette_fer_pca_umap_sup_10_01 = silhouette_score(fer_mean_sup_pca_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_sup_pca_umap_projection_train_10_01, y_train_fer2013).predict(fer_mean_sup_pca_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_fer_pca_umap_sup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train_fer2013))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(fer_mean_sup_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "fer_sup_umap_pca_projection_10_01_db_score = davies_bouldin_score(\n",
    "    fer_mean_sup_pca_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {fer_sup_umap_pca_projection_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gabor filters + PCA + UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gabor filters + PCA + Unsupervised UMAP 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "fer_unsup_gabor_pca_umap_projections_train_10_01= np.load('fer_unsup_gabor_pca_umap_projections_train_10_01.npy')\n",
    "fer_mean_unsup_gabor_pca_umap_projection_train_10_01= np.load('fer_mean_unsup_gabor_pca_umap_projection_train_10_01.npy')\n",
    "fer_std_unsup_gabor_pca_umap_projection_train_10_01= np.load('fer_std_unsup_gabor_pca_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "fer_unsup_gabor_pca_umap_projections_test_10_01= np.load('fer_unsup_gabor_pca_umap_projections_test_10_01.npy')\n",
    "fer_mean_unsup_gabor_pca_umap_projection_test_10_01= np.load('fer_mean_unsup_gabor_pca_umap_projection_test_10_01.npy')\n",
    "fer_std_unsup_gabor_pca_umap_projection_test_10_01= np.load('fer_std_unsup_gabor_pca_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gabor Kernels\n",
    "def create_gabor_kernels():\n",
    "    \"\"\"Generates a set of Gabor kernels with different orientations and frequencies.\"\"\"\n",
    "    kernels = []\n",
    "    ksize = 31  # Kernel size\n",
    "    sigma = 4.0  # Standard deviation of the Gaussian envelope\n",
    "    lambd = 10.0  # Wavelength of the sinusoidal factor\n",
    "    gamma = 0.5  # Spatial aspect ratio\n",
    "    for theta in np.arange(0, np.pi, np.pi / 4):  # 8 orientations\n",
    "        kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lambd, gamma, psi=0, ktype=cv2.CV_32F)\n",
    "        kernels.append(kernel)\n",
    "    return kernels\n",
    "\n",
    "# Apply Gabor Filters\n",
    "def apply_gabor_filters(images, kernels):\n",
    "    \"\"\"Applies a set of Gabor filters to a batch of images.\"\"\"\n",
    "    gabor_features = []\n",
    "    for image in images:\n",
    "        image_2d = image.reshape(48, 48)  # Reshape back to 2D (assumes 48x48 images)\n",
    "        responses = []\n",
    "        for kernel in kernels:\n",
    "            filtered = cv2.filter2D(image_2d, cv2.CV_32F, kernel)  # Apply Gabor filter\n",
    "            responses.append(filtered.flatten())  # Flatten the filtered image\n",
    "        gabor_features.append(np.concatenate(responses))  # Concatenate all filter responses\n",
    "    return np.array(gabor_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Gabor kernels\n",
    "gabor_kernels = create_gabor_kernels()\n",
    "print(f\"Generated {len(gabor_kernels)} Gabor kernels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Gabor filters to the training and test sets\n",
    "fer_x_train_gabor = apply_gabor_filters(x_train_fer2013, gabor_kernels)\n",
    "fer_x_test_gabor = apply_gabor_filters(x_test_fer2013, gabor_kernels)\n",
    "\n",
    "print(f\"Train Gabor feature shape: {fer_x_train_gabor.shape}\")\n",
    "print(f\"Test Gabor feature shape: {fer_x_test_gabor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "fer_x_train_gabor = scaler.fit_transform(fer_x_train_gabor)\n",
    "fer_x_test_gabor = scaler.transform(fer_x_test_gabor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying PCA to Gabor filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.95)\n",
    "x_fer_train_gabor_pca = pca.fit_transform(fer_x_train_gabor)\n",
    "x_fer_test_gabor_pca = pca.transform(fer_x_test_gabor)\n",
    "\n",
    "print(f\"Reduced train shape: {x_fer_train_gabor_pca.shape}\")\n",
    "print(f\"Reduced test shape: {x_fer_test_gabor_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_raf_train_gabor_pca.npy', x_fer_train_gabor_pca)\n",
    "np.save('x_raf_test_gabor_pca.npy', x_fer_test_gabor_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying UMAP to PCA results from Gabor Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "fer_unsup_gabor_pca_umap_projections_train_10_01 = []\n",
    "fer_unsup_gabor_pca_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_fer_train_gabor_pca)\n",
    "    fer_unsup_gabor_pca_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_fer_test_gabor_pca)\n",
    "    fer_unsup_gabor_pca_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "fer_unsup_gabor_pca_umap_projections_train_10_01 = np.array(fer_unsup_gabor_pca_umap_projections_train_10_01)\n",
    "fer_unsup_gabor_pca_umap_projections_test_10_01 = np.array(fer_unsup_gabor_pca_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "fer_mean_unsup_gabor_pca_umap_projection_train_10_01 = np.mean(fer_unsup_gabor_pca_umap_projections_train_10_01, axis=0)\n",
    "fer_std_unsup_gabor_pca_umap_projection_train_10_01 = np.std(fer_unsup_gabor_pca_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "fer_mean_unsup_gabor_pca_umap_projection_test_10_01 = np.mean(fer_unsup_gabor_pca_umap_projections_test_10_01, axis=0)\n",
    "fer_std_unsup_gabor_pca_umap_projection_test_10_01 = np.std(fer_unsup_gabor_pca_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('fer_unsup_gabor_pca_umap_projections_train_10_01.npy', fer_unsup_gabor_pca_umap_projections_train_10_01)\n",
    "np.save('fer_mean_unsup_gabor_pca_umap_projection_train_10_01.npy', fer_mean_unsup_gabor_pca_umap_projection_train_10_01)\n",
    "np.save('fer_std_unsup_gabor_pca_umap_projection_train_10_01.npy', fer_std_unsup_gabor_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('fer_unsup_gabor_pca_umap_projections_test_10_01.npy', fer_unsup_gabor_pca_umap_projections_test_10_01)\n",
    "np.save('fer_mean_unsup_gabor_pca_umap_projection_test_10_01.npy', fer_mean_unsup_gabor_pca_umap_projection_test_10_01)\n",
    "np.save('fer_std_unsup_gabor_pca_umap_projection_test_10_01.npy', fer_std_unsup_gabor_pca_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "unique_labels = np.unique(y_train_fer2013)\n",
    "cmap = plt.cm.get_cmap(\"tab10\", len(unique_labels))\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    fer_mean_unsup_gabor_pca_umap_projection_train_10_01[:, 0],\n",
    "    fer_mean_unsup_gabor_pca_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train_fer2013, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Gabor + PCA + Unsupervised UMAP Projection of FER2013 Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(0, 7))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(0, 7)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_fer_gabor_pca_umap_unsup_10_01 = adjusted_rand_score(y_test_fer2013, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_unsup_gabor_pca_umap_projection_train_10_01, y_train_fer2013).predict(fer_mean_unsup_gabor_pca_umap_projection_test_10_01)) # second argument is y_test_fer2013_pred_gabor_pca\n",
    "print(f\"ARI: {ari_fer_gabor_pca_umap_unsup_10_01:.4f}\")\n",
    "# Silhouette Score\n",
    "silhouette_fer_gabor_pca_umap_unsup_10_01 = silhouette_score(fer_mean_unsup_gabor_pca_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_unsup_gabor_pca_umap_projection_train_10_01, y_train_fer2013).predict(fer_mean_unsup_gabor_pca_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_fer_gabor_pca_umap_unsup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train_fer2013))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(fer_mean_unsup_gabor_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "fer_unsup_gabor_pca_umap_projection_10_01_db_score = davies_bouldin_score(\n",
    "    fer_mean_unsup_gabor_pca_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {fer_unsup_gabor_pca_umap_projection_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gabor filters + PCA + UMAP Supervised 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "fer_sup_gabor_pca_umap_projections_train_10_01= np.load('fer_sup_gabor_pca_umap_projections_train_10_01.npy')\n",
    "fer_mean_sup_gabor_pca_umap_projection_train_10_01= np.load('fer_mean_sup_gabor_pca_umap_projection_train_10_01.npy')\n",
    "fer_std_sup_gabor_pca_umap_projection_train_10_01= np.load('fer_std_sup_gabor_pca_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "fer_sup_gabor_pca_umap_projections_test_10_01= np.load('fer_sup_gabor_pca_umap_projections_test_10_01.npy')\n",
    "fer_mean_sup_gabor_pca_umap_projection_test_10_01= np.load('fer_mean_sup_gabor_pca_umap_projection_test_10_01.npy')\n",
    "fer_std_sup_gabor_pca_umap_projection_test_10_01= np.load('fer_std_sup_gabor_pca_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label information added to results from Unsupervised Gabor filters + PCA when computing UMAP using them as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for training and test sets\n",
    "fer_sup_gabor_pca_umap_projections_train_10_01 = []\n",
    "fer_sup_gabor_pca_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running Supervised UMAP - Iteration {run + 1}/{n_runs}...\")\n",
    "\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, \n",
    "        min_dist=min_dist, \n",
    "        n_components=n_components, \n",
    "        random_state=run\n",
    "    )\n",
    "\n",
    "    # Fit and transform the training data with labels\n",
    "    projection_train = umap_model.fit_transform(x_fer_train_gabor_pca, y_train_fer2013)\n",
    "    fer_sup_gabor_pca_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_fer_test_gabor_pca)\n",
    "    fer_sup_gabor_pca_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "fer_sup_gabor_pca_umap_projections_train_10_01 = np.array(fer_sup_gabor_pca_umap_projections_train_10_01)\n",
    "fer_sup_gabor_pca_umap_projections_test_10_01 = np.array(fer_sup_gabor_pca_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "fer_mean_sup_gabor_pca_umap_projection_train_10_01 = np.mean(fer_sup_gabor_pca_umap_projections_train_10_01, axis=0)\n",
    "fer_std_sup_gabor_pca_umap_projection_train_10_01 = np.std(fer_sup_gabor_pca_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "fer_mean_sup_gabor_pca_umap_projection_test_10_01 = np.mean(fer_sup_gabor_pca_umap_projections_test_10_01, axis=0)\n",
    "fer_std_sup_gabor_pca_umap_projection_test_10_01 = np.std(fer_sup_gabor_pca_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('fer_sup_gabor_pca_umap_projections_train_10_01.npy', fer_sup_gabor_pca_umap_projections_train_10_01)\n",
    "np.save('fer_mean_sup_gabor_pca_umap_projection_train_10_01.npy', fer_mean_sup_gabor_pca_umap_projection_train_10_01)\n",
    "np.save('fer_std_sup_gabor_pca_umap_projection_train_10_01.npy', fer_std_sup_gabor_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('fer_sup_gabor_pca_umap_projections_test_10_01.npy', fer_sup_gabor_pca_umap_projections_test_10_01)\n",
    "np.save('fer_mean_sup_gabor_pca_umap_projection_test_10_01.npy', fer_mean_sup_gabor_pca_umap_projection_test_10_01)\n",
    "np.save('fer_std_sup_gabor_pca_umap_projection_test_10_01.npy', fer_std_sup_gabor_pca_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"Supervised UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "unique_labels = np.unique(y_train_fer2013)\n",
    "cmap = plt.cm.get_cmap(\"tab10\", len(unique_labels))\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    fer_mean_sup_gabor_pca_umap_projection_train_10_01[:, 0],\n",
    "    fer_mean_sup_gabor_pca_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train_fer2013, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Gabor + PCA + Supervised UMAP Projection of FER2013 Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(0, 7))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(0, 7)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_fer_gabor_pca_umap_sup_10_01 = adjusted_rand_score(y_test_fer2013, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_sup_gabor_pca_umap_projection_train_10_01, y_train_fer2013).predict(fer_mean_sup_gabor_pca_umap_projection_test_10_01)) # second argument is y_test_fer2013_pred_gabor_pca\n",
    "print(f\"ARI: {ari_fer_gabor_pca_umap_sup_10_01:.4f}\")\n",
    "# Silhouette Score\n",
    "silhouette_fer_gabor_pca_umap_sup_10_01 = silhouette_score(fer_mean_sup_gabor_pca_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_sup_gabor_pca_umap_projection_train_10_01, y_train_fer2013).predict(fer_mean_sup_gabor_pca_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_fer_gabor_pca_umap_sup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train_fer2013))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(fer_mean_sup_gabor_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "fer_sup_gabor_pca_umap_projection_10_01_db_score = davies_bouldin_score(\n",
    "    fer_mean_sup_gabor_pca_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {fer_sup_gabor_pca_umap_projection_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLE + UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_fer_consistent(x_data, y_labels, sample_fraction=0.75):\n",
    "    \"\"\"\n",
    "    Downsample the dataset consistently, returning indices to ensure\n",
    "    the same points are selected in both spaces.\n",
    "    \n",
    "    Parameters:\n",
    "        x_data (np.array): Input data to downsample.\n",
    "        y_labels (np.array): Corresponding labels.\n",
    "        sample_fraction (float): Fraction of samples to retain per label.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Array of selected indices.\n",
    "    \"\"\"\n",
    "    sampled_indices = []\n",
    "    unique_labels = np.unique(y_labels)\n",
    "\n",
    "    for label in unique_labels:\n",
    "        # Select indices for the current label\n",
    "        label_indices = np.where(y_labels == label)[0]\n",
    "\n",
    "        # Handle cases with very few samples\n",
    "        n_samples = max(1, int(len(label_indices) * sample_fraction))\n",
    "        if n_samples > len(label_indices):\n",
    "            n_samples = len(label_indices)\n",
    "\n",
    "        # Sample a fraction of points for this label\n",
    "        sampled_indices_label = resample(\n",
    "            label_indices, n_samples=n_samples, replace=False, random_state=42\n",
    "        )\n",
    "        sampled_indices.extend(sampled_indices_label)\n",
    "\n",
    "    return np.array(sampled_indices)\n",
    "\n",
    "# Downsample training data\n",
    "fer_sampled_indices_train = downsample_fer_consistent(x_train_fer2013, y_train_fer2013, sample_fraction=0.75)\n",
    "x_train_fer_emotions_sampled = x_train_fer2013[fer_sampled_indices_train]\n",
    "y_train_fer_emotions_sampled = y_train_fer2013[fer_sampled_indices_train]\n",
    "\n",
    "# Downsample test data\n",
    "fer_sampled_indices_test = downsample_fer_consistent(x_test_fer2013, y_test_fer2013, sample_fraction=0.75)\n",
    "x_test_fer_emotions_sampled = x_test_fer2013[fer_sampled_indices_test]\n",
    "y_test_fer_emotions_sampled = y_test_fer2013[fer_sampled_indices_test]\n",
    "\n",
    "# Print results\n",
    "print(f\"Training set reduced to {len(x_train_fer_emotions_sampled)} samples.\")\n",
    "print(f\"Test set reduced to {len(x_test_fer_emotions_sampled)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for LLE\n",
    "n_neighbors = 10\n",
    "n_components = 253 # Same as the reduced from PCA\n",
    "\n",
    "# Initialize the LLE model\n",
    "lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components, method='standard', random_state=42)\n",
    "\n",
    "# Fit and transform the training data\n",
    "print(\"Running LLE on the training set...\")\n",
    "x_train_fer_lle = lle.fit_transform(x_train_fer_emotions_sampled)\n",
    "print(\"LLE transformation on training set completed.\")\n",
    "\n",
    "# Transform the test data using the fitted LLE model\n",
    "print(\"Running LLE on the test set...\")\n",
    "x_test_fer_lle = lle.transform(x_test_fer_emotions_sampled)\n",
    "print(\"LLE transformation on test set completed.\")\n",
    "\n",
    "# Print shapes of transformed data\n",
    "print(f\"Shape of LLE-transformed training data: {x_train_fer_lle.shape}\")\n",
    "print(f\"Shape of LLE-transformed test data: {x_test_fer_lle.shape}\")\n",
    "\n",
    "# Optional: Save the LLE-transformed data for later use\n",
    "np.save('x_train_lle.npy', x_train_fer_lle)\n",
    "np.save('x_test_lle.npy', x_test_fer_lle)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"LLE-transformed data has been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLE + UMAP Unsupervised 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the projections, mean, and standard deviation for the training set\n",
    "fer_unsup_lle_umap_projections_train_10_01= np.load('fer_unsup_lle_umap_projections_train_10_01.npy')\n",
    "fer_mean_unsup_lle_umap_projection_train_10_01= np.load('fer_mean_unsup_lle_umap_projection_train_10_01.npy')\n",
    "fer_std_unsup_lle_umap_projection_train_10_01= np.load('fer_std_unsup_lle_umap_projection_train_10_01.npy')\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "fer_unsup_lle_umap_projections_test_10_01= np.load('fer_unsup_lle_umap_projections_test_10_01.npy')\n",
    "fer_mean_unsup_lle_umap_projection_test_10_01= np.load('fer_mean_unsup_lle_umap_projection_test_10_01.npy')\n",
    "fer_std_unsup_lle_umap_projection_test_10_01= np.load('fer_std_unsup_lle_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "fer_unsup_lle_umap_projections_train_10_01 = []\n",
    "fer_unsup_lle_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_train_fer_lle)\n",
    "    fer_unsup_lle_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_fer_lle)\n",
    "    fer_unsup_lle_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "fer_unsup_lle_umap_projections_train_10_01 = np.array(fer_unsup_lle_umap_projections_train_10_01)\n",
    "fer_unsup_lle_umap_projections_test_10_01 = np.array(fer_unsup_lle_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "fer_mean_unsup_lle_umap_projection_train_10_01 = np.mean(fer_unsup_lle_umap_projections_train_10_01, axis=0)\n",
    "fer_std_unsup_lle_umap_projection_train_10_01 = np.std(fer_unsup_lle_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "fer_mean_unsup_lle_umap_projection_test_10_01 = np.mean(fer_unsup_lle_umap_projections_test_10_01, axis=0)\n",
    "fer_std_unsup_lle_umap_projection_test_10_01 = np.std(fer_unsup_lle_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('fer_unsup_lle_umap_projections_train_10_01.npy', fer_unsup_lle_umap_projections_train_10_01)\n",
    "np.save('fer_mean_unsup_lle_umap_projection_train_10_01.npy', fer_mean_unsup_lle_umap_projection_train_10_01)\n",
    "np.save('fer_std_unsup_lle_umap_projection_train_10_01.npy', fer_std_unsup_lle_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('fer_unsup_lle_umap_projections_test_10_01.npy', fer_unsup_lle_umap_projections_test_10_01)\n",
    "np.save('fer_mean_unsup_lle_umap_projection_test_10_01.npy', fer_mean_unsup_lle_umap_projection_test_10_01)\n",
    "np.save('fer_std_unsup_lle_umap_projection_test_10_01.npy', fer_std_unsup_lle_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "unique_labels = np.unique(y_train_fer_emotions_sampled)\n",
    "cmap = plt.cm.get_cmap(\"tab10\", len(unique_labels))\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    fer_mean_unsup_lle_umap_projection_train_10_01[:, 0],\n",
    "    fer_mean_unsup_lle_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train_fer_emotions_sampled, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"LLE + Unsupervised UMAP Projection of FER2013 Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(0, 7))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(0, 7)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_fer_lle_umap_unsup_10_01 = adjusted_rand_score(y_test_fer_emotions_sampled, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_unsup_lle_umap_projection_train_10_01, y_train_fer_emotions_sampled).predict(fer_mean_unsup_lle_umap_projection_test_10_01)) # second argument is y_test_pred_lle\n",
    "print(f\"ARI: {ari_fer_lle_umap_unsup_10_01:.2f}\")\n",
    "# Silhouette Score\n",
    "silhouette_fer_lle_umap_unsup_10_01 = silhouette_score(fer_mean_unsup_lle_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_unsup_lle_umap_projection_train_10_01, y_train_fer_emotions_sampled).predict(fer_mean_unsup_lle_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_fer_lle_umap_unsup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train_fer2013))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(fer_mean_unsup_lle_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "fer_lle_umap_unsup_10_01_db_score = davies_bouldin_score(\n",
    "    fer_mean_unsup_lle_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {fer_lle_umap_unsup_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLE + UMAP Supervised 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the projections, mean, and standard deviation for the training set\n",
    "fer_sup_lle_umap_projections_train_10_01= np.load('fer_sup_lle_umap_projections_train_10_01.npy')\n",
    "fer_mean_sup_lle_umap_projection_train_10_01= np.load('fer_mean_sup_lle_umap_projection_train_10_01.npy')\n",
    "fer_std_sup_lle_umap_projection_train_10_01= np.load('fer_std_sup_lle_umap_projection_train_10_01.npy')\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "fer_sup_lle_umap_projections_test_10_01= np.load('fer_sup_lle_umap_projections_test_10_01.npy')\n",
    "fer_mean_sup_lle_umap_projection_test_10_01= np.load('fer_mean_sup_lle_umap_projection_test_10_01.npy')\n",
    "fer_std_sup_lle_umap_projection_test_10_01= np.load('fer_std_sup_lle_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "fer_sup_lle_umap_projections_train_10_01 = []\n",
    "fer_sup_lle_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_train_fer_lle, y_train_fer_emotions_sampled)\n",
    "    fer_sup_lle_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_fer_lle)\n",
    "    fer_sup_lle_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "fer_sup_lle_umap_projections_train_10_01 = np.array(fer_sup_lle_umap_projections_train_10_01)\n",
    "fer_sup_lle_umap_projections_test_10_01 = np.array(fer_sup_lle_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "fer_mean_sup_lle_umap_projection_train_10_01 = np.mean(fer_sup_lle_umap_projections_train_10_01, axis=0)\n",
    "fer_std_sup_lle_umap_projection_train_10_01 = np.std(fer_sup_lle_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "fer_mean_sup_lle_umap_projection_test_10_01 = np.mean(fer_sup_lle_umap_projections_test_10_01, axis=0)\n",
    "fer_std_sup_lle_umap_projection_test_10_01 = np.std(fer_sup_lle_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('fer_sup_lle_umap_projections_train_10_01.npy', fer_sup_lle_umap_projections_train_10_01)\n",
    "np.save('fer_mean_sup_lle_umap_projection_train_10_01.npy', fer_mean_sup_lle_umap_projection_train_10_01)\n",
    "np.save('fer_std_sup_lle_umap_projection_train_10_01.npy', fer_std_sup_lle_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('fer_sup_lle_umap_projections_test_10_01.npy', fer_sup_lle_umap_projections_test_10_01)\n",
    "np.save('fer_mean_sup_lle_umap_projection_test_10_01.npy', fer_mean_sup_lle_umap_projection_test_10_01)\n",
    "np.save('fer_std_sup_lle_umap_projection_test_10_01.npy', fer_std_sup_lle_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "unique_labels = np.unique(y_train_fer_emotions_sampled)\n",
    "cmap = plt.cm.get_cmap(\"tab10\", len(unique_labels))\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    fer_mean_sup_lle_umap_projection_train_10_01[:, 0],\n",
    "    fer_mean_sup_lle_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train_fer_emotions_sampled, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"LLE + Supervised UMAP Projection of FER2013 Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(0, 7))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(0, 7)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_fer_lle_umap_sup_10_01 = adjusted_rand_score(y_test_fer_emotions_sampled, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_sup_lle_umap_projection_train_10_01, y_train_fer_emotions_sampled).predict(fer_mean_sup_lle_umap_projection_test_10_01)) # second argument is y_test_pred_lle\n",
    "print(f\"ARI: {ari_fer_lle_umap_sup_10_01:.2f}\")\n",
    "# Silhouette Score\n",
    "silhouette_fer_lle_umap_sup_10_01 = silhouette_score(fer_mean_sup_lle_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(fer_mean_sup_lle_umap_projection_train_10_01, y_train_fer_emotions_sampled).predict(fer_mean_sup_lle_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_fer_lle_umap_sup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train_fer2013))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(fer_mean_sup_lle_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "fer_lle_umap_sup_10_01_db_score = davies_bouldin_score(\n",
    "    fer_mean_sup_lle_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {fer_lle_umap_sup_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAF-DB Import and images preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionsDataloader(object):\n",
    "    def __init__(self, image_dir, label_file):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_file = label_file\n",
    "\n",
    "    def read_images_labels(self):  \n",
    "        train_images, train_labels = [], []\n",
    "        test_images, test_labels = [], []\n",
    "\n",
    "        # Read the label file and match labels to images\n",
    "        with open(self.label_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 2:\n",
    "                    image_name, label = parts[0], int(parts[1])\n",
    "                    aligned_image_name = f\"{image_name.split('.')[0]}_aligned.jpg\"\n",
    "                    image_path = os.path.join(self.image_dir, aligned_image_name)\n",
    "\n",
    "                    if os.path.exists(image_path):\n",
    "                        image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "                        image = image.resize((48, 48))  # Resize to 48x48\n",
    "\n",
    "                        # Check if the label belongs to train or test set\n",
    "                        if \"train\" in image_name:\n",
    "                            train_images.append(np.array(image))\n",
    "                            train_labels.append(label)\n",
    "                        elif \"test\" in image_name:\n",
    "                            test_images.append(np.array(image))\n",
    "                            test_labels.append(label)\n",
    "                    else:\n",
    "                        print(f\"Image not found: {aligned_image_name}\")\n",
    "\n",
    "        print(f\"Loaded {len(train_images)} training images and {len(test_images)} test images.\")\n",
    "        print(f\"Loaded {len(train_labels)} training labels and {len(test_labels)} test labels.\")\n",
    "\n",
    "        return (\n",
    "            (np.array(train_images), np.array(train_labels)),\n",
    "            (np.array(test_images), np.array(test_labels))\n",
    "        )\n",
    "\n",
    "    def load_data(self):\n",
    "        return self.read_images_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "input_path = 'C:/Users/Lorenzo/OneDrive/Documents/DTU/Python/2024 Fall/MSc Thesis'\n",
    "image_dir = os.path.join(input_path, 'extracted_data/Image/aligned')\n",
    "label_file = os.path.join(input_path, 'extracted_data/EmoLabel/list_patition_label.txt')\n",
    "\n",
    "# Instantiate and load the dataset\n",
    "emotions_dataloader = EmotionsDataloader(image_dir, label_file)\n",
    "(x_train, y_train), (x_test, y_test) = emotions_dataloader.load_data()\n",
    "\n",
    "# Print dataset shapes\n",
    "print(f\"Training set shape: {x_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set shape: {x_test.shape}, {y_test.shape}\")\n",
    "\n",
    "# Display some random train and test images\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images) / cols) + 1\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (image, title) in enumerate(zip(images, title_texts)):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        plt.title(title, fontsize=12)\n",
    "        plt.axis('off')\n",
    "\n",
    "# Show some random train and test images\n",
    "images_to_show = []\n",
    "titles_to_show = []\n",
    "\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(0, len(x_train))\n",
    "    images_to_show.append(x_train[idx])\n",
    "    titles_to_show.append(f\"Train[{idx}] = {y_train[idx]}\")\n",
    "\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(0, len(x_test))\n",
    "    images_to_show.append(x_test[idx])\n",
    "    titles_to_show.append(f\"Test[{idx}] = {y_test[idx]}\")\n",
    "\n",
    "show_images(images_to_show, titles_to_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YES ###\n",
    "\n",
    "assert len(x_train) == len(y_train), \"Mismatch in training images and labels!\"\n",
    "assert len(x_test) == len(y_test), \"Mismatch in test images and labels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YES ###\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Flatten the Images into 1D Vectors\n",
    "x_train_flattened = x_train.reshape(x_train.shape[0], -1)  # Flatten to (num_samples, 2304)\n",
    "x_test_flattened = x_test.reshape(x_test.shape[0], -1)    # Flatten to (num_samples, 2304)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Shape of x_train_flattened:\", x_train_flattened.shape)  # (num_train_samples, 2304)\n",
    "print(\"Shape of x_test_flattened:\", x_test_flattened.shape)    # (num_test_samples, 2304)\n",
    "\n",
    "# Step 2: Normalize the Flattened Data\n",
    "scaler = StandardScaler()\n",
    "x_train_emotion_norm = scaler.fit_transform(x_train_flattened)\n",
    "x_test_emotion_norm = scaler.transform(x_test_flattened)\n",
    "\n",
    "# Verify normalization\n",
    "print(\"x_train_norma mean:\", x_train_emotion_norm.mean(axis=0).mean())  # ~0\n",
    "print(\"x_train_norma std:\", x_test_emotion_norm.std(axis=0).mean())    # ~1\n",
    "\n",
    "# Print final shapes\n",
    "print(\"Final shape of x_train_norm:\", x_train_emotion_norm.shape)\n",
    "print(\"Final shape of x_test_norm:\", x_test_emotion_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YES ###\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"Missing values in x_train_emotion_norm: {np.isnan(x_train_emotion_norm).sum()}\")\n",
    "print(f\"Missing values in x_test_emotion_norm: {np.isnan(x_test_emotion_norm).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised UMAP 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "raf_unsup_umap_projections_train_10_01= np.load('raf_unsup_umap_projections_train_10_01.npy')\n",
    "raf_mean_unsup_umap_projection_train_10_01= np.load('raf_mean_unsup_umap_projection_train_10_01.npy')\n",
    "raf_std_unsup_umap_projection_train_10_01= np.load('raf_std_unsup_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "raf_unsup_umap_projections_test_10_01= np.load('raf_unsup_umap_projections_test_10_01.npy')\n",
    "raf_mean_unsup_umap_projection_test_10_01= np.load('raf_mean_unsup_umap_projection_test_10_01.npy')\n",
    "raf_std_unsup_umap_projection_test_10_01= np.load('raf_std_unsup_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "raf_unsup_umap_projections_train_10_01 = []\n",
    "raf_unsup_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_train_emotion_norm)\n",
    "    raf_unsup_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_emotion_norm)\n",
    "    raf_unsup_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_unsup_umap_projections_train_10_01 = np.array(raf_unsup_umap_projections_train_10_01)\n",
    "raf_unsup_umap_projections_test_10_01 = np.array(raf_unsup_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_unsup_umap_projection_train_10_01 = np.mean(raf_unsup_umap_projections_train_10_01, axis=0)\n",
    "raf_std_unsup_umap_projection_train_10_01 = np.std(raf_unsup_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_unsup_umap_projection_test_10_01 = np.mean(raf_unsup_umap_projections_test_10_01, axis=0)\n",
    "raf_std_unsup_umap_projection_test_10_01 = np.std(raf_unsup_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_unsup_umap_projections_train_10_01.npy', raf_unsup_umap_projections_train_10_01)\n",
    "np.save('raf_mean_unsup_umap_projection_train_10_01.npy', raf_mean_unsup_umap_projection_train_10_01)\n",
    "np.save('raf_std_unsup_umap_projection_train_10_01.npy', raf_std_unsup_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_unsup_umap_projections_test_10_01.npy', raf_unsup_umap_projections_test_10_01)\n",
    "np.save('raf_mean_unsup_umap_projection_test_10_01.npy', raf_mean_unsup_umap_projection_test_10_01)\n",
    "np.save('raf_std_unsup_umap_projection_test_10_01.npy', raf_std_unsup_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the projections, mean, and standard deviation for the training set\n",
    "raf_unsup_umap_projections_train_10_01= np.load('raf_unsup_umap_projections_train_10_01.npy')\n",
    "raf_mean_unsup_umap_projection_train_10_01= np.load('raf_mean_unsup_umap_projection_train_10_01.npy')\n",
    "raf_std_unsup_umap_projection_train_10_01= np.load('raf_std_unsup_umap_projection_train_10_01.npy')\n",
    "\n",
    "# Load the projections, mean, and standard deviation for the test set\n",
    "raf_unsup_umap_projections_test_10_01= np.load('raf_unsup_umap_projections_test_10_01.npy')\n",
    "raf_mean_unsup_umap_projection_test_10_01= np.load('raf_mean_unsup_umap_projection_test_10_01.npy')\n",
    "raf_std_unsup_umap_projection_test_10_01= np.load('raf_std_unsup_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_unsup_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_unsup_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Unsupervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_umap_unsup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_umap_projection_test_10_01)) # second argument is y_test_pred_pca\n",
    "print(f\"ARI: {ari_raf_umap_unsup_10_01:.4f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_umap_unsup_10_01 = silhouette_score(raf_mean_unsup_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_umap_unsup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_unsup_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_mean_unsup_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_unsup_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_mean_unsup_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised UMAP 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "raf_sup_umap_projections_train_10_01= np.load('raf_sup_umap_projections_train_10_01.npy')\n",
    "raf_mean_sup_umap_projection_train_10_01= np.load('raf_mean_sup_umap_projection_train_10_01.npy')\n",
    "raf_std_sup_umap_projection_train_10_01= np.load('raf_std_sup_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "raf_sup_umap_projections_test_10_01= np.load('raf_sup_umap_projections_test_10_01.npy')\n",
    "raf_mean_sup_umap_projection_test_10_01= np.load('raf_mean_sup_umap_projection_test_10_01.npy')\n",
    "raf_std_sup_umap_projection_test_10_01= np.load('raf_std_sup_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for training and test sets\n",
    "raf_sup_umap_projections_train_10_01 = []\n",
    "raf_sup_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running Supervised UMAP - Iteration {run + 1}/{n_runs}...\")\n",
    "\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, \n",
    "        min_dist=min_dist, \n",
    "        n_components=n_components, \n",
    "        random_state=run\n",
    "    )\n",
    "\n",
    "    # Fit and transform the training data with labels\n",
    "    projection_train = umap_model.fit_transform(x_train_emotion_norm, y_train)\n",
    "    raf_sup_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_emotion_norm)\n",
    "    raf_sup_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_sup_umap_projections_train_10_01 = np.array(raf_sup_umap_projections_train_10_01)\n",
    "raf_sup_umap_projections_test_10_01 = np.array(raf_sup_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_sup_umap_projection_train_10_01 = np.mean(raf_sup_umap_projections_train_10_01, axis=0)\n",
    "raf_std_sup_umap_projection_train_10_01 = np.std(raf_sup_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_sup_umap_projection_test_10_01 = np.mean(raf_sup_umap_projections_test_10_01, axis=0)\n",
    "raf_std_sup_umap_projection_test_10_01 = np.std(raf_sup_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_sup_umap_projections_train_10_01.npy', raf_sup_umap_projections_train_10_01)\n",
    "np.save('raf_mean_sup_umap_projection_train_10_01.npy', raf_mean_sup_umap_projection_train_10_01)\n",
    "np.save('raf_std_sup_umap_projection_train_10_01.npy', raf_std_sup_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_sup_umap_projections_test_10_01.npy', raf_sup_umap_projections_test_10_01)\n",
    "np.save('raf_mean_sup_umap_projection_test_10_01.npy', raf_mean_sup_umap_projection_test_10_01)\n",
    "np.save('raf_std_sup_umap_projection_test_10_01.npy', raf_std_sup_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"Supervised UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_sup_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_sup_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Supervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_umap_sup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_umap_projection_train_10_01, y_train).predict(raf_mean_sup_umap_projection_test_10_01)) # second argument is y_test_pred_pca\n",
    "print(f\"ARI: {ari_raf_umap_sup_10_01:.4f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_umap_sup_10_01 = silhouette_score(raf_mean_sup_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_umap_projection_train_10_01, y_train).predict(raf_mean_sup_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_umap_sup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_sup_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_mean_sup_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_sup_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_mean_sup_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Load the saved mean UMAP projections\n",
    "# mean_projection = np.load('raf_mean_sup_umap_projection_10_01.npy')  # Shape: (n_samples, 2)\n",
    "\n",
    "# # Step 2: Separate the mean projection by class\n",
    "# classes = np.unique(y_train)\n",
    "# class_gaussians = {}\n",
    "\n",
    "# # Calculate the mean and covariance for each class\n",
    "# for c in classes:\n",
    "#     class_points = mean_projection[y_train == c]  # Filter by class\n",
    "#     mean = np.mean(class_points, axis=0)\n",
    "#     cov = np.cov(class_points, rowvar=False)\n",
    "#     class_gaussians[c] = {\"mean\": mean, \"cov\": cov}\n",
    "\n",
    "# # Step 3: Visualize Gaussian distributions\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# # Plot UMAP embeddings for each class\n",
    "# for c in classes:\n",
    "#     class_points = mean_projection[y_train == c]\n",
    "#     plt.scatter(class_points[:, 0], class_points[:, 1], label=f\"Class {c}\", alpha=0.5, s=10)\n",
    "\n",
    "#     # Plot Gaussian contours\n",
    "#     mean = class_gaussians[c][\"mean\"]\n",
    "#     cov = class_gaussians[c][\"cov\"]\n",
    "#     x, y = np.meshgrid(\n",
    "#         np.linspace(mean[0] - 3, mean[0] + 3, 100), \n",
    "#         np.linspace(mean[1] - 3, mean[1] + 3, 100)\n",
    "#     )\n",
    "#     pos = np.dstack((x, y))\n",
    "#     rv = multivariate_normal(mean, cov)\n",
    "#     plt.contour(x, y, rv.pdf(pos), levels=5, alpha=0.8)\n",
    "\n",
    "# plt.title(\"UMAP Mean Projections with Gaussian Distributions per Class\")\n",
    "# plt.xlabel(\"UMAP Component 1\")\n",
    "# plt.ylabel(\"UMAP Component 2\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Step 4: Evaluate likelihood for a random point\n",
    "# random_point = np.array([0, 0])  # Example point in UMAP space\n",
    "# likelihoods = {c: multivariate_normal(class_gaussians[c][\"mean\"], class_gaussians[c][\"cov\"]).pdf(random_point)\n",
    "#                for c in classes}\n",
    "\n",
    "# print(\"Likelihoods for Random Point:\", likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA + UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply PCA\n",
    "pca = PCA(0.95)\n",
    "x_train_pca_emotions = pca.fit_transform(x_train_emotion_norm)\n",
    "x_test_pca_emotions = pca.transform(x_test_emotion_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original number of features: {x_train_emotion_norm.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the projections, mean, and standard deviation\n",
    "np.save('x_train_raf_pca_emotions.npy', x_train_pca_emotions)\n",
    "np.save('x_test_raf_pca_emotions.npy', x_test_pca_emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA + UMAP Unsupervised 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "raf_unsup_pca_umap_projections_train_10_01= np.load('raf_unsup_pca_umap_projections_train_10_01.npy')\n",
    "raf_mean_unsup_pca_umap_projection_train_10_01= np.load('raf_mean_unsup_pca_umap_projection_train_10_01.npy')\n",
    "raf_std_unsup_pca_umap_projection_train_10_01= np.load('raf_std_unsup_pca_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "raf_unsup_pca_umap_projections_test_10_01= np.load('raf_unsup_pca_umap_projections_test_10_01.npy')\n",
    "raf_mean_unsup_pca_umap_projection_test_10_01= np.load('raf_mean_unsup_pca_umap_projection_test_10_01.npy')\n",
    "raf_std_unsup_pca_umap_projection_test_10_01= np.load('raf_std_unsup_pca_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "raf_unsup_pca_umap_projections_train_10_01 = []\n",
    "raf_unsup_pca_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_train_pca_emotions)\n",
    "    raf_unsup_pca_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_pca_emotions)\n",
    "    raf_unsup_pca_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_unsup_pca_umap_projections_train_10_01 = np.array(raf_unsup_pca_umap_projections_train_10_01)\n",
    "raf_unsup_pca_umap_projections_test_10_01 = np.array(raf_unsup_pca_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_unsup_pca_umap_projection_train_10_01 = np.mean(raf_unsup_pca_umap_projections_train_10_01, axis=0)\n",
    "raf_std_unsup_pca_umap_projection_train_10_01 = np.std(raf_unsup_pca_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_unsup_pca_umap_projection_test_10_01 = np.mean(raf_unsup_pca_umap_projections_test_10_01, axis=0)\n",
    "raf_std_unsup_pca_umap_projection_test_10_01 = np.std(raf_unsup_pca_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_unsup_pca_umap_projections_train_10_01.npy', raf_unsup_pca_umap_projections_train_10_01)\n",
    "np.save('raf_mean_unsup_pca_umap_projection_train_10_01.npy', raf_mean_unsup_pca_umap_projection_train_10_01)\n",
    "np.save('raf_std_unsup_pca_umap_projection_train_10_01.npy', raf_std_unsup_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_unsup_pca_umap_projections_test_10_01.npy', raf_unsup_pca_umap_projections_test_10_01)\n",
    "np.save('raf_mean_unsup_pca_umap_projection_test_10_01.npy', raf_mean_unsup_pca_umap_projection_test_10_01)\n",
    "np.save('raf_std_unsup_pca_umap_projection_test_10_01.npy', raf_std_unsup_pca_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_unsup_pca_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_unsup_pca_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"PCA + Unsupervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_pca_umap_unsup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_pca_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_pca_umap_projection_test_10_01)) # second argument is y_test_pred_pca\n",
    "print(f\"ARI: {ari_raf_pca_umap_unsup_10_01:.2f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_pca_umap_unsup_10_01 = silhouette_score(raf_mean_unsup_pca_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_pca_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_pca_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_pca_umap_unsup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_unsup_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_mean_unsup_pca_umap_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_unsup_pca_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_mean_unsup_pca_umap_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA + UMAP Supervised 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "raf_sup_pca_umap_projections_train_10_01= np.load('raf_sup_pca_umap_projections_train_10_01.npy')\n",
    "raf_mean_sup_pca_umap_projection_train_10_01= np.load('raf_mean_sup_pca_umap_projection_train_10_01.npy')\n",
    "raf_std_sup_pca_umap_projection_train_10_01= np.load('raf_std_sup_pca_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "raf_sup_pca_umap_projections_test_10_01= np.load('raf_sup_pca_umap_projections_test_10_01.npy')\n",
    "raf_mean_sup_pca_umap_projection_test_10_01= np.load('raf_mean_sup_pca_umap_projection_test_10_01.npy')\n",
    "raf_std_sup_pca_umap_projection_test_10_01= np.load('raf_std_sup_pca_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for training and test sets\n",
    "raf_sup_pca_umap_projections_train_10_01 = []\n",
    "raf_sup_pca_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running Supervised UMAP - Iteration {run + 1}/{n_runs}...\")\n",
    "\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, \n",
    "        min_dist=min_dist, \n",
    "        n_components=n_components, \n",
    "        random_state=run\n",
    "    )\n",
    "\n",
    "    # Fit and transform the training data with labels\n",
    "    projection_train = umap_model.fit_transform(x_train_pca_emotions, y_train)\n",
    "    raf_sup_pca_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_pca_emotions)\n",
    "    raf_sup_pca_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_sup_pca_umap_projections_train_10_01 = np.array(raf_sup_pca_umap_projections_train_10_01)\n",
    "raf_sup_pca_umap_projections_test_10_01 = np.array(raf_sup_pca_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_sup_pca_umap_projection_train_10_01 = np.mean(raf_sup_pca_umap_projections_train_10_01, axis=0)\n",
    "raf_std_sup_pca_umap_projection_train_10_01 = np.std(raf_sup_pca_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_sup_pca_umap_projection_test_10_01 = np.mean(raf_sup_pca_umap_projections_test_10_01, axis=0)\n",
    "raf_std_sup_pca_umap_projection_test_10_01 = np.std(raf_sup_pca_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_sup_pca_umap_projections_train_10_01.npy', raf_sup_pca_umap_projections_train_10_01)\n",
    "np.save('raf_mean_sup_pca_umap_projection_train_10_01.npy', raf_mean_sup_pca_umap_projection_train_10_01)\n",
    "np.save('raf_std_sup_pca_umap_projection_train_10_01.npy', raf_std_sup_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_sup_pca_umap_projections_test_10_01.npy', raf_sup_pca_umap_projections_test_10_01)\n",
    "np.save('raf_mean_sup_pca_umap_projection_test_10_01.npy', raf_mean_sup_pca_umap_projection_test_10_01)\n",
    "np.save('raf_std_sup_pca_umap_projection_test_10_01.npy', raf_std_sup_pca_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"Supervised UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_sup_pca_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_sup_pca_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"PCA + Supervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_pca_umap_sup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_pca_umap_projection_train_10_01, y_train).predict(raf_mean_sup_pca_umap_projection_test_10_01)) # second argument is y_test_pred_pca\n",
    "print(f\"ARI: {ari_raf_pca_umap_sup_10_01:.2f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_pca_umap_sup_10_01 = silhouette_score(raf_mean_sup_pca_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_pca_umap_projection_train_10_01, y_train).predict(raf_mean_sup_pca_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_pca_umap_sup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_sup_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_mean_sup_pca_umap_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_sup_pca_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_mean_sup_pca_umap_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLE + UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for LLE\n",
    "n_neighbors = 10\n",
    "n_components = 147\n",
    "\n",
    "# Initialize the LLE model\n",
    "lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components, method='standard', random_state=42)\n",
    "\n",
    "# Fit and transform the training data\n",
    "print(\"Running LLE on the training set...\")\n",
    "x_train_raf_lle = lle.fit_transform(x_train_emotion_norm)\n",
    "print(\"LLE transformation on training set completed.\")\n",
    "\n",
    "# Transform the test data using the fitted LLE model\n",
    "print(\"Running LLE on the test set...\")\n",
    "x_test_raf_lle = lle.transform(x_test_emotion_norm)\n",
    "print(\"LLE transformation on test set completed.\")\n",
    "\n",
    "# Print shapes of transformed data\n",
    "print(f\"Shape of LLE-transformed training data: {x_train_raf_lle.shape}\")\n",
    "print(f\"Shape of LLE-transformed test data: {x_test_raf_lle.shape}\")\n",
    "\n",
    "# Optional: Save the LLE-transformed data for later use\n",
    "np.save('x_train_lle.npy', x_train_raf_lle)\n",
    "np.save('x_test_lle.npy', x_test_raf_lle)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"LLE-transformed data has been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLE + UMAP Unsupervised 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the projections, mean, and standard deviation for the training set\n",
    "raf_unsup_lle_umap_projections_train_10_01= np.load('raf_unsup_lle_umap_projections_train_10_01.npy')\n",
    "raf_mean_unsup_lle_umap_projection_train_10_01= np.load('raf_mean_unsup_lle_umap_projection_train_10_01.npy')\n",
    "raf_std_unsup_lle_umap_projection_train_10_01= np.load('raf_std_unsup_lle_umap_projection_train_10_01.npy')\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "raf_unsup_lle_umap_projections_test_10_01= np.load('raf_unsup_lle_umap_projections_test_10_01.npy')\n",
    "raf_mean_unsup_lle_umap_projection_test_10_01= np.load('raf_mean_unsup_lle_umap_projection_test_10_01.npy')\n",
    "raf_std_unsup_lle_umap_projection_test_10_01= np.load('raf_std_unsup_lle_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "raf_unsup_lle_umap_projections_train_10_01 = []\n",
    "raf_unsup_lle_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_train_raf_lle)\n",
    "    raf_unsup_lle_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_raf_lle)\n",
    "    raf_unsup_lle_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_unsup_lle_umap_projections_train_10_01 = np.array(raf_unsup_lle_umap_projections_train_10_01)\n",
    "raf_unsup_lle_umap_projections_test_10_01 = np.array(raf_unsup_lle_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_unsup_lle_umap_projection_train_10_01 = np.mean(raf_unsup_lle_umap_projections_train_10_01, axis=0)\n",
    "raf_std_unsup_lle_umap_projection_train_10_01 = np.std(raf_unsup_lle_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_unsup_lle_umap_projection_test_10_01 = np.mean(raf_unsup_lle_umap_projections_test_10_01, axis=0)\n",
    "raf_std_unsup_lle_umap_projection_test_10_01 = np.std(raf_unsup_lle_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_unsup_lle_umap_projections_train_10_01.npy', raf_unsup_lle_umap_projections_train_10_01)\n",
    "np.save('raf_mean_unsup_lle_umap_projection_train_10_01.npy', raf_mean_unsup_lle_umap_projection_train_10_01)\n",
    "np.save('raf_std_unsup_lle_umap_projection_train_10_01.npy', raf_std_unsup_lle_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_unsup_lle_umap_projections_test_10_01.npy', raf_unsup_lle_umap_projections_test_10_01)\n",
    "np.save('raf_mean_unsup_lle_umap_projection_test_10_01.npy', raf_mean_unsup_lle_umap_projection_test_10_01)\n",
    "np.save('raf_std_unsup_lle_umap_projection_test_10_01.npy', raf_std_unsup_lle_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_unsup_lle_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_unsup_lle_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"LLE + Unsupervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_lle_umap_unsup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_lle_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_lle_umap_projection_test_10_01)) # second argument is y_test_pred_lle\n",
    "print(f\"ARI: {ari_raf_lle_umap_unsup_10_01:.2f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_lle_umap_unsup_10_01 = silhouette_score(raf_mean_unsup_lle_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_lle_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_lle_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_lle_umap_unsup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_unsup_lle_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_lle_umap_unsup_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_unsup_lle_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_lle_umap_unsup_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLE + UMAP Supervised 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the projections, mean, and standard deviation for the training set\n",
    "raf_sup_lle_umap_projections_train_10_01= np.load('raf_sup_lle_umap_projections_train_10_01.npy')\n",
    "raf_mean_sup_lle_umap_projection_train_10_01= np.load('raf_mean_sup_lle_umap_projection_train_10_01.npy')\n",
    "raf_std_sup_lle_umap_projection_train_10_01= np.load('raf_std_sup_lle_umap_projection_train_10_01.npy')\n",
    "\n",
    "# Load the projections, mean, and standard deviation for the test set\n",
    "raf_sup_lle_umap_projections_test_10_01= np.load('raf_sup_lle_umap_projections_test_10_01.npy')\n",
    "raf_mean_sup_lle_umap_projection_test_10_01= np.load('raf_mean_sup_lle_umap_projection_test_10_01.npy')\n",
    "raf_std_sup_lle_umap_projection_test_10_01= np.load('raf_std_sup_lle_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "raf_sup_lle_umap_projections_train_10_01 = []\n",
    "raf_sup_lle_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_train_raf_lle,y_train)\n",
    "    raf_sup_lle_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_raf_lle)\n",
    "    raf_sup_lle_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_sup_lle_umap_projections_train_10_01 = np.array(raf_sup_lle_umap_projections_train_10_01)\n",
    "raf_sup_lle_umap_projections_test_10_01 = np.array(raf_sup_lle_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_sup_lle_umap_projection_train_10_01 = np.mean(raf_sup_lle_umap_projections_train_10_01, axis=0)\n",
    "raf_std_sup_lle_umap_projection_train_10_01 = np.std(raf_sup_lle_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_sup_lle_umap_projection_test_10_01 = np.mean(raf_sup_lle_umap_projections_test_10_01, axis=0)\n",
    "raf_std_sup_lle_umap_projection_test_10_01 = np.std(raf_sup_lle_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_sup_lle_umap_projections_train_10_01.npy', raf_sup_lle_umap_projections_train_10_01)\n",
    "np.save('raf_mean_sup_lle_umap_projection_train_10_01.npy', raf_mean_sup_lle_umap_projection_train_10_01)\n",
    "np.save('raf_std_sup_lle_umap_projection_train_10_01.npy', raf_std_sup_lle_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_sup_lle_umap_projections_test_10_01.npy', raf_sup_lle_umap_projections_test_10_01)\n",
    "np.save('raf_mean_sup_lle_umap_projection_test_10_01.npy', raf_mean_sup_lle_umap_projection_test_10_01)\n",
    "np.save('raf_std_sup_lle_umap_projection_test_10_01.npy', raf_std_sup_lle_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_sup_lle_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_sup_lle_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"LLE + Supervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_lle_umap_sup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_lle_umap_projection_train_10_01, y_train).predict(raf_mean_sup_lle_umap_projection_test_10_01)) # second argument is y_test_pred_lle\n",
    "print(f\"ARI: {ari_raf_lle_umap_sup_10_01:.2f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_lle_umap_sup_10_01 = silhouette_score(raf_mean_sup_lle_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_lle_umap_projection_train_10_01, y_train).predict(raf_mean_sup_lle_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_lle_umap_sup_10_01:.2f}\")\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_sup_lle_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_lle_umap_sup_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_sup_lle_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_lle_umap_sup_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gabor Filters + PCA + UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gabor filters + PCA + Unsupervised UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "raf_unsup_gabor_pca_umap_projections_train_10_01= np.load('raf_unsup_gabor_pca_umap_projections_train_10_01.npy')\n",
    "raf_mean_unsup_gabor_pca_umap_projection_train_10_01= np.load('raf_mean_unsup_gabor_pca_umap_projection_train_10_01.npy')\n",
    "raf_std_unsup_gabor_pca_umap_projection_train_10_01= np.load('raf_std_unsup_gabor_pca_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "raf_unsup_gabor_pca_umap_projections_test_10_01= np.load('raf_unsup_gabor_pca_umap_projections_test_10_01.npy')\n",
    "raf_mean_unsup_gabor_pca_umap_projection_test_10_01= np.load('raf_mean_unsup_gabor_pca_umap_projection_test_10_01.npy')\n",
    "raf_std_unsup_gabor_pca_umap_projection_test_10_01= np.load('raf_std_unsup_gabor_pca_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gabor Kernels\n",
    "def create_gabor_kernels():\n",
    "    \"\"\"Generates a set of Gabor kernels with different orientations and frequencies.\"\"\"\n",
    "    kernels = []\n",
    "    ksize = 31  # Kernel size\n",
    "    sigma = 4.0  # Standard deviation of the Gaussian envelope\n",
    "    lambd = 10.0  # Wavelength of the sinusoidal factor\n",
    "    gamma = 0.5  # Spatial aspect ratio\n",
    "    for theta in np.arange(0, np.pi, np.pi / 4):  # 8 orientations\n",
    "        kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lambd, gamma, psi=0, ktype=cv2.CV_32F)\n",
    "        kernels.append(kernel)\n",
    "    return kernels\n",
    "\n",
    "# Apply Gabor Filters\n",
    "def apply_gabor_filters(images, kernels):\n",
    "    \"\"\"Applies a set of Gabor filters to a batch of images.\"\"\"\n",
    "    gabor_features = []\n",
    "    for image in images:\n",
    "        image_2d = image.reshape(48, 48)  # Reshape back to 2D (assumes 48x48 images)\n",
    "        responses = []\n",
    "        for kernel in kernels:\n",
    "            filtered = cv2.filter2D(image_2d, cv2.CV_32F, kernel)  # Apply Gabor filter\n",
    "            responses.append(filtered.flatten())  # Flatten the filtered image\n",
    "        gabor_features.append(np.concatenate(responses))  # Concatenate all filter responses\n",
    "    return np.array(gabor_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Gabor kernels\n",
    "gabor_kernels = create_gabor_kernels()\n",
    "print(f\"Generated {len(gabor_kernels)} Gabor kernels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Gabor filters to the training and test sets\n",
    "x_train_gabor = apply_gabor_filters(x_train_emotion_norm, gabor_kernels)\n",
    "x_test_gabor = apply_gabor_filters(x_test_emotion_norm, gabor_kernels)\n",
    "\n",
    "print(f\"Train Gabor feature shape: {x_train_gabor.shape}\")\n",
    "print(f\"Test Gabor feature shape: {x_test_gabor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_raf_train_gabor = scaler.fit_transform(x_train_gabor)\n",
    "x_raf_test_gabor = scaler.transform(x_test_gabor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying PCA to Gabor filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.95)\n",
    "x_raf_train_pca_gabor = pca.fit_transform(x_raf_train_gabor)\n",
    "x_raf_test_pca_gabor = pca.transform(x_raf_test_gabor)\n",
    "\n",
    "print(f\"Reduced train shape: {x_raf_train_pca_gabor.shape}\")\n",
    "print(f\"Reduced test shape: {x_raf_test_pca_gabor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_raf_train_gabor_pca.npy', x_raf_train_pca_gabor)\n",
    "np.save('x_raf_test_gabor_pca.npy', x_raf_test_pca_gabor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "raf_unsup_gabor_pca_umap_projections_train_10_01 = []\n",
    "raf_unsup_gabor_pca_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_raf_train_pca_gabor)\n",
    "    raf_unsup_gabor_pca_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_raf_test_pca_gabor)\n",
    "    raf_unsup_gabor_pca_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_unsup_gabor_pca_umap_projections_train_10_01 = np.array(raf_unsup_gabor_pca_umap_projections_train_10_01)\n",
    "raf_unsup_gabor_pca_umap_projections_test_10_01 = np.array(raf_unsup_gabor_pca_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_unsup_gabor_pca_umap_projection_train_10_01 = np.mean(raf_unsup_gabor_pca_umap_projections_train_10_01, axis=0)\n",
    "raf_std_unsup_gabor_pca_umap_projection_train_10_01 = np.std(raf_unsup_gabor_pca_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_unsup_gabor_pca_umap_projection_test_10_01 = np.mean(raf_unsup_gabor_pca_umap_projections_test_10_01, axis=0)\n",
    "raf_std_unsup_gabor_pca_umap_projection_test_10_01 = np.std(raf_unsup_gabor_pca_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_unsup_gabor_pca_umap_projections_train_10_01.npy', raf_unsup_gabor_pca_umap_projections_train_10_01)\n",
    "np.save('raf_mean_unsup_gabor_pca_umap_projection_train_10_01.npy', raf_mean_unsup_gabor_pca_umap_projection_train_10_01)\n",
    "np.save('raf_std_unsup_gabor_pca_umap_projection_train_10_01.npy', raf_std_unsup_gabor_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_unsup_gabor_pca_umap_projections_test_10_01.npy', raf_unsup_gabor_pca_umap_projections_test_10_01)\n",
    "np.save('raf_mean_unsup_gabor_pca_umap_projection_test_10_01.npy', raf_mean_unsup_gabor_pca_umap_projection_test_10_01)\n",
    "np.save('raf_std_unsup_gabor_pca_umap_projection_test_10_01.npy', raf_std_unsup_gabor_pca_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_unsup_gabor_pca_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_unsup_gabor_pca_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Gabor + PCA + Unsupervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_gabor_pca_umap_unsup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_gabor_pca_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_gabor_pca_umap_projection_test_10_01)) # second argument is y_test_pred_gabor_pca\n",
    "print(f\"ARI: {ari_raf_gabor_pca_umap_unsup_10_01:.2f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_gabor_pca_umap_unsup_10_01 = silhouette_score(raf_mean_unsup_gabor_pca_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_gabor_pca_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_gabor_pca_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_gabor_pca_umap_unsup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_unsup_gabor_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_unsup_gabor_pca_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_unsup_gabor_pca_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_unsup_gabor_pca_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gabor filters + PCA + UMAP Supervised 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "raf_sup_gabor_pca_umap_projections_train_10_01= np.load('raf_sup_gabor_pca_umap_projections_train_10_01.npy')\n",
    "raf_mean_sup_gabor_pca_umap_projection_train_10_01= np.load('raf_mean_sup_gabor_pca_umap_projection_train_10_01.npy')\n",
    "raf_std_sup_gabor_pca_umap_projection_train_10_01= np.load('raf_std_sup_gabor_pca_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "raf_sup_gabor_pca_umap_projections_test_10_01= np.load('raf_sup_gabor_pca_umap_projections_test_10_01.npy')\n",
    "raf_mean_sup_gabor_pca_umap_projection_test_10_01= np.load('raf_mean_sup_gabor_pca_umap_projection_test_10_01.npy')\n",
    "raf_std_sup_gabor_pca_umap_projection_test_10_01= np.load('raf_std_sup_gabor_pca_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for training and test sets\n",
    "raf_sup_gabor_pca_umap_projections_train_10_01 = []\n",
    "raf_sup_gabor_pca_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running Supervised UMAP - Iteration {run + 1}/{n_runs}...\")\n",
    "\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, \n",
    "        min_dist=min_dist, \n",
    "        n_components=n_components, \n",
    "        random_state=run\n",
    "    )\n",
    "\n",
    "    # Fit and transform the training data with labels\n",
    "    projection_train = umap_model.fit_transform(x_raf_train_pca_gabor, y_train)\n",
    "    raf_sup_gabor_pca_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_raf_test_pca_gabor)\n",
    "    raf_sup_gabor_pca_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_sup_gabor_pca_umap_projections_train_10_01 = np.array(raf_sup_gabor_pca_umap_projections_train_10_01)\n",
    "raf_sup_gabor_pca_umap_projections_test_10_01 = np.array(raf_sup_gabor_pca_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_sup_gabor_pca_umap_projection_train_10_01 = np.mean(raf_sup_gabor_pca_umap_projections_train_10_01, axis=0)\n",
    "raf_std_sup_gabor_pca_umap_projection_train_10_01 = np.std(raf_sup_gabor_pca_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_sup_gabor_pca_umap_projection_test_10_01 = np.mean(raf_sup_gabor_pca_umap_projections_test_10_01, axis=0)\n",
    "raf_std_sup_gabor_pca_umap_projection_test_10_01 = np.std(raf_sup_gabor_pca_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_sup_gabor_pca_umap_projections_train_10_01.npy', raf_sup_gabor_pca_umap_projections_train_10_01)\n",
    "np.save('raf_mean_sup_gabor_pca_umap_projection_train_10_01.npy', raf_mean_sup_gabor_pca_umap_projection_train_10_01)\n",
    "np.save('raf_std_sup_gabor_pca_umap_projection_train_10_01.npy', raf_std_sup_gabor_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_sup_gabor_pca_umap_projections_test_10_01.npy', raf_sup_gabor_pca_umap_projections_test_10_01)\n",
    "np.save('raf_mean_sup_gabor_pca_umap_projection_test_10_01.npy', raf_mean_sup_gabor_pca_umap_projection_test_10_01)\n",
    "np.save('raf_std_sup_gabor_pca_umap_projection_test_10_01.npy', raf_std_sup_gabor_pca_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"Supervised UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_sup_gabor_pca_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_sup_gabor_pca_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Gabor + PCA + Supervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_gabor_pca_umap_sup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_gabor_pca_umap_projection_train_10_01, y_train).predict(raf_mean_sup_gabor_pca_umap_projection_test_10_01)) # second argument is y_test_pred_gabor_pca\n",
    "print(f\"ARI: {ari_raf_gabor_pca_umap_sup_10_01:.2f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_gabor_pca_umap_sup_10_01 = silhouette_score(raf_mean_sup_gabor_pca_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_gabor_pca_umap_projection_train_10_01, y_train).predict(raf_mean_sup_gabor_pca_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_gabor_pca_umap_sup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_sup_gabor_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_sup_gabor_pca_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_sup_gabor_pca_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_sup_gabor_pca_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of RAF-DB & FER2013 datasets combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAF-DB images where uploaded again with a different name\n",
    "\n",
    "#  Set file paths\n",
    "input_path = 'C:/Users/Lorenzo/OneDrive/Documents/DTU/Python/2024 Fall/MSc Thesis'\n",
    "image_dir = os.path.join(input_path, 'extracted_data/Image/aligned')\n",
    "label_file = os.path.join(input_path, 'extracted_data/EmoLabel/list_patition_label.txt')\n",
    "\n",
    "# Instantiate and load the dataset\n",
    "emotions_dataloader = EmotionsDataloader(image_dir, label_file)\n",
    "(x_train_RAF, y_train_RAF), (x_test_RAF, y_test_RAF) = emotions_dataloader.load_data()\n",
    "\n",
    "# Print dataset shapes\n",
    "print(f\"Training set shape: {x_train_RAF.shape}, {y_train_RAF.shape}\")\n",
    "print(f\"Testing set shape: {x_test_RAF.shape}, {y_test_RAF.shape}\")\n",
    "\n",
    "# Display some random train and test images\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images) / cols) + 1\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (image, title) in enumerate(zip(images, title_texts)):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        plt.title(title, fontsize=12)\n",
    "        plt.axis('off')\n",
    "\n",
    "# Show some random train and test images\n",
    "images_to_show = []\n",
    "titles_to_show = []\n",
    "\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(0, len(x_train_RAF))\n",
    "    images_to_show.append(x_train_RAF[idx])\n",
    "    titles_to_show.append(f\"Train[{idx}] = {y_train_RAF[idx]}\")\n",
    "\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(0, len(x_test_RAF))\n",
    "    images_to_show.append(x_test_RAF[idx])\n",
    "    titles_to_show.append(f\"Test[{idx}] = {y_test_RAF[idx]}\")\n",
    "\n",
    "show_images(images_to_show, titles_to_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Training Data (RAF)\n",
    "train_raf_df = pd.DataFrame({\n",
    "    \"emotion\": y_train_RAF,\n",
    "    \"pixels\": [\" \".join(map(str, x.flatten())) for x in x_train_RAF],  # Flatten images and store as strings\n",
    "    \"Dataset\": \"RAF\",\n",
    "    \"Usage\": \"Training\"\n",
    "})\n",
    "\n",
    "# Convert Testing Data (RAF)\n",
    "test_raf_df = pd.DataFrame({\n",
    "    \"emotion\": y_test_RAF,\n",
    "    \"pixels\": [\" \".join(map(str, x.flatten())) for x in x_test_RAF],  # Flatten images and store as strings\n",
    "    \"Dataset\": \"RAF\",\n",
    "    \"Usage\": \"Testing\"\n",
    "})\n",
    "\n",
    "# Combine RAF Train and Test\n",
    "raf_combined_df = pd.concat([train_raf_df, test_raf_df], ignore_index=True)\n",
    "\n",
    "# Verify the structure of the RAF DataFrame\n",
    "print(raf_combined_df.head())\n",
    "print(\"RAF Combined Shape:\", raf_combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine RAF and FER2013 datasets\n",
    "combined_df = pd.concat([raf_combined_df, df], ignore_index=True)\n",
    "\n",
    "# Verify the combined DataFrame\n",
    "print(\"Combined Dataset Shape:\", combined_df.shape)\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining both datasets RAF-DB & FER2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'pixels' column to numerical arrays for verification\n",
    "combined_df[\"pixels\"] = combined_df[\"pixels\"].apply(lambda x: np.array(list(map(int, x.split()))))\n",
    "\n",
    "# Check the shape of the first row to confirm it is flattened\n",
    "print(\"Shape of a single image (pixels):\", combined_df[\"pixels\"].iloc[0].shape)  # Expected: (2304,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check mean and standard deviation of pixel values\n",
    "pixel_values = np.vstack(combined_df[\"pixels\"].values)\n",
    "print(\"Mean of pixel values:\", pixel_values.mean())\n",
    "print(\"Standard deviation of pixel values:\", pixel_values.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the pixel values\n",
    "scaler = StandardScaler()\n",
    "normalized_pixels = scaler.fit_transform(pixel_values)  # Normalize pixel values\n",
    "\n",
    "# Update the 'pixels' column with normalized values\n",
    "combined_df[\"pixels\"] = list(normalized_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 'Usage' column values to 'Testing' if they are 'PrivateTest' or 'PublicTest'\n",
    "combined_df.loc[(combined_df[\"Usage\"] == \"PrivateTest\") | (combined_df[\"Usage\"] == \"PublicTest\"), \"Usage\"] = \"Testing\"\n",
    "\n",
    "# Check if the update was successful\n",
    "print(combined_df[\"Usage\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the 'Dataset' column as 0 (FER2013) and 1 (RAF)\n",
    "combined_df[\"Dataset\"] = combined_df[\"Dataset\"].map({\"FER2013\": 0, \"RAF\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df[\"Dataset\"].isna().sum())  # Check for NaN values in the 'Dataset' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in Dataset column after encoding:\", combined_df[\"Dataset\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "merged_train_data = combined_df[combined_df[\"Usage\"] == \"Training\"]\n",
    "merged_test_data = combined_df[combined_df[\"Usage\"] != \"Training\"]\n",
    "\n",
    "# Extract X (pixels) and y (Dataset) for training and testing\n",
    "merged_x_train = np.vstack(merged_train_data[\"pixels\"].values)  # Convert list of arrays to 2D array\n",
    "merged_y_train = merged_train_data[\"Dataset\"].values\n",
    "\n",
    "merged_x_test = np.vstack(merged_test_data[\"pixels\"].values)  # Convert list of arrays to 2D array\n",
    "merged_y_test = merged_test_data[\"Dataset\"].values\n",
    "\n",
    "# Verify shapes\n",
    "print(\"merged_x_train shape:\", merged_x_train.shape)  # Expected: (num_train_samples, 2304)\n",
    "print(\"merged_y_train shape:\", merged_y_train.shape)  # Expected: (num_train_samples,)\n",
    "print(\"merged_x_test shape:\", merged_x_test.shape)    # Expected: (num_test_samples, 2304)\n",
    "print(\"merged_y_test shape:\", merged_y_test.shape)    # Expected: (num_test_samples,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n_neighhours = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 50\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for training and test sets\n",
    "raf_fer_sup_umap_train_50_01 = []\n",
    "raf_fer_sup_umap_test_50_01 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running supervised UMAP - Iteration {run + 1}/{n_runs}...\")\n",
    "\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, \n",
    "        min_dist=min_dist, \n",
    "        n_components=n_components, \n",
    "        random_state=run\n",
    "    )\n",
    "\n",
    "    # Fit and transform the training data with labels\n",
    "    projection_train = umap_model.fit_transform(merged_x_train, merged_y_train)\n",
    "    raf_fer_sup_umap_train_50_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(merged_x_test)\n",
    "    raf_fer_sup_umap_test_50_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_fer_sup_umap_train_50_01 = np.array(raf_fer_sup_umap_train_50_01)\n",
    "raf_fer_sup_umap_test_50_01 = np.array(raf_fer_sup_umap_test_50_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_fer_mean_sup_umap_projection_train_50_01 = np.mean(raf_fer_sup_umap_train_50_01, axis=0)\n",
    "raf_fer_std_sup_umap_projection_train_50_01 = np.std(raf_fer_sup_umap_train_50_01, axis=0)\n",
    "\n",
    "raf_fer_mean_sup_umap_projection_test_50_01 = np.mean(raf_fer_sup_umap_test_50_01, axis=0)\n",
    "raf_fer_std_sup_umap_projection_test_50_01 = np.std(raf_fer_sup_umap_test_50_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_fer_sup_umap_train_50_01.npy', raf_fer_sup_umap_train_50_01)\n",
    "np.save('raf_fer_mean_sup_umap_projection_train_50_01.npy', raf_fer_mean_sup_umap_projection_train_50_01)\n",
    "np.save('raf_fer_std_sup_umap_projection_train_50_01.npy', raf_fer_std_sup_umap_projection_train_50_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_fer_sup_umap_test_50_01.npy', raf_fer_sup_umap_test_50_01)\n",
    "np.save('raf_fer_mean_sup_umap_projection_test_50_01.npy', raf_fer_mean_sup_umap_projection_test_50_01)\n",
    "np.save('raf_fer_std_sup_umap_projection_test_50_01.npy', raf_fer_std_sup_umap_projection_test_50_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"supervised UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Define a custom colormap with green and blue\n",
    "custom_cmap = ListedColormap([\"lightgreen\", \"pink\"])\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_fer_mean_sup_umap_projection_train_50_01[:, 0],\n",
    "    raf_fer_mean_sup_umap_projection_train_50_01[:, 1],\n",
    "    c=merged_y_train,\n",
    "    cmap=custom_cmap,\n",
    "    s=5,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"n_neighbours=50 Supervised UMAP Projection of RAFDB-FER2013 Merged Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks([0, 1])  # Ensure ticks align with both labels\n",
    "cbar.set_ticklabels([\"Original source 0 - (FER2013)\", \"Original source 1 - (RAF-DB)\"])  # Customize labels\n",
    "cbar.set_label(\"Image Original Source Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_fer_umap_sup_50_01 = adjusted_rand_score(merged_y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_fer_mean_sup_umap_projection_train_50_01, merged_y_train).predict(raf_fer_mean_sup_umap_projection_test_50_01)) # second argument is y_test_pred_pca\n",
    "print(f\"ARI: {ari_raf_fer_umap_sup_50_01:.4f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_fer_umap_sup_50_01 = silhouette_score(raf_fer_mean_sup_umap_projection_test_50_01, KNeighborsClassifier(n_neighbors=1).fit(raf_fer_mean_sup_umap_projection_train_50_01, merged_y_train).predict(raf_fer_mean_sup_umap_projection_test_50_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_fer_umap_sup_50_01:.2f}\")\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(merged_y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_fer_mean_sup_umap_projection_train_50_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_fer_sup_umap_projection_50_01_db_score = davies_bouldin_score(\n",
    "    raf_fer_mean_sup_umap_projection_train_50_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_fer_sup_umap_projection_50_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n_neighbours = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "raf_fer_sup_umap_train_10_01= np.load('raf_fer_sup_umap_train_10_01.npy')\n",
    "raf_fer_mean_sup_umap_projection_train_10_01= np.load('raf_fer_mean_sup_umap_projection_train_10_01.npy')\n",
    "raf_fer_std_sup_umap_projection_train_10_01= np.load('raf_fer_std_sup_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "raf_fer_sup_umap_test_10_01= np.load('raf_fer_sup_umap_test_10_01.npy')\n",
    "raf_fer_mean_sup_umap_projection_test_10_01= np.load('raf_fer_mean_sup_umap_projection_test_10_01.npy')\n",
    "raf_fer_std_sup_umap_projection_test_10_01= np.load('raf_fer_std_sup_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for training and test sets\n",
    "raf_fer_sup_umap_train_10_01 = []\n",
    "raf_fer_sup_umap_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running Supervised UMAP - Iteration {run + 1}/{n_runs}...\")\n",
    "\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, \n",
    "        min_dist=min_dist, \n",
    "        n_components=n_components, \n",
    "        random_state=run\n",
    "    )\n",
    "\n",
    "    # Fit and transform the training data with labels\n",
    "    projection_train = umap_model.fit_transform(merged_x_train, merged_y_train)\n",
    "    raf_fer_sup_umap_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(merged_x_test)\n",
    "    raf_fer_sup_umap_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_fer_sup_umap_train_10_01 = np.array(raf_fer_sup_umap_train_10_01)\n",
    "raf_fer_sup_umap_test_10_01 = np.array(raf_fer_sup_umap_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_fer_mean_sup_umap_projection_train_10_01 = np.mean(raf_fer_sup_umap_train_10_01, axis=0)\n",
    "raf_fer_std_sup_umap_projection_train_10_01 = np.std(raf_fer_sup_umap_train_10_01, axis=0)\n",
    "\n",
    "raf_fer_mean_sup_umap_projection_test_10_01 = np.mean(raf_fer_sup_umap_test_10_01, axis=0)\n",
    "raf_fer_std_sup_umap_projection_test_10_01 = np.std(raf_fer_sup_umap_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_fer_sup_umap_train_10_01.npy', raf_fer_sup_umap_train_10_01)\n",
    "np.save('raf_fer_mean_sup_umap_projection_train_10_01.npy', raf_fer_mean_sup_umap_projection_train_10_01)\n",
    "np.save('raf_fer_std_sup_umap_projection_train_10_01.npy', raf_fer_std_sup_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_fer_sup_umap_test_10_01.npy', raf_fer_sup_umap_test_10_01)\n",
    "np.save('raf_fer_mean_sup_umap_projection_test_10_01.npy', raf_fer_mean_sup_umap_projection_test_10_01)\n",
    "np.save('raf_fer_std_sup_umap_projection_test_10_01.npy', raf_fer_std_sup_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"Supervised UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Define a custom colormap with green and blue\n",
    "custom_cmap = ListedColormap([\"lightgreen\", \"blue\"])\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_fer_mean_sup_umap_projection_train_10_01[:, 0],\n",
    "    raf_fer_mean_sup_umap_projection_train_10_01[:, 1],\n",
    "    c=merged_y_train,\n",
    "    cmap=custom_cmap,\n",
    "    s=5,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Supervised UMAP Projection of RAFDB-FER2013 Merged Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks([0, 1])  # Ensure ticks align with both labels\n",
    "cbar.set_ticklabels([\"Original source 0 - (FER2013)\", \"Original source 1 - (RAF-DB)\"])  # Customize labels\n",
    "cbar.set_label(\"Image Original Source Labels\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_fer_umap_sup_10_01 = adjusted_rand_score(merged_y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_fer_mean_sup_umap_projection_train_10_01, merged_y_train).predict(raf_fer_mean_sup_umap_projection_test_10_01)) # second argument is y_test_pred_pca\n",
    "print(f\"ARI: {ari_raf_fer_umap_sup_10_01:.4f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_fer_umap_sup_10_01 = silhouette_score(raf_fer_mean_sup_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_fer_mean_sup_umap_projection_train_10_01, merged_y_train).predict(raf_fer_mean_sup_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_fer_umap_sup_10_01:.2f}\")\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(merged_y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_fer_mean_sup_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_fer_sup_umap_projection_10_01_db_score = davies_bouldin_score(\n",
    "    raf_fer_mean_sup_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_fer_sup_umap_projection_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n_neighbors = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 50\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for training and test sets\n",
    "raf_fer_unsup_umap_train_50_01 = []\n",
    "raf_fer_unsup_umap_test_50_01 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running unsupervised UMAP - Iteration {run + 1}/{n_runs}...\")\n",
    "\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, \n",
    "        min_dist=min_dist, \n",
    "        n_components=n_components, \n",
    "        random_state=run\n",
    "    )\n",
    "\n",
    "    # Fit and transform the training data with labels\n",
    "    projection_train = umap_model.fit_transform(merged_x_train)\n",
    "    raf_fer_unsup_umap_train_50_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(merged_x_test)\n",
    "    raf_fer_unsup_umap_test_50_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_fer_unsup_umap_train_50_01 = np.array(raf_fer_unsup_umap_train_50_01)\n",
    "raf_fer_unsup_umap_test_50_01 = np.array(raf_fer_unsup_umap_test_50_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_fer_mean_unsup_umap_projection_train_50_01 = np.mean(raf_fer_unsup_umap_train_50_01, axis=0)\n",
    "raf_fer_std_unsup_umap_projection_train_50_01 = np.std(raf_fer_unsup_umap_train_50_01, axis=0)\n",
    "\n",
    "raf_fer_mean_unsup_umap_projection_test_50_01 = np.mean(raf_fer_unsup_umap_test_50_01, axis=0)\n",
    "raf_fer_std_unsup_umap_projection_test_50_01 = np.std(raf_fer_unsup_umap_test_50_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_fer_unsup_umap_train_50_01.npy', raf_fer_unsup_umap_train_50_01)\n",
    "np.save('raf_fer_mean_unsup_umap_projection_train_50_01.npy', raf_fer_mean_unsup_umap_projection_train_50_01)\n",
    "np.save('raf_fer_std_unsup_umap_projection_train_50_01.npy', raf_fer_std_unsup_umap_projection_train_50_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_fer_unsup_umap_test_50_01.npy', raf_fer_unsup_umap_test_50_01)\n",
    "np.save('raf_fer_mean_unsup_umap_projection_test_50_01.npy', raf_fer_mean_unsup_umap_projection_test_50_01)\n",
    "np.save('raf_fer_std_unsup_umap_projection_test_50_01.npy', raf_fer_std_unsup_umap_projection_test_50_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"unsupervised UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Define a custom colormap with green and blue\n",
    "custom_cmap = ListedColormap([\"lightgreen\", \"pink\"])\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_fer_mean_unsup_umap_projection_train_50_01[:, 0],\n",
    "    raf_fer_mean_unsup_umap_projection_train_50_01[:, 1],\n",
    "    c=merged_y_train,\n",
    "    cmap=custom_cmap,\n",
    "    s=5,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"n_neighbours=50 Unsupervised UMAP Projection of RAFDB-FER2013 Merged Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks([0, 1])  # Ensure ticks align with both labels\n",
    "cbar.set_ticklabels([\"Original source 0 - (FER2013)\", \"Original source 1 - (RAF-DB)\"])  # Customize labels\n",
    "cbar.set_label(\"Image Original Source Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_fer_umap_unsup_50_01 = adjusted_rand_score(merged_y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_fer_mean_unsup_umap_projection_train_50_01, merged_y_train).predict(raf_fer_mean_unsup_umap_projection_test_50_01)) # second argument is y_test_pred_pca\n",
    "print(f\"ARI: {ari_raf_fer_umap_unsup_50_01:.4f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_fer_umap_unsup_50_01 = silhouette_score(raf_fer_mean_unsup_umap_projection_test_50_01, KNeighborsClassifier(n_neighbors=1).fit(raf_fer_mean_unsup_umap_projection_train_50_01, merged_y_train).predict(raf_fer_mean_unsup_umap_projection_test_50_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_fer_umap_unsup_50_01:.2f}\")\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(merged_y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_fer_mean_unsup_umap_projection_train_50_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_fer_unsup_umap_projection_50_01_db_score = davies_bouldin_score(\n",
    "    raf_fer_mean_unsup_umap_projection_train_50_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_fer_unsup_umap_projection_50_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n_neighbours= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "raf_fer_unsup_umap_train_10_01= np.load('raf_fer_unsup_umap_train_10_01.npy')\n",
    "raf_fer_mean_unsup_umap_projection_train_10_01= np.load('raf_fer_mean_unsup_umap_projection_train_10_01.npy')\n",
    "raf_fer_std_unsup_umap_projection_train_10_01= np.load('raf_fer_std_unsup_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "raf_fer_unsup_umap_test_10_01= np.load('raf_fer_unsup_umap_test_10_01.npy')\n",
    "raf_fer_mean_unsup_umap_projection_test_10_01= np.load('raf_fer_mean_unsup_umap_projection_test_10_01.npy')\n",
    "raf_fer_std_unsup_umap_projection_test_10_01= np.load('raf_fer_std_unsup_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for training and test sets\n",
    "raf_fer_unsup_umap_train_10_01 = []\n",
    "raf_fer_unsup_umap_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running unsupervised UMAP - Iteration {run + 1}/{n_runs}...\")\n",
    "\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, \n",
    "        min_dist=min_dist, \n",
    "        n_components=n_components, \n",
    "        random_state=run\n",
    "    )\n",
    "\n",
    "    # Fit and transform the training data with labels\n",
    "    projection_train = umap_model.fit_transform(merged_x_train)\n",
    "    raf_fer_unsup_umap_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(merged_x_test)\n",
    "    raf_fer_unsup_umap_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_fer_unsup_umap_train_10_01 = np.array(raf_fer_unsup_umap_train_10_01)\n",
    "raf_fer_unsup_umap_test_10_01 = np.array(raf_fer_unsup_umap_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_fer_mean_unsup_umap_projection_train_10_01 = np.mean(raf_fer_unsup_umap_train_10_01, axis=0)\n",
    "raf_fer_std_unsup_umap_projection_train_10_01 = np.std(raf_fer_unsup_umap_train_10_01, axis=0)\n",
    "\n",
    "raf_fer_mean_unsup_umap_projection_test_10_01 = np.mean(raf_fer_unsup_umap_test_10_01, axis=0)\n",
    "raf_fer_std_unsup_umap_projection_test_10_01 = np.std(raf_fer_unsup_umap_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_fer_unsup_umap_train_10_01.npy', raf_fer_unsup_umap_train_10_01)\n",
    "np.save('raf_fer_mean_unsup_umap_projection_train_10_01.npy', raf_fer_mean_unsup_umap_projection_train_10_01)\n",
    "np.save('raf_fer_std_unsup_umap_projection_train_10_01.npy', raf_fer_std_unsup_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_fer_unsup_umap_test_10_01.npy', raf_fer_unsup_umap_test_10_01)\n",
    "np.save('raf_fer_mean_unsup_umap_projection_test_10_01.npy', raf_fer_mean_unsup_umap_projection_test_10_01)\n",
    "np.save('raf_fer_std_unsup_umap_projection_test_10_01.npy', raf_fer_std_unsup_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"unsupervised UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Define a custom colormap with green and blue\n",
    "custom_cmap = ListedColormap([\"lightgreen\", \"blue\"])\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_fer_mean_unsup_umap_projection_train_10_01[:, 0],\n",
    "    raf_fer_mean_unsup_umap_projection_train_10_01[:, 1],\n",
    "    c=merged_y_train,\n",
    "    cmap=custom_cmap,\n",
    "    s=5,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Unsupervised UMAP Projection of RAFDB-FER2013 Merged Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks([0, 1])  # Ensure ticks align with both labels\n",
    "cbar.set_ticklabels([\"Original source 0 - (FER2013)\", \"Original source 1 - (RAF-DB)\"])  # Customize labels\n",
    "cbar.set_label(\"Image Original Source Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_fer_umap_unsup_10_01 = adjusted_rand_score(merged_y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_fer_mean_unsup_umap_projection_train_10_01, merged_y_train).predict(raf_fer_mean_unsup_umap_projection_test_10_01)) # second argument is y_test_pred_pca\n",
    "print(f\"ARI: {ari_raf_fer_umap_unsup_10_01:.4f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_fer_umap_unsup_10_01 = silhouette_score(raf_fer_mean_unsup_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_fer_mean_unsup_umap_projection_train_10_01, merged_y_train).predict(raf_fer_mean_unsup_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_fer_umap_unsup_10_01:.2f}\")\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(merged_y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_fer_mean_unsup_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_fer_unsup_umap_projection_10_01_db_score = davies_bouldin_score(\n",
    "    raf_fer_mean_unsup_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_fer_unsup_umap_projection_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom colormap with green and blue\n",
    "custom_cmap = ListedColormap([\"lightgreen\", \"blue\"])\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot the unsupervised UMAP\n",
    "scatter1 = axes[0].scatter(\n",
    "    raf_fer_mean_unsup_umap_projection_train_10_01[:, 0],\n",
    "    raf_fer_mean_unsup_umap_projection_train_10_01[:, 1],\n",
    "    c=merged_y_train,\n",
    "    cmap=custom_cmap,\n",
    "    s=5,\n",
    "    alpha=0.8\n",
    ")\n",
    "axes[0].set_title(\"Unsupervised UMAP Projection of RAFDB-FER2013 Merged Training Data (10 Runs)\")\n",
    "axes[0].set_xlabel(\"UMAP Component 1\")\n",
    "axes[0].set_ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Plot the supervised UMAP\n",
    "scatter2 = axes[1].scatter(\n",
    "    raf_fer_mean_sup_umap_projection_train_10_01[:, 0],\n",
    "    raf_fer_mean_sup_umap_projection_train_10_01[:, 1],\n",
    "    c=merged_y_train,\n",
    "    cmap=custom_cmap,\n",
    "    s=5,\n",
    "    alpha=0.8\n",
    ")\n",
    "axes[1].set_title(\"Supervised UMAP Projection of RAFDB-FER2013 Merged Training Data (10 Runs)\")\n",
    "axes[1].set_xlabel(\"UMAP Component 1\")\n",
    "axes[1].set_ylabel(\"UMAP Component 2\")\n",
    "cbar2 = fig.colorbar(scatter2, ax=axes[1], orientation=\"vertical\")\n",
    "cbar2.set_ticks([0, 1])  # Ensure ticks align with both labels\n",
    "cbar2.set_ticklabels([\"Original source 0 - (FER2013)\", \"Original source 1 - (RAF-DB)\"])  # Customize labels\n",
    "cbar2.set_label(\"Image Original Source Labels\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
