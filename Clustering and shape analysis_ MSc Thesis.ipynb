{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import struct\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import hdbscan\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, confusion_matrix, accuracy_score, davies_bouldin_score, adjusted_rand_score\n",
    "from sklearn.metrics import pairwise_distances as sklearn_pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import mode, linregress, norm\n",
    "from array import array\n",
    "from os.path import join\n",
    "from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding, MDS,trustworthiness\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Data Loader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Data Loader Class\n",
    "\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath, training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())\n",
    "        \n",
    "        # Convert labels to NumPy array\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        \n",
    "        return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Reading Dataset via MNISTDataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Reading Dataset via MnistDataloader class\n",
    "%matplotlib inline\n",
    "\n",
    "# Set file paths of MNIST Datasets\n",
    "input_path = 'C:/Users/Lorenzo/OneDrive/Documents/DTU/Python/2024 Fall/MSc Thesis'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "# Helper function to show a list of images with their relating titles\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n",
    "\n",
    "# Load MINST dataset\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "# Show some random training and test images \n",
    "images_2_show = []\n",
    "titles_2_show = []\n",
    "for i in range(0, 10):\n",
    "    r = random.randint(1, 60000)\n",
    "    images_2_show.append(x_train[r])\n",
    "    titles_2_show.append('training image [' + str(r) + '] = ' + str(y_train[r]))    \n",
    "\n",
    "for i in range(0, 5):\n",
    "    r = random.randint(1, 10000)\n",
    "    images_2_show.append(x_test[r])        \n",
    "    titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test[r]))    \n",
    "\n",
    "show_images(images_2_show, titles_2_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of images to a NumPy array\n",
    "x_train_array = np.array(x_train)\n",
    "x_test_array = np.array(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Option 1) Normalization done by Standarization (zero mean and unit variance). Recommended for DR techniques and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Flatten images if needed\n",
    "x_train_flattened1 = x_train_array.reshape(x_train_array.shape[0], -1)\n",
    "x_test_flattened1 = x_test_array.reshape(x_test_array.shape[0], -1)\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "x_train_standardized1 = scaler.fit_transform(x_train_flattened1)\n",
    "x_test_standardized1 = scaler.transform(x_test_flattened1)\n",
    "\n",
    "print(\"Data standardized: Mean =\", x_train_standardized1.mean(), \"Std Dev =\", x_train_standardized1.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Option 2) Normalizaiton done by Scailing reshaping the images scaling to [0,1]. However more in use for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the 28x28 images into 784-dimensional vectors\n",
    "x_train_flattened = np.array([img.flatten() for img in x_train_array])\n",
    "x_test_flattened = np.array([img.flatten() for img in x_test_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing by 255 scales the pixel intensity values to the [0, 1] range.\n",
    "# Hhelps improve performance and consistency in clustering and dimensionality reduction algorithms. \n",
    "# Making it a common practice in image-based data processing.\n",
    "\n",
    "x_train_normalized = x_train_flattened / 255.0\n",
    "x_test_normalized = x_test_flattened / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check normalization for x_train_normalized\n",
    "train_mean = x_train_normalized.mean(axis=0)  # Mean for each feature\n",
    "train_std = x_train_normalized.std(axis=0)    # Standard deviation for each feature\n",
    "\n",
    "# Check normalization for x_test_normalized\n",
    "test_mean = x_test_normalized.mean(axis=0)  # Mean for each feature\n",
    "test_std = x_test_normalized.std(axis=0)    # Standard deviation for each feature\n",
    "\n",
    "# Print results\n",
    "print(\"Train Data - Mean (per feature):\")\n",
    "print(train_mean)\n",
    "print(\"Train Data - Standard Deviation (per feature):\")\n",
    "print(train_std)\n",
    "\n",
    "print(\"\\nTest Data - Mean (per feature):\")\n",
    "print(test_mean)\n",
    "print(\"Test Data - Standard Deviation (per feature):\")\n",
    "print(test_std)\n",
    "\n",
    "# Verify if data is normalized\n",
    "if np.allclose(train_mean, 0, atol=1e-2) and np.allclose(train_std, 1, atol=1e-2):\n",
    "    print(\"\\nx_train_normalized is properly normalized (zero mean, unit variance).\")\n",
    "else:\n",
    "    print(\"\\nx_train_normalized is NOT properly normalized.\")\n",
    "\n",
    "if np.allclose(test_mean, 0, atol=1e-2) and np.allclose(test_std, 1, atol=1e-2):\n",
    "    print(\"x_test_normalized is properly normalized (zero mean, unit variance).\")\n",
    "else:\n",
    "    print(\"x_test_normalized is NOT properly normalized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature Review Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply PCA\n",
    "pca = PCA(0.95)\n",
    "x_train_pca = pca.fit_transform(x_train_normalized)\n",
    "x_test_pca = pca.transform(x_test_normalized)\n",
    "\n",
    "print(f\"Original number of features: {x_train_normalized.shape[1]}\")\n",
    "print(f\"Reduced number of features: {x_train_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_pca_c2 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(x_train_pca, y_train).predict(x_test_pca)) # second argument is y_test_pred_pca\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette_pca_c2 = silhouette_score(x_test_pca, KNeighborsClassifier(n_neighbors=1).fit(x_train_pca, y_train).predict(x_test_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ari_pca_c2)\n",
    "print(silhouette_pca_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_pca_c2, y_train)\n",
    "y_test_pred_pca_c2 = svm_clf.predict(x_test_pca_c2)\n",
    "svm_accuracy_pca = accuracy_score(y_test, y_test_pred_pca_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying PCA the 784-dimensional data was reduced to 154 dimensions while preserving 95% of the variance. Meaning that 154 new features (principal components) retain almost all the important information (variance) from the original 784 features, with minimal information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PCA embeddings to .npy file\n",
    "np.save(\"pca_embeddings.npy\", x_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_embeddings=np.load(\"pca_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: K-Means Clustering\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans.fit(x_train_pca)\n",
    "train_cluster_labels = kmeans.labels_\n",
    "test_cluster_labels = kmeans.predict(x_test_pca)\n",
    "\n",
    "# Step 4: Cluster Evaluation\n",
    "cluster_label_mapping = {}\n",
    "\n",
    "for cluster in range(10):\n",
    "    indices = np.where(train_cluster_labels == cluster)\n",
    "    if len(indices[0]) > 0:\n",
    "        most_common_label = mode(np.array(y_train)[indices]).mode[0]\n",
    "        cluster_label_mapping[cluster] = most_common_label\n",
    "    else:\n",
    "        cluster_label_mapping[cluster] = -1\n",
    "\n",
    "print(\"\\nCluster to Label Mapping:\")\n",
    "for cluster, label in cluster_label_mapping.items():\n",
    "    print(f\"Cluster {cluster}: Label {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Cluster 3** and **Cluster 7** both correspond to Label 0 (digit zero). Indicating that the clustering algorithm created two different clusters that mostly contain images of 0.\n",
    "- Similarly, **Cluster 2** and **Cluster 9** both correspond to Label 1. This overlap suggests that the digits might not be completely separated into unique clusters by K-Means.\n",
    "- This overlap may arise because of variation in handwriting styles or because some digits (like 0, 1, or 8) have distinct shapes depending on how they are written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save K-Means cluster labels from PCA-reduced data\n",
    "np.save(\"kmeans_labels_pca.npy\", train_cluster_labels)\n",
    "\n",
    "# Save cluster-to-label mapping for PCA-based K-Means\n",
    "with open(\"cluster_to_label_mapping_pca.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Cluster\", \"Label\"])\n",
    "    for cluster, label in cluster_label_mapping.items():\n",
    "        writer.writerow([cluster, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Predict Test Labels\n",
    "test_predicted_labels = [cluster_label_mapping[cluster] for cluster in test_cluster_labels]\n",
    "\n",
    "# Step 6: Evaluate Performance\n",
    "accuracy = accuracy_score(y_test, test_predicted_labels)\n",
    "print(f\"\\nClustering-based classification accuracy on test set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying PCA unsupervised clusters and mapping them to labels, can be correctly classified about 59.5% of the test images.\n",
    "\n",
    "*Possible options for improving*\n",
    "\n",
    "- **Dimensionality Reduction Tuning** try retaining 90-99% variance and observe if it affects the clustering accuracy.\n",
    "- **Alternative Clustering Methods** check other clustering algorithms (e.g., Gaussian Mixture Models, Agglomerative Clustering)\n",
    "- **Feature Engineering** try extracting additional features (e.g., edges, corners) that might be more informative for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA results load\n",
    "\n",
    "train_cluster_labels = np.load(f\"kmeans_labels_pca.npy\")\n",
    "x_train_pca = np.load(f'pca_embeddings.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plots & visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Clusters\n",
    "def plot_cluster_images(cluster_number, num_images=5):\n",
    "    indices = np.where(train_cluster_labels == cluster_number)[0]\n",
    "    if len(indices) == 0:\n",
    "        print(f\"No images found for cluster {cluster_number}\")\n",
    "        return\n",
    "    selected_indices = np.random.choice(indices, size=min(num_images, len(indices)), replace=False)\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(x_train_array[idx], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Label: {y_train[idx]}\")\n",
    "    plt.suptitle(f\"Images from Cluster {cluster_number}\")\n",
    "    plt.show()\n",
    "\n",
    "# for cluster in range(10):\n",
    "#     plot_cluster_images(cluster_number=cluster, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance\n",
    "explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.text(20, 0.85, '95% Variance Threshold', color='red', fontsize=12)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reduce to 2 components for visualization purposes\n",
    "# pca_2d = PCA(n_components=2)\n",
    "# x_train_pca_2d = pca_2d.fit_transform(x_train_normalized)\n",
    "\n",
    "# Plot the 2D projection with cluster labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=x_train_pca[:, 0], y=x_train_pca[:, 1], hue=train_cluster_labels, palette='tab10', s=10, legend='full')\n",
    "plt.title(\"2D Scatter Plot of PCA-reduced Data with Cluster Labels\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map predicted clusters to actual labels for y_train\n",
    "y_train_predicted_labels = [cluster_label_mapping[cluster] for cluster in train_cluster_labels]\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_train, y_train_predicted_labels, labels=range(10))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel(\"Predicted Cluster Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix of Cluster-Label Mapping\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reconstructed_data = pca.inverse_transform(x_reduced)\n",
    "reconstruction_error = mean_squared_error(x_train_normalized, reconstructed_data)\n",
    "print(f\"Reconstruction Error: {reconstruction_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save confusion matrix for PCA-based K-Means to CSV\n",
    "conf_matrix_pca_df = pd.DataFrame(conf_matrix, index=range(10), columns=range(10))\n",
    "conf_matrix_pca_df.to_csv(\"confusion_matrix_pca.csv\", index_label=\"True Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to measure how well PCA-reduced representation preserves the separability of classes (digits).\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Train a logistic regression model on PCA-reduced training data\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(x_train_pca, y_train)\n",
    "\n",
    "# Evaluate accuracy on the test set\n",
    "y_test_pred = clf.predict(x_test_pca)\n",
    "pca_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Classification accuracy with PCA features: {pca_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression (or any classifier) can utilize the PCA-reduced data to classify the digits.\n",
    "\n",
    "- This approach evaluates the quality of PCA embeddings for a supervised task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_pca = silhouette_score(x_train_pca, y_train)\n",
    "ari_pca = adjusted_rand_score(y_train, train_cluster_labels)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_pca:.4f}\")\n",
    "print(f\"Adjusted Rand Index: {ari_pca:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T- SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE to reduce to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "x_train_tsne = tsne.fit_transform(x_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save t-SNE embeddings to .npy file\n",
    "np.save(\"tsne_embeddings.npy\", x_train_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tsne= np.load(f'tsne_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 2D embeddings\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=x_train_tsne[:, 0], y=x_train_tsne[:, 1], hue=y_train, palette='tab10', s=10, legend='full')\n",
    "plt.title(\"t-SNE - 2D Scatter Plot of MNIST Data (True Labels)\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.legend(title=\"True Label\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means on the 2D t-SNE output\n",
    "kmeans_tsne = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans_tsne.fit(x_train_tsne)\n",
    "\n",
    "# Retrieve cluster labels assigned by K-Means\n",
    "train_cluster_labels_tsne = kmeans_tsne.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save K-Means cluster labels from t-SNE-reduced data\n",
    "np.save(\"kmeans_labels_tsne.npy\", train_cluster_labels_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from cluster labels to actual digit labels\n",
    "cluster_label_mapping_tsne = {}\n",
    "\n",
    "for cluster in range(10):\n",
    "    # Get indices of samples in the current cluster\n",
    "    indices = np.where(train_cluster_labels_tsne == cluster)[0]\n",
    "    \n",
    "    # Find the most common actual label among these samples\n",
    "    if len(indices) > 0:\n",
    "        most_common_label = mode(np.array(y_train)[indices]).mode[0]\n",
    "        cluster_label_mapping_tsne[cluster] = most_common_label\n",
    "    else:\n",
    "        cluster_label_mapping_tsne[cluster] = -1  # Assign -1 if the cluster is empty\n",
    "\n",
    "print(\"Cluster to Label Mapping for t-SNE + K-Means:\")\n",
    "for cluster, label in cluster_label_mapping_tsne.items():\n",
    "    print(f\"Cluster {cluster}: Label {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels based on the cluster-to-label mapping\n",
    "train_predicted_labels_tsne = [cluster_label_mapping_tsne[cluster] for cluster in train_cluster_labels_tsne]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_tsne = accuracy_score(y_train, train_predicted_labels_tsne)\n",
    "print(f\"Clustering-based classification accuracy on training set (t-SNE + K-Means): {accuracy_tsne:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cluster-to-label mapping for t-SNE-based K-Means\n",
    "with open(\"cluster_to_label_mapping_tsne.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Cluster\", \"Label\"])\n",
    "    for cluster, label in cluster_label_mapping_tsne.items():\n",
    "        writer.writerow([cluster, label])\n",
    "\n",
    "print(\"Cluster-to-label mappings saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "conf_matrix_tsne = confusion_matrix(y_train, train_predicted_labels_tsne, labels=range(10))\n",
    "\n",
    "# Save confusion matrix for t-SNE-based K-Means to CSV\n",
    "conf_matrix_tsne_df = pd.DataFrame(conf_matrix_tsne, index=range(10), columns=range(10))\n",
    "conf_matrix_tsne_df.to_csv(\"confusion_matrix_tsne.csv\", index_label=\"True Label\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_tsne, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel(\"Predicted Cluster Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix of Cluster-Label Mapping (t-SNE + K-Means)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load t-SNE embeddings (already saved as .npy file)\n",
    "x_train_tsne = np.load(\"tsne_embeddings.npy\")\n",
    "\n",
    "# Train logistic regression on t-SNE-reduced training data\n",
    "clf_tsne = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_tsne.fit(x_train_tsne, y_train)\n",
    "\n",
    "# Predict and evaluate accuracy\n",
    "y_test_tsne = tsne.fit_transform(x_test_normalized)  # Reduce test data to 2D using t-SNE\n",
    "y_test_pred_tsne = clf_tsne.predict(y_test_tsne)\n",
    "tsne_accuracy = accuracy_score(y_test, y_test_pred_tsne)\n",
    "\n",
    "print(f\"Classification accuracy with t-SNE features: {tsne_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute silhouette score for t-SNE embeddings\n",
    "silhouette_tsne = silhouette_score(x_train_tsne, train_cluster_labels_tsne)\n",
    "\n",
    "# Compute ARI for t-SNE embeddings\n",
    "ari_tsne = adjusted_rand_score(y_train, train_cluster_labels_tsne)\n",
    "\n",
    "print(f\"Silhouette Score (t-SNE): {silhouette_tsne:.4f}\")\n",
    "print(f\"Adjusted Rand Index (t-SNE): {ari_tsne:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISOMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Computational Cost:* Isomap can be slow on large datasets, so consider experimenting with a subset of the data if computation time is an issue. **Data was FLATTENED & NORMALIZED, the downsampled use was 35%**\n",
    "\n",
    "With the full set the algorithm was running for more than 2 hs and never ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 10.000 images instead of the full dataset. It should keep the subset representative without overwhelming computational resources.\n",
    "# subset_size = 10000\n",
    "\n",
    "# # Randomly sample indices for the subset\n",
    "# subset_indices = np.random.choice(len(x_train_normalized), subset_size, replace=False)\n",
    "\n",
    "# # Create subset for Isomap\n",
    "# x_train_subset = x_train_normalized[subset_indices]\n",
    "# y_train_subset = np.array(y_train)[subset_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Downsample the Dataset Consistently\n",
    "def downsample_mnist_consistent(x_data, y_labels, sample_fraction=0.35):\n",
    "    \"\"\"\n",
    "    Downsample the dataset consistently, returning indices to ensure\n",
    "    the same points are selected in both spaces.\n",
    "    \"\"\"\n",
    "    sampled_indices = []\n",
    "    unique_labels = np.unique(y_labels)\n",
    "    for label in unique_labels:\n",
    "        # Select indices for the current label\n",
    "        label_indices = np.where(y_labels == label)[0]\n",
    "        # Sample a fraction of points for this label\n",
    "        sampled_indices_label = resample(\n",
    "            label_indices, n_samples=int(len(label_indices) * sample_fraction), replace=False\n",
    "        )\n",
    "        sampled_indices.extend(sampled_indices_label)\n",
    "    return np.array(sampled_indices)\n",
    "\n",
    "# Get consistent indices for sampling\n",
    "sampled_indices = downsample_mnist_consistent(x_train_normalized, y_train, sample_fraction=0.35)\n",
    "\n",
    "# Step 2: Use the Sampled Indices to Extract Points from Both Spaces\n",
    "# Downsample the high-dimensional original space\n",
    "x_sampled = x_train_normalized[sampled_indices]\n",
    "y_sampled = y_train[sampled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sampled indices\n",
    "np.save(\"sampled_indices.npy\", sampled_indices)\n",
    "\n",
    "# Save the downsampled dataset\n",
    "np.save(\"x_sampled.npy\", x_sampled)\n",
    "np.save(\"y_sampled.npy\", y_sampled)\n",
    "\n",
    "print(\"Downsampling saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sampled indices and sets\n",
    "sampled_indices= np.load(\"sampled_indices.npy\")\n",
    "x_sampled= np.load(\"x_sampled.npy\")\n",
    "y_sampled= np.load(\"y_sampled.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Isomap to the subset of data\n",
    "isomap = Isomap(n_components=50, n_neighbors=5)  # Reduced n_neighbors to 5 for faster computation\n",
    "# x_train_isomap_subset = isomap.fit_transform(x_train_subset)\n",
    "x_reduced = isomap.fit_transform(x_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the Isomap-reduced data\n",
    "# np.save(\"isomap_embedding_subsets.npy\", x_train_isomap_subset)\n",
    "np.save(\"isomap_embedding_subsets_evenly_downsampled.npy\", x_reduced)\n",
    "print(\"Isomap embeddings saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reduced= np.load(f\"isomap_embedding_subsets_evenly_downsampled.npy\")\n",
    "train_cluster_labels_isomap_downsampled = np.load(f\"kmeans_labels_isomap_evenly_downsampled.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means on the Isomap-reduced data\n",
    "kmeans_isomap_downsampled = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans_isomap_downsampled.fit(x_reduced)\n",
    "\n",
    "# Retrieve the cluster labels assigned by K-Means\n",
    "train_cluster_labels_isomap_downsampled = kmeans_isomap_downsampled.labels_\n",
    "\n",
    "# Save K-Means cluster labels\n",
    "np.save(\"kmeans_labels_isomap_evenly_downsampled.npy\", train_cluster_labels_isomap_downsampled)\n",
    "print(\"K-Means cluster labels saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure y_sampled corresponds to the sampled indices of y_train\n",
    "assert np.array_equal(y_sampled, y_train[sampled_indices]), \"Mismatch between y_sampled and sampled y_train\"\n",
    "\n",
    "# Create a mapping from cluster labels to actual digit labels\n",
    "from collections import Counter\n",
    "\n",
    "cluster_label_mapping_isomap_subset = {}\n",
    "\n",
    "for cluster in range(10):\n",
    "    # Get indices of samples in the current cluster\n",
    "    indices = np.where(train_cluster_labels_isomap_downsampled == cluster)[0]\n",
    "    \n",
    "    if len(indices) > 0:\n",
    "        # Find the most common actual label among these samples\n",
    "        most_common_label = Counter(y_sampled[indices]).most_common(1)[0][0]\n",
    "        cluster_label_mapping_isomap_subset[cluster] = most_common_label\n",
    "    else:\n",
    "        cluster_label_mapping_isomap_subset[cluster] = -1  # Assign -1 if the cluster is empty\n",
    "\n",
    "# Print the cluster-to-label mapping\n",
    "print(\"Cluster to Label Mapping for Isomap + K-Means:\")\n",
    "for cluster, label in cluster_label_mapping_isomap_subset.items():\n",
    "    print(f\"Cluster {cluster}: Label {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cluster-to-label mapping as a CSV file\n",
    "with open(\"cluster_to_label_mapping_isomap_downsampled.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Cluster\", \"Label\"])\n",
    "    for cluster, label in cluster_label_mapping_isomap_subset.items():\n",
    "        writer.writerow([cluster, label])\n",
    "\n",
    "print(\"Cluster-to-label mapping saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels based on the cluster-to-label mapping\n",
    "train_predicted_labels_isomap_downsampled = [cluster_label_mapping_isomap_subset[cluster] for cluster in train_cluster_labels_isomap_downsampled]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_isomap_subset = accuracy_score(y_sampled, train_predicted_labels_isomap_downsampled)\n",
    "print(f\"Clustering-based classification accuracy on training subset (Isomap + K-Means): {accuracy_isomap_subset:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 2D embeddings\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=x_reduced[:, 0], y=x_reduced[:, 1], hue=y_sampled, palette='tab10', s=10, legend='full')\n",
    "plt.title(\"Isomap - 2D Scatter Plot of MNIST Data\")\n",
    "plt.xlabel(\"Isomap Component 1\")\n",
    "plt.ylabel(\"Isomap Component 2\")\n",
    "plt.legend(title=\"True Label\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot the confusion matrix\n",
    "conf_matrix_isomap_subset = confusion_matrix(y_train_subset, train_predicted_labels_isomap_subset, labels=range(10))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_isomap_subset, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel(\"Predicted Cluster Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix of Cluster-Label Mapping (Isomap + K-Means on Subset)\")\n",
    "plt.show()\n",
    "\n",
    "# Save the confusion matrix for future reference\n",
    "conf_matrix_isomap_df = pd.DataFrame(conf_matrix_isomap_subset, index=range(10), columns=range(10))\n",
    "conf_matrix_isomap_df.to_csv(\"confusion_matrix_isomap_subset.csv\", index_label=\"True Label\")\n",
    "\n",
    "print(\"Confusion matrix saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PCA projections\n",
    "x_train_pca = np.load(f'pca_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply HDBSCAN clustering\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=30, min_samples=10, metric='euclidean')\n",
    "train_cluster_labels_hdbscan = hdbscan_clusterer.fit_predict(x_train_pca)\n",
    "\n",
    "# Save HDBSCAN cluster labels for future use\n",
    "np.save(\"hdbscan_labels.npy\", train_cluster_labels_hdbscan)\n",
    "print(\"HDBSCAN cluster labels saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cluster_labels_hdbscan = np.load(f'hdbscan_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sizes = np.bincount(train_cluster_labels_hdbscan[train_cluster_labels_hdbscan != -1])  # Exclude noise\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(len(cluster_sizes)), cluster_sizes, color='blue', alpha=0.7)\n",
    "plt.title('Cluster Sizes (Excluding Noise)')\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('Number of Points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PCA projections and HDBSCAN cluster labels\n",
    "x_train_pca = np.load('pca_embeddings.npy')\n",
    "train_cluster_labels_hdbscan = np.load('hdbscan_labels.npy')\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = sns.scatterplot(\n",
    "    x=x_train_pca[:, 0], \n",
    "    y=x_train_pca[:, 1], \n",
    "    hue=train_cluster_labels_hdbscan, \n",
    "    palette='tab20',  # Adjust the palette if needed\n",
    "    s=10, \n",
    "    legend='full'\n",
    ")\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"HDBSCAN Clustering on PCA-Reduced Data\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster Labels\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_ratio = np.sum(train_cluster_labels_hdbscan == -1) / len(train_cluster_labels_hdbscan)\n",
    "print(f\"Noise Ratio: {noise_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out noise points (noise is labeled as -1 by HDBSCAN)\n",
    "valid_indices = train_cluster_labels_hdbscan != -1\n",
    "filtered_labels = train_cluster_labels_hdbscan[valid_indices]\n",
    "filtered_y_train = np.array(\n",
    "y_train)[valid_indices]\n",
    "\n",
    "# Create a mapping from clusters to actual digit labels\n",
    "cluster_label_mapping_hdbscan = {}\n",
    "\n",
    "for cluster in np.unique(filtered_labels):\n",
    "    indices = np.where(filtered_labels == cluster)[0]\n",
    "    if len(indices) > 0:\n",
    "        most_common_label = mode(filtered_y_train[indices]).mode[0]\n",
    "        cluster_label_mapping_hdbscan[cluster] = most_common_label\n",
    "    else:\n",
    "        cluster_label_mapping_hdbscan[cluster] = -1\n",
    "\n",
    "print(\"Cluster to Label Mapping for HDBSCAN:\")\n",
    "for cluster, label in cluster_label_mapping_hdbscan.items():\n",
    "    print(f\"Cluster {cluster}: Label {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cluster_to_label_mapping_hdbscan.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Cluster\", \"Label\"])\n",
    "    for cluster, label in cluster_label_mapping_hdbscan.items():\n",
    "        writer.writerow([cluster, label])\n",
    "\n",
    "print(\"Cluster-to-label mapping for HDBSCAN saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels using the cluster-to-label mapping, ignoring noise points\n",
    "train_predicted_labels_hdbscan = [cluster_label_mapping_hdbscan.get(cluster, -1) for cluster in train_cluster_labels_hdbscan]\n",
    "valid_predicted_labels = [label for i, label in enumerate(train_predicted_labels_hdbscan) if train_cluster_labels_hdbscan[i] != -1]\n",
    "valid_true_labels = [y_train[i] for i in range(len(y_train)) if train_cluster_labels_hdbscan[i] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy_hdbscan = accuracy_score(valid_true_labels, valid_predicted_labels)\n",
    "print(f\"Clustering-based classification accuracy on valid points (HDBSCAN): {accuracy_hdbscan:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix for valid points\n",
    "conf_matrix_hdbscan = confusion_matrix(valid_true_labels, valid_predicted_labels, labels=range(10))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_hdbscan, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel(\"Predicted Cluster Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix of Cluster-Label Mapping (HDBSCAN)\")\n",
    "plt.show()\n",
    "\n",
    "# Save the confusion matrix as a CSV\n",
    "conf_matrix_hdbscan_df = pd.DataFrame(conf_matrix_hdbscan, index=range(10), columns=range(10))\n",
    "conf_matrix_hdbscan_df.to_csv(\"confusion_matrix_hdbscan.csv\", index_label=\"True Label\")\n",
    "\n",
    "print(\"Confusion matrix for HDBSCAN saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply Locally Linear Embedding (LLE)\n",
    "n_components = 50  # Number of dimensions to reduce to\n",
    "n_neighbors = 10   # Adjust based on your data\n",
    "lle = LocallyLinearEmbedding(n_components=n_components, n_neighbors=n_neighbors)\n",
    "\n",
    "print(\"Running LLE...\")\n",
    "x_train_lle = lle.fit_transform(x_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LLE-reduced data for future use\n",
    "np.save(\"lle_embeddings.npy\", x_train_lle)\n",
    "print(\"LLE embeddings saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means on the LLE-reduced data\n",
    "kmeans_lle = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans_lle.fit(x_train_lle)\n",
    "\n",
    "# Retrieve cluster labels assigned by K-Means\n",
    "train_cluster_labels_lle = kmeans_lle.labels_\n",
    "\n",
    "# Save K-Means cluster labels for future use\n",
    "np.save(\"kmeans_labels_lle.npy\", train_cluster_labels_lle)\n",
    "print(\"K-Means cluster labels for LLE saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from clusters to actual digit labels\n",
    "cluster_label_mapping_lle = {}\n",
    "\n",
    "for cluster in range(10):\n",
    "    # Get indices of samples in the current cluster\n",
    "    indices = np.where(train_cluster_labels_lle == cluster)[0]\n",
    "    \n",
    "    # Find the most common actual label among these samples\n",
    "    if len(indices) > 0:\n",
    "        most_common_label = Counter(y_sampled[indices]).most_common(1)[0][0]\n",
    "        cluster_label_mapping_lle[cluster] = most_common_label\n",
    "    else:\n",
    "        cluster_label_mapping_lle[cluster] = -1  # Assign -1 if the cluster is empty\n",
    "\n",
    "# Save cluster-to-label mapping to CSV\n",
    "with open(\"cluster_to_label_mapping_lle.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Cluster\", \"Label\"])\n",
    "    for cluster, label in cluster_label_mapping_lle.items():\n",
    "        writer.writerow([cluster, label])\n",
    "print(\"Cluster-to-label mapping for LLE saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Predict Labels and Calculate Accuracy\n",
    "train_predicted_labels_lle = [cluster_label_mapping_lle[cluster] for cluster in train_cluster_labels_lle]\n",
    "accuracy_lle = accuracy_score(y_sampled, train_predicted_labels_lle)\n",
    "print(f\"Clustering-based classification accuracy on training set (LLE + K-Means): {accuracy_lle:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce LLE-reduced data to 2 components for visualization using PCA\n",
    "pca_2d = PCA(n_components=2)\n",
    "x_train_lle_2d = pca_2d.fit_transform(x_train_lle)\n",
    "\n",
    "# Plot the second and third components of LLE-reduced data (components 1 and 2 if zero-indexed)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=x_train_lle[:, 1], y=x_train_lle[:, 2], hue=train_cluster_labels_lle, palette='tab10', s=10, legend='full')\n",
    "plt.title(\"2D Scatter Plot of LLE-reduced Data with Cluster Labels (Components 2 and 3)\")\n",
    "plt.xlabel(\"LLE Component 2\")\n",
    "plt.ylabel(\"LLE Component 3\")\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels using the cluster-to-label mapping\n",
    "train_predicted_labels_lle = [cluster_label_mapping_lle[cluster] for cluster in train_cluster_labels_lle]\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix_lle = confusion_matrix(y_train_subset, train_predicted_labels_lle, labels=range(10))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_lle, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel(\"Predicted Cluster Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix of Cluster-Label Mapping (LLE + K-Means)\")\n",
    "plt.show()\n",
    "\n",
    "# Save the confusion matrix as a CSV\n",
    "conf_matrix_lle_df = pd.DataFrame(conf_matrix_lle, index=range(10), columns=range(10))\n",
    "conf_matrix_lle_df.to_csv(\"confusion_matrix_lle.csv\", index_label=\"True Label\")\n",
    "\n",
    "print(\"Confusion matrix for LLE saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply UMAP for Dimensionality Reduction\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform UMAP on the training set\n",
    "x_train_umap = reducer.fit_transform(x_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Visualize the UMAP Results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_train_umap[:, 0], x_train_umap[:, 1], c=y_train, cmap=\"tab10\", s=5, alpha=0.8)\n",
    "plt.title(\"UMAP Projection of MNIST Dataset\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "plt.colorbar(label=\"MNIST Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_train_umap.npy', x_train_umap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual comparison between methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sampled indices\n",
    "sampled_indices = np.load(\"sampled_indices.npy\")\n",
    "\n",
    "# Load the downsampled dataset\n",
    "x_sampled = np.load(\"x_sampled.npy\")\n",
    "y_sampled = np.load(\"y_sampled.npy\")\n",
    "\n",
    "print(\"Downsampling loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings and cluster labels\n",
    "x_train_pca = np.load(f'pca_embeddings.npy')\n",
    "train_cluster_labels_pca = np.load(f'kmeans_labels_pca.npy')\n",
    "\n",
    "x_train_tsne = np.load('tsne_embeddings.npy')\n",
    "train_cluster_labels_tsne = np.load('kmeans_labels_tsne.npy')\n",
    "\n",
    "x_reduced= np.load(f\"isomap_embedding_subsets_evenly_downsampled.npy\")\n",
    "train_cluster_labels_isomap_downsampled = np.load(f\"kmeans_labels_isomap_evenly_downsampled.npy\")\n",
    "\n",
    "train_cluster_labels_hdbscan = np.load('hdbscan_labels.npy')\n",
    "\n",
    "x_train_lle = np.load('lle_embeddings.npy')\n",
    "train_cluster_labels_lle = np.load('kmeans_labels_lle.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trustworthiness(original_data, reduced_data, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Calculate Trustworthiness.\n",
    "\n",
    "    Parameters:\n",
    "    - original_data: High-dimensional original dataset (n_samples, n_features)\n",
    "    - reduced_data: Reduced-dimensional embeddings (n_samples, n_components)\n",
    "    - n_neighbors: Number of neighbors for trustworthiness calculation\n",
    "\n",
    "    Returns:\n",
    "    - trust: Trustworthiness score\n",
    "    \"\"\"\n",
    "    # Check for matching number of rows\n",
    "    if original_data.shape[0] != reduced_data.shape[0]:\n",
    "        raise ValueError(f\"Shape mismatch: original_data has {original_data.shape[0]} rows, \"\n",
    "                         f\"but reduced_data has {reduced_data.shape[0]} rows.\")\n",
    "    \n",
    "    # Calculate Trustworthiness\n",
    "    trust = trustworthiness(original_data, reduced_data, n_neighbors=n_neighbors)\n",
    "    print(f\"Trustworthiness: {trust:.4f}\")\n",
    "    return trust\n",
    "\n",
    "def calculate_neighborhood_preservation(original_data, reduced_data):\n",
    "    \"\"\"\n",
    "    Calculate Neighborhood Preservation by correlating pairwise distances.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_data: High-dimensional original dataset\n",
    "    - reduced_data: Reduced-dimensional embeddings\n",
    "    \n",
    "    Returns:\n",
    "    - neighborhood_preservation: Correlation of pairwise distances between spaces\n",
    "    \"\"\"\n",
    "    original_distances = cdist(original_data, original_data)\n",
    "    reduced_distances = cdist(reduced_data, reduced_data)\n",
    "    neighborhood_preservation = np.corrcoef(\n",
    "        original_distances.flatten(), reduced_distances.flatten()\n",
    "    )[0, 1]\n",
    "    print(f\"Neighborhood Preservation: {neighborhood_preservation:.4f}\")\n",
    "    return neighborhood_preservation\n",
    "\n",
    "def calculate_silhouette_score(reduced_data, cluster_labels):\n",
    "    \"\"\"\n",
    "    Calculate Silhouette Score for clustering in the reduced space.\n",
    "    \n",
    "    Parameters:\n",
    "    - reduced_data: Reduced-dimensional embeddings\n",
    "    - cluster_labels: Cluster labels from K-Means or another clustering algorithm\n",
    "    \n",
    "    Returns:\n",
    "    - silhouette: Silhouette score\n",
    "    \"\"\"\n",
    "    silhouette = silhouette_score(reduced_data, cluster_labels)\n",
    "    print(f\"Silhouette Score: {silhouette:.4f}\")\n",
    "    return silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings and cluster labels for the downsampled subset\n",
    "x_sampled_pca = x_train_pca[sampled_indices]  # PCA embeddings for the downsampled subset\n",
    "x_sampled_tsne = x_train_tsne[sampled_indices]  # t-SNE embeddings for the downsampled subset\n",
    "cluster_labels_pca_sampled = train_cluster_labels_pca[sampled_indices]\n",
    "cluster_labels_tsne_sampled = train_cluster_labels_tsne[sampled_indices]\n",
    "\n",
    "# Isomap and LLE embeddings already computed on the downsampled dataset\n",
    "x_sampled_isomap = x_reduced  # Isomap embeddings for downsampled subset\n",
    "cluster_labels_isomap_sampled = train_cluster_labels_isomap_downsampled\n",
    "x_sampled_lle = x_train_lle\n",
    "cluster_labels_lle_sampled = train_cluster_labels_lle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"x_sampled shape: {x_sampled.shape}\")\n",
    "for name, (reduced_data, _) in techniques_downsampled.items():\n",
    "    print(f\"{name} reduced_data shape: {reduced_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Techniques dictionary with aligned embeddings and cluster labels\n",
    "techniques_downsampled = {\n",
    "    \"PCA\": (x_sampled_pca, cluster_labels_pca_sampled),\n",
    "    \"t-SNE\": (x_sampled_tsne, cluster_labels_tsne_sampled),\n",
    "    \"Isomap\": (x_sampled_isomap, cluster_labels_isomap_sampled),\n",
    "    \"LLE\": (x_sampled_lle, cluster_labels_lle_sampled),\n",
    "}\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {name: {} for name in techniques_downsampled.keys()}\n",
    "\n",
    "# Calculate metrics for each technique\n",
    "for name, (reduced_data, cluster_labels) in techniques.items():\n",
    "    print(f\"\\nEvaluating metrics for {name}...\")\n",
    "\n",
    "    # Trustworthiness\n",
    "    results[name][\"Trustworthiness\"] = calculate_trustworthiness(x_sampled, reduced_data)\n",
    "\n",
    "    # Neighborhood Preservation\n",
    "    results[name][\"Neighborhood Preservation\"] = calculate_neighborhood_preservation(x_sampled, reduced_data)\n",
    "\n",
    "    # Silhouette Score\n",
    "    results[name][\"Silhouette Score\"] = calculate_silhouette_score(reduced_data, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Comparison\n",
    "def plot_embeddings(embeddings, labels, title):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    # Replace -1 (noise) with NaN for better plotting\n",
    "    valid_labels = np.array(labels, dtype=float)\n",
    "    valid_labels[valid_labels == -1] = np.nan\n",
    "    sns.scatterplot(x=embeddings[:, 0], y=embeddings[:, 1], hue=valid_labels, \n",
    "                    palette='tab10', s=50, legend='full')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_embeddings(x_train_pca, train_cluster_labels_pca, 'PCA Embeddings')\n",
    "plot_embeddings(x_train_tsne, train_cluster_labels_tsne, 'T-SNE Embeddings')\n",
    "plot_embeddings(x_reduced, train_cluster_labels_isomap_downsampled, 'Isomap Embeddings')\n",
    "plot_embeddings(x_train_lle, train_cluster_labels_lle, 'LLE Embeddings')\n",
    "\n",
    "# Silhouette Scores and Plots\n",
    "def plot_silhouette(embeddings, labels, title):\n",
    "    # Exclude noise points for silhouette calculation\n",
    "    valid_indices = labels != -1\n",
    "    valid_embeddings = embeddings[valid_indices]\n",
    "    valid_labels = labels[valid_indices]\n",
    "\n",
    "    silhouette_avg = silhouette_score(valid_embeddings, valid_labels)\n",
    "    sample_silhouette_values = silhouette_samples(valid_embeddings, valid_labels)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    y_lower = 10\n",
    "    for i in np.unique(valid_labels):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[valid_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values)\n",
    "        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10\n",
    "\n",
    "    plt.title(f'Silhouette plot for {title} (avg score: {silhouette_avg:.2f})')\n",
    "    plt.xlabel(\"Silhouette coefficient values\")\n",
    "    plt.ylabel(\"Cluster label\")\n",
    "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_silhouette(x_train_pca, train_cluster_labels_pca, 'PCA')\n",
    "plot_silhouette(x_train_tsne, train_cluster_labels_tsne, 'T-SNE')\n",
    "plot_silhouette(x_reduced, train_cluster_labels_isomap_downsampled, 'Isomap')\n",
    "plot_silhouette(x_train_lle, train_cluster_labels_lle, 'LLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in a table\n",
    "results_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"Technique\", \"Trustworthiness\", \"Neighborhood Preservation\", \"Silhouette Score\"]\n",
    ")\n",
    "\n",
    "display(results_df)\n",
    "# tools.display_dataframe_to_user(name=\"Dimensionality Reduction Metrics Comparison\", dataframe=results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pairwise_distances(embeddings, title):\n",
    "    # Sample data to avoid memory issues\n",
    "    sample_size = 1000\n",
    "    if len(embeddings) > sample_size:\n",
    "        indices = np.random.choice(len(embeddings), sample_size, replace=False)\n",
    "        embeddings_sampled = embeddings[indices]\n",
    "    else:\n",
    "        embeddings_sampled = embeddings\n",
    "\n",
    "    distances = pairwise_distances(embeddings_sampled)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(distances, cmap='viridis', cbar_kws={'label': 'Pairwise Distance'})\n",
    "    plt.title(f'Pairwise Distance Heatmap for {title}')\n",
    "    plt.xlabel('Sample Points')\n",
    "    plt.ylabel('Sample Points')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_pairwise_distances(x_train_pca, 'PCA')\n",
    "plot_pairwise_distances(x_train_tsne, 'T-SNE')\n",
    "plot_pairwise_distances(x_reduced, 'Isomap')\n",
    "plot_pairwise_distances(x_train_lle, 'LLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings and cluster labels\n",
    "x_train_pca = np.load('pca_embeddings.npy')\n",
    "train_cluster_labels_pca = np.load('kmeans_labels_pca.npy')\n",
    "\n",
    "x_train_tsne = np.load('tsne_embeddings.npy')\n",
    "# train_cluster_labels_tsne = np.load('kmeans_labels_tsne.npy')\n",
    "\n",
    "x_reduced_isomap = np.load('isomap_embedding_subsets_evenly_downsampled.npy')\n",
    "train_cluster_labels_isomap = np.load('kmeans_labels_isomap_evenly_downsampled.npy')\n",
    "\n",
    "x_train_lle = np.load('lle_embeddings.npy')\n",
    "train_cluster_labels_lle = np.load('kmeans_labels_lle.npy')\n",
    "\n",
    "x_train_umap = np.load('x_train_umap.npy')\n",
    "\n",
    "# Define the embeddings and labels\n",
    "methods = {\n",
    "    'PCA': (x_train_pca, train_cluster_labels_pca),\n",
    "    'Isomap': (x_reduced_isomap, train_cluster_labels_isomap),\n",
    "    'LLE': (x_train_lle, train_cluster_labels_lle),\n",
    "    'MDS': (x_train_lle, train_cluster_labels_lle),\n",
    "    't-SNE': (x_train_tsne, y_train),\n",
    "    'UMAP': (x_train_umap, y_train)\n",
    "}\n",
    "\n",
    "# Create a grid of subplots\n",
    "fig, axes = plt.subplots(len(methods), 1, figsize=(10, 18))  # Adjust for clearer layout\n",
    "fig.tight_layout(pad=6.0)\n",
    "\n",
    "# Define the label names (digits 0-9)\n",
    "label_names = [f\"Digit {i}\" for i in range(10)]\n",
    "\n",
    "for i, (ax, (method, (embedding, labels))) in enumerate(zip(axes, methods.items())):\n",
    "    scatter = sns.scatterplot(\n",
    "        x=embedding[:, 0], \n",
    "        y=embedding[:, 1], \n",
    "        hue=labels, \n",
    "        palette='tab10', \n",
    "        s=15,  # Increased for better visibility\n",
    "        ax=ax,\n",
    "        legend=(i == 0)  # Add legend only to the first plot\n",
    "    )\n",
    "    ax.set_title(f'{method} Embeddings', fontsize=14, pad=10, loc='center')\n",
    "\n",
    "    # Hide x and y axis ticks and labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    # Set equal aspect ratio for symmetry\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Customize the legend for the first plot\n",
    "    if i == 0:\n",
    "        handles, _ = scatter.get_legend_handles_labels()\n",
    "        ax.legend(handles, label_names, title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    else:\n",
    "        ax.legend().remove()\n",
    "\n",
    "# Add a super title for all plots\n",
    "# plt.suptitle('Dimensionality Reduction Methods on MNIST', y=1, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_neighbours Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Behavior:**\n",
    "- *UMAP and KMeans Runs:* It performs 35 runs, applying UMAP for dimensionality reduction and KMeans for clustering.\n",
    "- *Centroid Calculation:* After each run, it calculates the centroids of the KMeans clusters and stores them.\n",
    "- *Mean and Standard Deviation Calculation:* Once all runs are complete, it calculates the mean and standard deviation of the centroids across the runs.\n",
    "- *Save Results:* The UMAP projections and KMeans centroids are saved as .npy files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_neighbors = 5, n_runs = 35, n_clusters = 10 (for KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the n_neighbors Analysis\n",
    "umap_projections_5 = np.load(f'umap_projections_neighbors_5.npy')\n",
    "centroid_mean_5_35= np.load(f'centroid_mean_5_35.npy')\n",
    "centroid_std_5_35= np.load(f'centroid_std_5_35.npy')\n",
    "kmeans_centroids_5 = np.load(f\"kmeans_centroids_neighbors_5.npy\")\n",
    "df_results_v2=pd.read_csv('result_table_neighbors_v2_5_35.csv')\n",
    "mean_distance_matrix_5_35= np.load(f'mean_distance_matrix_neighbors_5_35.npy')\n",
    "distance_matrix_std_5_35= np.load(f\"distance_matrix_std_5_35.npy\")\n",
    "normalized_distance_matrix_std_5_35= np.load(f'normalized_distance_matrix_std_5_35.npy')\n",
    "normalized_mean_distance_matrix_5_35= np.load(f'normalized_mean_distance_matrix_5_35.npy')\n",
    "mst_std_5_35= np.load(f'mst_std_5_35.npy')\n",
    "mst_5_35= np.load(f'mst_5_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "\n",
    "# Define the number of UMAP and KMeans runs\n",
    "n_runs = 35\n",
    "n_clusters = 10  # Set the number of clusters (for KMeans)\n",
    "n_neighbors = 5\n",
    "\n",
    "# Store UMAP and KMeans results for each run\n",
    "umap_projections = []\n",
    "kmeans_centroids_list = []  # Use this to store centroids for each run\n",
    "\n",
    "# Define a helper function to calculate the centroid of each cluster\n",
    "def calculate_centroids(kmeans, x_umap):\n",
    "    centroids = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = x_umap[kmeans.labels_ == i]\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        centroids.append(centroid)\n",
    "    return np.array(centroids)\n",
    "\n",
    "# Run UMAP and KMeans multiple times\n",
    "for run in range(n_runs):\n",
    "    # Apply UMAP with the same parameters for each run\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=0.1, n_components=2, random_state=None)  # No random_state to allow randomness, use random_state=None\n",
    "    x_train_umap = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Apply KMeans clustering on the UMAP projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_train_umap)\n",
    "\n",
    "    # Calculate centroids for this run\n",
    "    centroids = calculate_centroids(kmeans, x_train_umap)\n",
    "    \n",
    "    # Store the UMAP projections and KMeans models\n",
    "    umap_projections.append(x_train_umap)\n",
    "    kmeans_centroids_list.append(centroids)\n",
    "\n",
    "# Now we calculate the mean and standard deviation of the centroids across all runs\n",
    "kmeans_centroids = np.array(kmeans_centroids_list)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "# Calculate mean and std deviation for centroids' coordinates\n",
    "centroid_mean = np.mean(kmeans_centroids, axis=0)\n",
    "centroid_std = np.std(kmeans_centroids, axis=0)\n",
    "\n",
    "# Save the UMAP projections and KMeans centroids\n",
    "np.save(f'umap_projections_neighbors_{n_neighbors}.npy', np.array(umap_projections))\n",
    "np.save(f'kmeans_centroids_neighbors_{n_neighbors}.npy', np.array(kmeans_centroids_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "\n",
    "# Save the centroid_mean and centroid_std\n",
    "np.save(f'centroid_mean_{n_neighbors}_35.npy', np.array(centroid_mean))\n",
    "np.save(f'centroid_std_{n_neighbors}_35.npy', np.array(centroid_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x = np.zeros(10)\n",
    "std_dev_y = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_5[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_5[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_5[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_5_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2 = pd.DataFrame(data_v2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true = df_results_v2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true = trials_all_true[trials_all_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false = trials_all_true[~trials_all_true].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Save the result table to a CSV file\n",
    "df_results_v2.to_csv(f'result_table_neighbors_v2_{5}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal outliers process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array into a DataFrame with 'Cluster', 'x_mean', and 'y_mean'\n",
    "centroid_mean_neighbors_5_df = pd.DataFrame(centroid_mean_5_35, columns=['x_mean', 'y_mean'])\n",
    "centroid_mean_neighbors_5_df['Cluster'] = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y coordinates\n",
    "df_results_v2[['x', 'y']] = pd.DataFrame(df_results_v2['Centroid Coord'].tolist(), index=df_results_v2.index)\n",
    "\n",
    "# Merge the mean centroids dataframe with the results dataframe on 'Cluster'\n",
    "df_merged = pd.merge(df_results_v2, centroid_mean_neighbors_5_df, on='Cluster', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot changes in X-coordinate for each cluster over all runs\n",
    "n_runs = 35\n",
    "n_clusters = 10\n",
    "n_neighbors = 5\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_5[:, cluster, 0], marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Cluster {cluster} X-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('X Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot changes in Y-coordinate for each cluster over all runs\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_5[:, cluster, 1], marker='o', linestyle='-', color='g')\n",
    "    plt.title(f'Cluster {cluster} Y-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Y Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance from each centroid to its cluster's mean\n",
    "df_merged['Distance_to_Mean'] = np.sqrt((df_merged['x'] - df_merged['x_mean'])**2 + (df_merged['y'] - df_merged['y_mean'])**2)\n",
    "\n",
    "# Apply an outlier threshold (e.g., 90th percentile of the distance per cluster)\n",
    "def filter_outliers(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return df[df['Distance_to_Mean'] <= threshold]\n",
    "\n",
    "# Apply the filtering function for each cluster\n",
    "df_no_outliers = df_merged.groupby('Cluster').apply(filter_outliers).reset_index(drop=True)\n",
    "\n",
    "# Step 7: Drop unnecessary columns if needed (like 'x' and 'y' if only the distance matters)\n",
    "df_no_outliers_cleaned = df_no_outliers.drop(columns=['x', 'y', 'x_mean', 'y_mean'])\n",
    "\n",
    "# Step 8: Check the size of the resulting dataframe\n",
    "print(f\"Original DataFrame size: {df_merged.shape}\")\n",
    "print(f\"DataFrame size after removing outliers: {df_no_outliers_cleaned.shape}\")\n",
    "\n",
    "# Display the final dataframe to the user\n",
    "df_no_outliers_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by 'Cluster'\n",
    "clusters_grouped = df_no_outliers_cleaned.groupby('Cluster')\n",
    "\n",
    "# Create a dictionary to store arrays for each cluster's centroids\n",
    "clusters_centroids = {}\n",
    "\n",
    "# Loop through each group (cluster) and store the centroids in arrays\n",
    "for cluster, group in clusters_grouped:\n",
    "    # Extract centroids (x, y) as a NumPy array\n",
    "    centroids_array = np.array(group['Centroid Coord'].tolist())  # Assuming 'Centroid Coord' contains [x, y] pairs\n",
    "    clusters_centroids[cluster] = centroids_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the size of each cluster\n",
    "cluster_sizes = {cluster: len(centroids) for cluster, centroids in clusters_centroids.items()}\n",
    "\n",
    "# Print the size of each cluster\n",
    "for cluster, size in cluster_sizes.items():\n",
    "    print(f\"Cluster {cluster} has {size} centroids considered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to verify that if it is fine to have all clusters with the same number of centroids after filtering out outliers. This must be due to:\n",
    "- The Distance Distributions are Likely Very Similar\n",
    "- Uniform Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each cluster and plot the distribution of distances\n",
    "for cluster, group in clusters_grouped:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(group['Distance_to_Mean'], bins=10, edgecolor='black')\n",
    "    plt.title(f'Cluster {cluster}: Distance to Mean Distribution')\n",
    "    plt.xlabel('Distance to Mean')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentile threshold per cluster check\n",
    "def check_percentiles(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return threshold\n",
    "\n",
    "# Function applied to each cluster and print the result\n",
    "for cluster, group in clusters_grouped:\n",
    "    threshold = check_percentiles(group)\n",
    "    print(f\"Cluster {cluster}: 90th percentile threshold = {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster, calculate the 70th percentile of distances and filter accordingly\n",
    "for cluster, group in clusters_grouped:\n",
    "    # Calculate the 70th percentile threshold for the current cluster\n",
    "    threshold = np.percentile(group['Distance_to_Mean'], 70)\n",
    "    \n",
    "    # Filter centroids based on the 70th percentile\n",
    "    filtered_group = group[group['Distance_to_Mean'] <= threshold]\n",
    "    \n",
    "    # Print the size of the group before and after filtering\n",
    "    print(f\"Cluster {cluster}: Original size = {len(group)}, Filtered size = {len(filtered_group)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance matrix n=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Mean matrix\n",
    "Elemnt d_{ij} has the distance between the center of cluster i and cluster j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store distance matrices for each run\n",
    "distance_matrices_5_35 = []\n",
    "\n",
    "# Iterate over all runs and calculate the distance matrix for each run\n",
    "for run_centroids in kmeans_centroids_5:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix_5_35 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    distance_matrices_5_35.append(distance_matrix_5_35)\n",
    "\n",
    "# Convert the list of distance matrices to a numpy array (35 runs, 10x10 distance matrices)\n",
    "distance_matrices_5_35 = np.array(distance_matrices_5_35)\n",
    "\n",
    "# Calculate the mean distance matrix across all runs\n",
    "mean_distance_matrix_5_35 = np.mean(distance_matrices_5_35, axis=0)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix = (mean_distance_matrix_5_35 - np.min(mean_distance_matrix_5_35)) / (np.max(mean_distance_matrix_5_35) - np.min(mean_distance_matrix_5_35))\n",
    "\n",
    "# Plot of the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=5)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_5_all_runs.npy', distance_matrices_5_35)\n",
    "np.save('mean_distance_matrix_neighbors_5_35.npy', mean_distance_matrix_5_35)\n",
    "\n",
    "# Mean distance matrix\n",
    "print(f\"Mean distance matrix across all runs:\\n{mean_distance_matrix_5_35}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MST Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_distance_matrix_5_35= np.load(f'mean_distance_matrix_neighbors_5_35.npy')\n",
    "# mean_distance_matrix_5_35=np.round(mean_distance_matrix_5_35,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_5_35 = (mean_distance_matrix_5_35 - np.min(mean_distance_matrix_5_35)) / (np.max(mean_distance_matrix_5_35) - np.min(mean_distance_matrix_5_35))\n",
    "np.save('normalized_mean_distance_matrix_5_35.npy', normalized_mean_distance_matrix_5_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_5_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_5_35,3))\n",
    "np.save('G_5_35.npy',G_5_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_5_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_5_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_5_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_5_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_5_35 = nx.minimum_spanning_tree(G_5_35)\n",
    "np.save('mst_5_35.npy', mst_5_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_5_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_5_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_5_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_5_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST - n_neighbors=5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Std. dev. Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the pairwise distance matrix for the standard deviations\n",
    "distance_matrix_std_5_35 = cdist(centroid_std_5_35, centroid_std_5_35, metric='euclidean')\n",
    "\n",
    "# Normalize the distance matrix\n",
    "normalized_distance_matrix_std_5_35 = (distance_matrix_std_5_35 - np.min(distance_matrix_std_5_35)) / (np.max(distance_matrix_std_5_35) - np.min(distance_matrix_std_5_35))\n",
    "\n",
    "# Visualize the normalized distance matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(normalized_distance_matrix_std_5_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=5)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrix for later analysis\n",
    "np.save(\"distance_matrix_std_5_35.npy\", distance_matrix_std_5_35)\n",
    "np.save(\"normalized_distance_matrix_std_5_35.npy\", normalized_distance_matrix_std_5_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_std_5_35 = nx.from_numpy_array(np.round(normalized_distance_matrix_std_5_35,3))\n",
    "np.save('G_std_5_35.npy',G_std_5_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_std_5_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_std_5_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_std_5_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_std_5_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_std_5_35 = nx.minimum_spanning_tree(G_std_5_35)\n",
    "np.save('mst_std_5_35.npy',mst_std_5_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos_std_5_35 = nx.spring_layout(mst_std_5_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_std_5_35, pos_std_5_35, with_labels=True, node_color='lightyellow', edge_color='green', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels_std_5_35 = nx.get_edge_attributes(mst_std_5_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_5_35, pos_std_5_35, edge_labels=edge_labels_std_5_35, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST Std. Deviation - n_neighbors=5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower upper bound MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_5_35, \"MST - Mean Distances\", axes[0], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_5_35, \"MST - Lower Limit\", axes[1], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_5_35, \"MST - Upper Limit\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_std_5_35 = nx.from_numpy_array(np.round(normalized_distance_matrix_std_5_35,3))\n",
    "np.save('G_std_5_35.npy',G_std_5_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_std_5_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_std_5_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_std_5_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_std_5_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_std_5_35 = nx.minimum_spanning_tree(G_std_5_35)\n",
    "np.save('mst_std_5_35.npy',mst_std_5_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos_std_5_35 = nx.spring_layout(mst_std_5_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_std_5_35, pos_std_5_35, with_labels=True, node_color='lightyellow', edge_color='green', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels_std_5_35 = nx.get_edge_attributes(mst_std_5_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_5_35, pos_std_5_35, edge_labels=edge_labels_std_5_35, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST Std. Deviation - n_neighbors=5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(matrix, title, xlabel, ylabel, figsize=(10, 8), cmap=\"viridis\", annot=True):\n",
    "    \"\"\"\n",
    "    Plots a heatmap for a given matrix with customizable parameters.\n",
    "\n",
    "    Args:\n",
    "        matrix (ndarray): The 2D matrix to plot as a heatmap.\n",
    "        title (str): Title of the heatmap.\n",
    "        xlabel (str): Label for the x-axis.\n",
    "        ylabel (str): Label for the y-axis.\n",
    "        figsize (tuple): Size of the figure (default: (10, 8)).\n",
    "        cmap (str): Color map to use (default: \"viridis\").\n",
    "        annot (bool): Whether to annotate cells with their values (default: True).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(matrix, annot=annot, cmap=cmap, fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function for both heatmaps\n",
    "plot_heatmap(\n",
    "    normalized_distance_matrix_std_5_35,\n",
    "    title=\"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=5)\",\n",
    "    xlabel=\"Cluster\",\n",
    "    ylabel=\"Cluster\",\n",
    "    figsize=(8, 6)\n",
    ")\n",
    "\n",
    "plot_heatmap(\n",
    "    normalized_mean_distance_matrix_5_35,\n",
    "    title=\"Normalized Mean Distance Matrix (k=10, n_neighbors=5)\",\n",
    "    xlabel=\"Cluster\",\n",
    "    ylabel=\"Cluster\",\n",
    "    figsize=(8, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create heatmaps\n",
    "def plot_heatmaps_side_by_side(matrices, titles, figsize=(16, 8), cmap=\"viridis\", annot=True):\n",
    "    \"\"\"\n",
    "    Plots multiple heatmaps side by side for given matrices and titles.\n",
    "\n",
    "    Args:\n",
    "        matrices (list): List of 2D matrices to plot as heatmaps.\n",
    "        titles (list): List of titles corresponding to each matrix.\n",
    "        figsize (tuple): Size of the entire figure (default: (16, 8)).\n",
    "        cmap (str): Color map to use for all heatmaps (default: \"viridis\").\n",
    "        annot (bool): Whether to annotate cells with their values (default: True).\n",
    "    \"\"\"\n",
    "    n = len(matrices)  # Number of heatmaps\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "\n",
    "    for i, (matrix, title) in enumerate(zip(matrices, titles)):\n",
    "        sns.heatmap(matrix, annot=annot, cmap=cmap, fmt=\".2f\", linewidths=0.5, ax=axes[i])\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_xlabel(\"Cluster\")\n",
    "        axes[i].set_ylabel(\"Cluster\" if i == 0 else \"\")  # Only label y-axis for the first plot\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the two heatmaps\n",
    "plot_heatmaps_side_by_side(\n",
    "    matrices=[\n",
    "        normalized_distance_matrix_std_5_35,\n",
    "        normalized_mean_distance_matrix_5_35\n",
    "    ],\n",
    "    titles=[\n",
    "        \"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=5)\",\n",
    "        \"Normalized Mean Distance Matrix (k=10, n_neighbors=5)\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean Matrix --> spatial distances --> **position** of clusters in UMAP.\n",
    "- Std. Deviation --> variability differences --> focuses on **stability**.\n",
    "\n",
    "Standard deviation heatmap complements the mean matrix by showing not just where clusters are located in the embedding but also how consistently they are represented across multiple runs. Together, they provide a complete picture of spatial relationships and stability in your UMAP analysis.\n",
    "\n",
    "**Why Look for Low Values in Both Graphs?**\n",
    "- Spatial Proximity (Mean Matrix):\n",
    "\n",
    "    Clusters that are spatially close (low values in the mean matrix) may share relationships in the original data, such as being part of the same neighborhood or having similar data features.\n",
    "- Similar Variability (Std Deviation Matrix):\n",
    "\n",
    "    Clusters with low variability differences (low values in the std deviation matrix) are consistently represented across multiple UMAP runs. This indicates that their behavior is robust and not affected by randomness or parameter sensitivity.\n",
    "\n",
    "*Clusters that are spatially close and consistently represented likely:*\n",
    "\n",
    "- Belong to the same data manifold.\n",
    "- Represent well-defined groups in your dataset.\n",
    "- Are stable and reliable in your UMAP projection.\n",
    "\n",
    "**What Do Low-Low Clusters Tell Us?**\n",
    "- Stable Relationships:\n",
    "\n",
    "    If two clusters have low values in both graphs, their relationship is stable and meaningful across runs.\n",
    "    They might share similar structures or characteristics in the data.\n",
    "- Robustness:\n",
    "\n",
    "    Clusters that show low variability are less sensitive to UMAP parameters, making them more reliable for downstream tasks (e.g., classification, clustering).\n",
    "- Potential for Merging:\n",
    "\n",
    "    If two clusters have consistently low distances in both graphs, they might represent subclusters of the same group. This could indicate that they can be merged into a single cluster, depending on your analysis goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for \"low\" values\n",
    "threshold = 0.65\n",
    "\n",
    "# Identify pairs of clusters with low values in both matrices\n",
    "low_low_pairs = []\n",
    "for i in range(normalized_mean_distance_matrix_5_35.shape[0]):\n",
    "    for j in range(normalized_mean_distance_matrix_5_35.shape[1]):\n",
    "        if i != j:  # Skip diagonal\n",
    "            mean_value = normalized_mean_distance_matrix_5_35[i, j]\n",
    "            std_value = normalized_distance_matrix_std_5_35[i, j]\n",
    "            if mean_value < threshold and std_value < threshold:\n",
    "                low_low_pairs.append((i, j, mean_value, std_value))\n",
    "\n",
    "# Display the results\n",
    "for pair in low_low_pairs:\n",
    "    print(f\"Clusters {pair[0]} and {pair[1]}: Mean Distance = {pair[2]:.2f}, Std Distance = {pair[3]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.65 can seems like a high value, since it is on the upper-mid range.\n",
    "\n",
    "Depending on the goal of the analysis we can think of it as:\n",
    "- If the aim is to identify the strongest relationships between clusters, a lower threshold would make more sense.\n",
    "- If we want to explore the broader connections, then it is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Replace with your cluster pairs from low_low_pairs\n",
    "low_low_pairs = [(0, 9, 0.64, 0.51), (1, 7, 0.65, 0.30),(1, 8, 0.62, 0.50)]  # Example cluster pairs\n",
    "\n",
    "# UMAP projections and cluster labels (replace with your actual data)\n",
    "umap_projections = np.load(\"umap_projections_neighbors_5.npy\")\n",
    "kmeans_labels = np.load(\"kmeans_labels_list_5_35.npy\")  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Function to plot clusters\n",
    "def plot_clusters(umap_projection, labels, cluster_pair, run_idx):\n",
    "    cluster_a, cluster_b = cluster_pair\n",
    "    points_a = umap_projection[labels == cluster_a]\n",
    "    points_b = umap_projection[labels == cluster_b]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(points_a[:, 0], points_a[:, 1], color=\"blue\", label=f\"Cluster {cluster_a}\", alpha=0.6)\n",
    "    plt.scatter(points_b[:, 0], points_b[:, 1], color=\"orange\", label=f\"Cluster {cluster_b}\", alpha=0.6)\n",
    "    plt.title(f\"Run {run_idx}: Cluster {cluster_a} vs. Cluster {cluster_b}\")\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze each cluster pair\n",
    "for cluster_pair in low_low_pairs:\n",
    "    cluster_a, cluster_b = cluster_pair[0], cluster_pair[1]\n",
    "    print(f\"Analyzing Cluster Pair: {cluster_a} and {cluster_b}\")\n",
    "    \n",
    "    # For simplicity, visualize them in a specific UMAP run (e.g., the first run)\n",
    "    run_idx = 0  # Use the first run for visualization\n",
    "    plot_clusters(umap_projections[run_idx], kmeans_labels[run_idx], (cluster_a, cluster_b), run_idx)\n",
    "\n",
    "    # Calculate additional statistics if needed\n",
    "    distances_a_to_b = np.linalg.norm(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a].mean(axis=0) - \n",
    "                                      umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b].mean(axis=0))\n",
    "    print(f\"Mean Centroid Distance (Run {run_idx}): {distances_a_to_b:.2f}\")\n",
    "\n",
    "    # Variability comparison\n",
    "    cluster_a_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a], axis=0)\n",
    "    cluster_b_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b], axis=0)\n",
    "    print(f\"Cluster {cluster_a} Std Dev: {cluster_a_std}\")\n",
    "    print(f\"Cluster {cluster_b} Std Dev: {cluster_b_std}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 1 and Cluster 8** have a moderate spatial relationship with visible overlap in the UMAP space. Their differing variability patterns suggest distinct structures, but the overlap points might represent shared features or transitions between the clusters.\n",
    "The large spatial separation between their centroids suggests they represent distinct structures or classes in the data.\n",
    "\n",
    "**Cluster 0 and Cluster 9** 9 appears more compact and stable, while Cluster 0 is larger and more variable.\n",
    "Their distinct regions in the UMAP space and differing standard deviations reinforce their meaningful separation.\n",
    "Insights from Variability:\n",
    "\n",
    "The variability of Cluster 0 could indicate sensitivity to UMAP parameters or noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_mnist_overlap(umap_projection, kmeans_labels, mnist_labels, cluster_pair, run_idx):\n",
    "    cluster_a, cluster_b = cluster_pair\n",
    "\n",
    "    # Get points in Cluster A and Cluster B\n",
    "    points_a_indices = np.where(kmeans_labels == cluster_a)[0]\n",
    "    points_b_indices = np.where(kmeans_labels == cluster_b)[0]\n",
    "\n",
    "    # Find the overlapping points (indices)\n",
    "    overlap_indices = np.intersect1d(points_a_indices, points_b_indices)\n",
    "\n",
    "    # Get the original labels of overlapping points\n",
    "    overlap_labels = np.array(mnist_labels)[overlap_indices]\n",
    "\n",
    "    # Analyze the original labels\n",
    "    overlap_label_counts = pd.Series(overlap_labels).value_counts()\n",
    "\n",
    "    # Display the overlap statistics\n",
    "    print(f\"Overlap between Cluster {cluster_a} and Cluster {cluster_b} (Run {run_idx}):\")\n",
    "    print(f\"Number of overlapping points: {len(overlap_indices)}\")\n",
    "    print(f\"Original label distribution of overlapping points:\\n{overlap_label_counts}\")\n",
    "\n",
    "    # Visualize the overlap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(umap_projection[points_a_indices, 0], umap_projection[points_a_indices, 1], color=\"blue\", label=f\"Cluster {cluster_a}\", alpha=0.5)\n",
    "    plt.scatter(umap_projection[points_b_indices, 0], umap_projection[points_b_indices, 1], color=\"orange\", label=f\"Cluster {cluster_b}\", alpha=0.5)\n",
    "    if len(overlap_indices) > 0:\n",
    "        plt.scatter(umap_projection[overlap_indices, 0], umap_projection[overlap_indices, 1], color=\"red\", label=\"Overlap\", alpha=0.7)\n",
    "    plt.title(f\"Cluster Overlap: Cluster {cluster_a} vs. Cluster {cluster_b} (Run {run_idx})\")\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Load your MNIST data\n",
    "dataloader = MnistDataloader(\n",
    "    training_images_filepath=\"train-images.idx3-ubyte\",\n",
    "    training_labels_filepath=\"train-labels.idx1-ubyte\",\n",
    "    test_images_filepath=\"t10k-images.idx3-ubyte\",\n",
    "    test_labels_filepath=\"t10k-labels.idx1-ubyte\"\n",
    ")\n",
    "\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = dataloader.load_data()\n",
    "\n",
    "# Flatten the training images for UMAP (if needed for alignment with projections)\n",
    "x_train_flattened = np.array([np.array(img).flatten() for img in x_train])\n",
    "\n",
    "# Example variables (replace these with your actual data)\n",
    "run_idx = 0  # Analyze the first UMAP run\n",
    "cluster_pair = (1, 8)  # Compare Cluster 1 and Cluster 8\n",
    "umap_projection = umap_projections[run_idx]  # UMAP projection for the given run\n",
    "kmeans_labels = kmeans_labels[run_idx]  # KMeans labels for the given run\n",
    "\n",
    "# Examine overlap\n",
    "examine_mnist_overlap(umap_projection, kmeans_labels, y_train, cluster_pair, run_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interval of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_5_35 = distance_matrix_std_5_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_5_35 = z_score * sem_matrix_5_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_5_35 = mean_distance_matrix_5_35 - margin_of_error_matrix_5_35\n",
    "upper_limit_intconf_matrix_5_35 = mean_distance_matrix_5_35 + margin_of_error_matrix_5_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_5_35 = np.maximum(lower_limit_intconf_matrix_5_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_5_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_5_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_5_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_5_35.npy', lower_limit_intconf_matrix_5_35)\n",
    "np.save('upper_limit_intconf_matrix_5_35.npy', upper_limit_intconf_matrix_5_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_5_35 = normalize_matrix(lower_limit_intconf_matrix_5_35)\n",
    "norm_upper_limit_intconf_matrix_5_35 = normalize_matrix(upper_limit_intconf_matrix_5_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_5_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=5)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=5)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_5_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=5)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define clusters\n",
    "clusters = np.arange(1, 10)  # Assuming we are skipping Cluster 0 (comparison against itself)\n",
    "\n",
    "# Define the data for each n_neighbors\n",
    "data = {\n",
    "    5: {\n",
    "        \"mean\": mean_distance_matrix_5_35[0, 1:],  # Distances from cluster 0 to other clusters\n",
    "        \"lower\": lower_limit_intconf_matrix_5_35[0, 1:],  # Lower bounds\n",
    "        \"upper\": upper_limit_intconf_matrix_5_35[0, 1:]    # Upper bounds\n",
    "    },\n",
    "    10: {\n",
    "        \"mean\": mean_distance_matrix_10_35[0, 1:],  # Distances from cluster 0 to other clusters\n",
    "        \"lower\": lower_limit_intconf_matrix_10_35[0, 1:],  # Lower bounds\n",
    "        \"upper\": upper_limit_intconf_matrix_10_35[0, 1:]    # Upper bounds\n",
    "    },\n",
    "    20: {\n",
    "        \"mean\": mean_distance_matrix_20_35[0, 1:],  # Distances from cluster 0 to other clusters\n",
    "        \"lower\": lower_limit_intconf_matrix_20_35[0, 1:],  # Lower bounds\n",
    "        \"upper\": upper_limit_intconf_matrix_20_35[0, 1:]    # Upper bounds\n",
    "    },\n",
    "    30: {\n",
    "        \"mean\": mean_distance_matrix_30_35[0, 1:],  # Distances from cluster 0 to other clusters\n",
    "        \"lower\": lower_limit_intconf_matrix_30_35[0, 1:],  # Lower bounds\n",
    "        \"upper\": upper_limit_intconf_matrix_30_35[0, 1:]    # Upper bounds\n",
    "    },\n",
    "    50: {\n",
    "        \"mean\": mean_distance_matrix_50_35[0, 1:],  # Distances from cluster 0 to other clusters\n",
    "        \"lower\": lower_limit_intconf_matrix_50_35[0, 1:],  # Lower bounds\n",
    "        \"upper\": upper_limit_intconf_matrix_50_35[0, 1:]    # Upper bounds\n",
    "    },\n",
    "    100: {\n",
    "    \"mean\": mean_distance_matrix_100_35[0, 1:],  # Distances from cluster 0 to other clusters\n",
    "    \"lower\": lower_limit_intconf_matrix_100_35[0, 1:],  # Lower bounds\n",
    "    \"upper\": upper_limit_intconf_matrix_100_35[0, 1:]    # Upper bounds\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define colors for each n_neighbors\n",
    "colors = {5: \"orange\", 10: \"blue\", 20: \"yellow\", 30: \"grey\", 50: \"green\", 100: \"red\"}\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "width = 0.15  # Bar width\n",
    "x = np.arange(len(clusters))  # X positions for clusters\n",
    "\n",
    "for idx, (n_neighbors, values) in enumerate(data.items()):\n",
    "    # Calculate positions for the current set of bars\n",
    "    x_positions = x + (idx - len(data)/2) * width\n",
    "\n",
    "    # Plot bars for the mean distances\n",
    "    ax.bar(\n",
    "        x_positions,\n",
    "        values[\"mean\"],  # Mean distances\n",
    "        yerr=[\n",
    "            values[\"mean\"] - values[\"lower\"],  # Lower error\n",
    "            values[\"upper\"] - values[\"mean\"]   # Upper error\n",
    "        ],\n",
    "        width=width,\n",
    "        color=colors[n_neighbors],\n",
    "        alpha=0.7,\n",
    "        label=f\"n={n_neighbors}\",\n",
    "        capsize=5\n",
    "    )\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel(\"Clusters\", fontsize=14)\n",
    "ax.set_ylabel(\"Distance\", fontsize=14)\n",
    "ax.set_title(\"Confidence Intervals of Distances from Cluster 0 to Other Clusters\", fontsize=16)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"{i}\" for i in clusters], fontsize=12)\n",
    "ax.legend(title=\"n_neighbors\", fontsize=10)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Define the clusters\n",
    "clusters = np.arange(10)  # Include all clusters (0 to 9)\n",
    "\n",
    "# Start a PDF document to save the plots\n",
    "with PdfPages('confidence_intervals_clusters.pdf') as pdf:\n",
    "    for base_cluster in clusters:\n",
    "        # Define the data for each n_neighbors\n",
    "        data = {\n",
    "            5: {\n",
    "                \"mean\": mean_distance_matrix_5_35[base_cluster, np.arange(10) != base_cluster],  # Skip the base cluster\n",
    "                \"lower\": lower_limit_intconf_matrix_5_35[base_cluster, np.arange(10) != base_cluster],\n",
    "                \"upper\": upper_limit_intconf_matrix_5_35[base_cluster, np.arange(10) != base_cluster]\n",
    "            },\n",
    "            10: {\n",
    "                \"mean\": mean_distance_matrix_10_35[base_cluster, np.arange(10) != base_cluster],\n",
    "                \"lower\": lower_limit_intconf_matrix_10_35[base_cluster, np.arange(10) != base_cluster],\n",
    "                \"upper\": upper_limit_intconf_matrix_10_35[base_cluster, np.arange(10) != base_cluster]\n",
    "            },\n",
    "            20: {\n",
    "                \"mean\": mean_distance_matrix_20_35[base_cluster, np.arange(10) != base_cluster],\n",
    "                \"lower\": lower_limit_intconf_matrix_20_35[base_cluster, np.arange(10) != base_cluster],\n",
    "                \"upper\": upper_limit_intconf_matrix_20_35[base_cluster, np.arange(10) != base_cluster]\n",
    "            },\n",
    "            30: {\n",
    "                \"mean\": mean_distance_matrix_30_35[base_cluster, np.arange(10) != base_cluster],\n",
    "                \"lower\": lower_limit_intconf_matrix_30_35[base_cluster, np.arange(10) != base_cluster],\n",
    "                \"upper\": upper_limit_intconf_matrix_30_35[base_cluster, np.arange(10) != base_cluster]\n",
    "            },\n",
    "            50: {\n",
    "                \"mean\": mean_distance_matrix_50_35[base_cluster, np.arange(10) != base_cluster],\n",
    "                \"lower\": lower_limit_intconf_matrix_50_35[base_cluster, np.arange(10) != base_cluster],\n",
    "                \"upper\": upper_limit_intconf_matrix_50_35[base_cluster, np.arange(10) != base_cluster]\n",
    "            },\n",
    "            100: {\n",
    "                \"mean\": mean_distance_matrix_100_35[base_cluster, np.arange(10) != base_cluster],\n",
    "                \"lower\": lower_limit_intconf_matrix_100_35[base_cluster, np.arange(10) != base_cluster],\n",
    "                \"upper\": upper_limit_intconf_matrix_100_35[base_cluster, np.arange(10) != base_cluster]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Define colors for each n_neighbors\n",
    "        colors = {5: \"orange\", 10: \"blue\", 20: \"yellow\", 30: \"grey\", 50: \"green\", 100: \"red\"}\n",
    "\n",
    "        # Plotting\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        width = 0.15  # Bar width\n",
    "        x = np.arange(len(clusters) - 1)  # X positions for clusters\n",
    "\n",
    "        for idx, (n_neighbors, values) in enumerate(data.items()):\n",
    "            # Calculate positions for the current set of bars\n",
    "            x_positions = x + (idx - len(data) / 2) * width\n",
    "\n",
    "            # Plot bars for the mean distances\n",
    "            ax.bar(\n",
    "                x_positions,\n",
    "                values[\"mean\"],  # Mean distances\n",
    "                yerr=[\n",
    "                    values[\"mean\"] - values[\"lower\"],  # Lower error\n",
    "                    values[\"upper\"] - values[\"mean\"]   # Upper error\n",
    "                ],\n",
    "                width=width,\n",
    "                color=colors[n_neighbors],\n",
    "                alpha=0.7,\n",
    "                label=f\"n={n_neighbors}\",\n",
    "                capsize=5\n",
    "            )\n",
    "\n",
    "        # Add labels, title, and legend\n",
    "        ax.set_xlabel(\"Clusters\", fontsize=12)\n",
    "        ax.set_ylabel(\"Distance\", fontsize=12)\n",
    "        ax.set_title(f\"Confidence Intervals of Distances from Cluster {base_cluster} to Other Clusters\", fontsize=14)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([f\"{i}\" for i in np.arange(10) if i != base_cluster], fontsize=10)\n",
    "        ax.legend(title=\"n_neighbors\", fontsize=10)\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the figure to the PDF\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"All graphs have been saved to 'confidence_intervals_clusters.pdf'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intra class evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option A) with fix radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "radius = 0.5\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_5_35 = []\n",
    "neighbor_counts_5_35 = []\n",
    "kmeans_labels_list_5_35 = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_5):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_5_35 = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_5_35.append(kmeans_labels_5_35)\n",
    "    \n",
    "    run_max_distances_5_35 = []\n",
    "    run_neighbor_counts_5_35 = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_5_35 == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_5_35 = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_5_35.append(max_distance_5_35)\n",
    "        \n",
    "        # Calculate number of neighbors within the radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_5_35 = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_5_35.append(neighbors_within_radius_5_35)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_5_35.append(run_max_distances_5_35)\n",
    "    neighbor_counts_5_35.append(run_neighbor_counts_5_35)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_5_35 = np.array(max_distances_5_35)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_5_35 = np.array(neighbor_counts_5_35)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_5_35 = np.array(kmeans_labels_list_5_35)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_5_35.npy', max_distances_5_35)\n",
    "np.save('neighbor_counts_within_radius_5_35.npy', neighbor_counts_5_35)\n",
    "np.save('kmeans_labels_list_5_35 .npy', kmeans_labels_list_5_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_5_35)\n",
    "print(\"\\nNeighbor counts within radius for each run and each cluster:\\n\", neighbor_counts_5_35)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option B) with radius --> half of the min. distance between two clusters.\n",
    "\n",
    "It is calculated from dist. matrix of a specific run. So, it can be found in any of the 35 runs but it won't necessarily appear in the mean dist. matrix because here vlues are averaged across all runs and thus will likely not match specific values from an individual run's matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_distances_5_35_d = np.load(f'max_intra_cluster_distances_dynamic_5_35.npy')\n",
    "neighbor_counts_5_35_d = np.load(f'neighbor_counts_within_dynamic_radius_5_35.npy')\n",
    "kmeans_labels_list_5_35_d = np.load(f'kmeans_labels_list_5_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the minimum distance between cluster centroids across all runs\n",
    "# all_min_distances = []\n",
    "\n",
    "# for run_centroids in kmeans_centroids_5:\n",
    "#     # Compute pairwise distances between centroids\n",
    "#     pairwise_distances = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "#     # Get the minimum non-zero distance\n",
    "#     min_distance = np.min(pairwise_distances[np.nonzero(pairwise_distances)])\n",
    "#     all_min_distances.append(min_distance)\n",
    "\n",
    "# # Use half the smallest minimum distance as the radius\n",
    "# dynamic_radius = min(all_min_distances) / 2\n",
    "# print(f\"Dynamic radius for neighbor count: {dynamic_radius}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to track the minimum distance and corresponding clusters\n",
    "overall_min_distance = float('inf')\n",
    "min_distance_clusters = None\n",
    "min_distance_run_idx = None\n",
    "\n",
    "for run_idx, run_centroids in enumerate(kmeans_centroids_5):\n",
    "    # Compute pairwise distances between centroids\n",
    "    pairwise_distances_5 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    \n",
    "    # Get the indices of the minimum non-zero distance\n",
    "    np.fill_diagonal(pairwise_distances_5, np.inf)  # Ignore zero distances (self-comparisons)\n",
    "    min_distance = np.min(pairwise_distances_5)\n",
    "    if min_distance < overall_min_distance:\n",
    "        overall_min_distance = min_distance\n",
    "        # Find the indices of the clusters corresponding to the minimum distance\n",
    "        cluster_indices = np.unravel_index(np.argmin(pairwise_distances_5), pairwise_distances_5.shape)\n",
    "        min_distance_clusters = cluster_indices\n",
    "        min_distance_run_idx = run_idx\n",
    "\n",
    "# Calculate dynamic radius\n",
    "dynamic_radius_5_35 = overall_min_distance / 2\n",
    "print(f\"Dynamic radius: {dynamic_radius_5_35}\")\n",
    "print(f\"Minimum distance: {overall_min_distance}\")\n",
    "print(f\"Clusters contributing to minimum distance: {min_distance_clusters}\")\n",
    "print(f\"Run index: {min_distance_run_idx}\")\n",
    "\n",
    "# Save dynamic radius\n",
    "np.save('dynamic_radius_results_5_35.npy', dynamic_radius_5_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "\n",
    "# Dynamic radius, previously calculated\n",
    "radius = dynamic_radius_5_35\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_5_35_d = []\n",
    "neighbor_counts_5_35_d = []\n",
    "kmeans_labels_list_5_35_d = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_5):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_5_35_d = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_5_35_d.append(kmeans_labels_5_35_d)\n",
    "    \n",
    "    run_max_distances_5_35_d = []\n",
    "    run_neighbor_counts_5_35_d = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_5_35_d == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_5_35_d = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_5_35_d.append(max_distance_5_35_d)\n",
    "        \n",
    "        # Calculate number of neighbors within the dynamic radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_5_35_d = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_5_35_d.append(neighbors_within_radius_5_35_d)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_5_35_d.append(run_max_distances_5_35_d)\n",
    "    neighbor_counts_5_35_d.append(run_neighbor_counts_5_35_d)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_5_35_d = np.array(max_distances_5_35_d)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_5_35_d = np.array(neighbor_counts_5_35_d)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_5_35_d = np.array(kmeans_labels_list_5_35_d)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_dynamic_5_35.npy', max_distances_5_35_d)\n",
    "np.save('neighbor_counts_within_dynamic_radius_5_35.npy', neighbor_counts_5_35_d)\n",
    "np.save('kmeans_labels_list_5_35.npy', kmeans_labels_list_5_35_d)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_5_35_d)\n",
    "print(\"\\nNeighbor counts within dynamic radius for each run and each cluster:\\n\", neighbor_counts_5_35_d)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot neighbor counts for each cluster across all runs\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Iterate through each cluster\n",
    "for cluster_idx in range(neighbor_counts_5_35_d.shape[1]):\n",
    "    plt.subplot(2, 5, cluster_idx + 1)  # Create subplots for 10 clusters (2 rows, 5 columns)\n",
    "    plt.plot(range(1, neighbor_counts_5_35_d.shape[0] + 1), neighbor_counts_5_35_d[:, cluster_idx], marker='o')\n",
    "    plt.title(f'Cluster {cluster_idx}')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Neighbor Count')\n",
    "    plt.xticks(range(1, neighbor_counts_5_35_d.shape[0] + 1, 5))  # Show every 5th run on the x-axis for clarity\n",
    "    plt.grid(True)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Neighbor Counts per Cluster Across Runs', y=1.02, fontsize=16)  # Add a global title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "# Load neighbor counts (if not already loaded)\n",
    "neighbor_counts_5_35_d = np.load('neighbor_counts_within_dynamic_radius_5_35.npy')  # Shape: (n_runs, n_clusters)\n",
    "\n",
    "# Calculate mean and max values across clusters for each run\n",
    "mean_neighbors = np.mean(neighbor_counts_5_35_d, axis=1)  # Shape: (n_runs,)\n",
    "max_neighbors = np.max(neighbor_counts_5_35_d, axis=1)    # Shape: (n_runs,)\n",
    "\n",
    "# Compute trend lines for mean and max\n",
    "runs = np.arange(1, len(mean_neighbors) + 1)\n",
    "mean_slope, mean_intercept, _, _, _ = linregress(runs, mean_neighbors)\n",
    "max_slope, max_intercept, _, _, _ = linregress(runs, max_neighbors) \n",
    "\n",
    "# Calculate trend line values\n",
    "mean_trend = mean_slope * runs + mean_intercept\n",
    "max_trend = max_slope * runs + max_intercept\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Mean neighbor counts\n",
    "plt.plot(runs, mean_neighbors, label='Mean Neighbor Count', marker='o', color='blue')\n",
    "\n",
    "# Max neighbor counts\n",
    "plt.plot(runs, max_neighbors, label='Max Neighbor Count', marker='s', color='orange')\n",
    "\n",
    "# Trend lines\n",
    "plt.plot(runs, mean_trend, linestyle='--', color='green',label='Mean Trend Line')\n",
    "plt.plot(runs, max_trend, linestyle='--', color='green', label='Max Trend Line')\n",
    "\n",
    "# Add labels, legend, and title\n",
    "plt.title('n=5 Neighbor Counts Across Runs (Mean vs. Max with Trend Lines)', fontsize=16)\n",
    "plt.xlabel('Run', fontsize=12)\n",
    "plt.ylabel('Neighbor Count', fontsize=12)\n",
    "plt.xticks(range(1, len(mean_neighbors) + 1, 5))  # Show every 5th run for readability\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(f'neighbor_counts_plot_n_5_35.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle\n",
    "\n",
    "# Load the UMAP projections and KMeans labels\n",
    "umap_projections = np.load('umap_projections_neighbors_5.npy')\n",
    "kmeans_centroids = np.load('kmeans_centroids_neighbors_5.npy')\n",
    "kmeans_labels = np.load('kmeans_labels_list_5_35.npy')\n",
    "\n",
    "# Define the dynamic radius\n",
    "radius = dynamic_radius_5_35\n",
    "\n",
    "# Select a specific run to visualize\n",
    "run_idx = 3  # Choose the run index (e.g., run 3)\n",
    "x_umap = umap_projections[run_idx]\n",
    "centroids = kmeans_centroids[run_idx]\n",
    "labels = kmeans_labels[run_idx]\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "for cluster_idx in range(n_clusters):\n",
    "    # Get points in the current cluster\n",
    "    cluster_points = x_umap[labels == cluster_idx]\n",
    "    \n",
    "    # Plot the points\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster_idx}', alpha=0.7)\n",
    "    \n",
    "    # Add a circle with the dynamic radius around the centroid\n",
    "    centroid = centroids[cluster_idx]\n",
    "    circle = Circle(centroid, radius, color='black', fill=False, linestyle='--', alpha=0.8)\n",
    "    plt.gca().add_artist(circle)\n",
    "    \n",
    "    # Plot the centroid\n",
    "    plt.scatter(centroid[0], centroid[1], color='red', s=100, edgecolor='black', label=f'Centroid {cluster_idx}')\n",
    "\n",
    "# Final touches\n",
    "plt.title(f\"Cluster Visualization with Dynamic Radius (Run {run_idx})\")\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_neighbors= 10, n_runs = 35, n_clusters = 10 (for KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the n_neighbors Analysis\n",
    "umap_projections_10 = np.load(f'umap_projections_neighbors_10.npy')\n",
    "centroid_mean_10_35= np.load(f'centroid_mean_10_35.npy')\n",
    "centroid_std_10_35= np.load(f'centroid_std_10_35.npy')\n",
    "kmeans_centroids_10 = np.load(f\"kmeans_centroids_neighbors_10.npy\")\n",
    "df_results_v2=pd.read_csv('result_table_neighbors_10_35.csv')\n",
    "mean_distance_matrix_10_35= np.load(f'mean_distance_matrix_neighbors_10_35.npy')\n",
    "distance_matrix_std_10_35= np.load(f\"distance_matrix_std_10_35.npy\")\n",
    "normalized_distance_matrix_std_10_35= np.load(f'normalized_distance_matrix_std_10_35.npy')\n",
    "normalized_mean_distance_matrix_10_35= np.load(f'normalized_mean_distance_matrix_10_35.npy')\n",
    "mst_std_10_35= np.load(f'mst_std_10_35.npy')\n",
    "mst_10_35= np.load(f'mst_10_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of UMAP and KMeans runs\n",
    "n_runs = 35\n",
    "n_clusters = 10  # Set the number of clusters (for KMeans)\n",
    "n_neighbors = 10\n",
    "\n",
    "# Store UMAP and KMeans results for each run\n",
    "umap_projections_10_35 = []\n",
    "kmeans_centroids_list_10_35 = []  # Use this to store centroids for each run\n",
    "\n",
    "# Define a helper function to calculate the centroid of each cluster\n",
    "def calculate_centroids(kmeans, x_umap):\n",
    "    centroids = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = x_umap[kmeans.labels_ == i]\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        centroids.append(centroid)\n",
    "    return np.array(centroids)\n",
    "\n",
    "# Run UMAP and KMeans multiple times\n",
    "for run in range(n_runs):\n",
    "    # Apply UMAP with the same parameters for each run\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=0.1, n_components=2, random_state=None)  # No random_state to allow randomness, use random_state=None\n",
    "    x_train_umap = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Apply KMeans clustering on the UMAP projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_train_umap)\n",
    "\n",
    "    # Calculate centroids for this run\n",
    "    centroids = calculate_centroids(kmeans, x_train_umap)\n",
    "    \n",
    "    # Store the UMAP projections and KMeans models\n",
    "    umap_projections_10_35.append(x_train_umap)\n",
    "    kmeans_centroids_list_10_35.append(centroids)\n",
    "\n",
    "# Now we calculate the mean and standard deviation of the centroids across all runs\n",
    "kmeans_centroids_10_35 = np.array(kmeans_centroids_list_10_35)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "# Calculate mean and std deviation for centroids' coordinates\n",
    "centroid_mean = np.mean(kmeans_centroids_10_35, axis=0)\n",
    "centroid_std = np.std(kmeans_centroids_10_35, axis=0)\n",
    "\n",
    "# Save the UMAP projections and KMeans centroids using NumPy\n",
    "np.save(f'umap_projections_neighbors_{n_neighbors}.npy', np.array(umap_projections_10_35))\n",
    "np.save(f'kmeans_centroids_neighbors_{n_neighbors}.npy', np.array(kmeans_centroids_list_10_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the UMAP projections\n",
    "umap_projections_10 = np.load(f'umap_projections_neighbors_10.npy')\n",
    "\n",
    "# To see the contents of the UMAP projections\n",
    "print(umap_projections_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_centroids_10 = np.load(f\"kmeans_centroids_neighbors_10.npy\")  # Load the saved centroids data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the centroid_mean and centroid_std\n",
    "np.save(f'centroid_mean_{n_neighbors}_35.npy', np.array(centroid_mean))\n",
    "np.save(f'centroid_std_{n_neighbors}_35.npy', np.array(centroid_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_mean_10_35= np.load(f'centroid_mean_10_35.npy')\n",
    "centroid_std_10_35= np.load(f'centroid_std_10_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_10 = np.zeros(10)\n",
    "std_dev_y_10 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_10[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_10[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_10[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_10[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_10)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_v2_n10 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_10[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_std_10_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x[cluster], mean_x + 2 * std_dev_x[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y[cluster], mean_y + 2 * std_dev_y[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std_v2_n10 = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_v2_n10.append([trial + 1, cluster, centroid_coord, inside_2_std_v2_n10])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_v2_n10 = pd.DataFrame(data_v2_n10, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true_v2_n10 = df_results_v2_n10.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true_v2_n10 = trials_all_true_v2_n10[trials_all_true_v2_n10].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true_v2_n10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false_v2_n10 = trials_all_true_v2_n10[~trials_all_true_v2_n10].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result table to a CSV file\n",
    "df_results_v2_n10.to_csv(f'result_table_neighbors_v2_{10}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal outliers process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array into a DataFrame with 'Cluster', 'x_mean', and 'y_mean'\n",
    "centroid_mean_neighbors_10_df = pd.DataFrame(centroid_mean_10_35, columns=['x_mean', 'y_mean'])\n",
    "centroid_mean_neighbors_10_df['Cluster'] = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y coordinates\n",
    "df_results_v2[['x', 'y']] = pd.DataFrame(df_results_v2['Centroid Coord'].tolist(), index=df_results_v2.index)\n",
    "\n",
    "# Merge the mean centroids dataframe with the results dataframe on 'Cluster'\n",
    "df_merged = pd.merge(df_results_v2, centroid_mean_neighbors_10_df, on='Cluster', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot changes in X-coordinate for each cluster over all runs\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_10[:, cluster, 0], marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Cluster {cluster} X-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('X Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot changes in Y-coordinate for each cluster over all runs\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_10[:, cluster, 1], marker='o', linestyle='-', color='g')\n",
    "    plt.title(f'Cluster {cluster} Y-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Y Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance from each centroid to its cluster's mean\n",
    "df_merged['Distance_to_Mean'] = np.sqrt((df_merged['x'] - df_merged['x_mean'])**2 + (df_merged['y'] - df_merged['y_mean'])**2)\n",
    "\n",
    "# Apply an outlier threshold (e.g., 90th percentile of the distance per cluster)\n",
    "def filter_outliers(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return df[df['Distance_to_Mean'] <= threshold]\n",
    "\n",
    "# Apply the filtering function for each cluster\n",
    "df_no_outliers = df_merged.groupby('Cluster').apply(filter_outliers).reset_index(drop=True)\n",
    "\n",
    "# Step 7: Drop unnecessary columns if needed (like 'x' and 'y' if only the distance matters)\n",
    "df_no_outliers_cleaned_10_35 = df_no_outliers.drop(columns=['x', 'y', 'x_mean', 'y_mean'])\n",
    "\n",
    "# Step 8: Check the size of the resulting dataframe\n",
    "print(f\"Original DataFrame size: {df_merged.shape}\")\n",
    "print(f\"DataFrame size after removing outliers: {df_no_outliers_cleaned.shape}\")\n",
    "\n",
    "# Display the final dataframe\n",
    "df_no_outliers_cleaned_10_35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by 'Cluster'\n",
    "clusters_grouped_10_35 = df_no_outliers_cleaned_10_35.groupby('Cluster')\n",
    "\n",
    "# Create a dictionary to store arrays for each cluster's centroids\n",
    "clusters_centroids_10_35 = {}\n",
    "\n",
    "# Loop through each group (cluster) and store the centroids in arrays\n",
    "for cluster, group in clusters_grouped_10_35:\n",
    "    # Extract centroids (x, y) as a NumPy array\n",
    "    centroids_array = np.array(group['Centroid Coord'].tolist())  # Assuming 'Centroid Coord' contains [x, y] pairs\n",
    "    clusters_centroids_10_35[cluster] = centroids_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the size of each cluster\n",
    "cluster_sizes_10_35 = {cluster: len(centroids) for cluster, centroids in clusters_centroids_10_35.items()}\n",
    "\n",
    "# Print the size of each cluster\n",
    "for cluster, size in cluster_sizes_10_35.items():\n",
    "    print(f\"Cluster {cluster} has {size} centroids considered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to verify that if it is fine to have all clusters with the same number of centroids after filtering out outliers. This must be due to:\n",
    "- The Distance Distributions are Likely Very Similar\n",
    "- Uniform Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each cluster and plot the distribution of distances\n",
    "for cluster, group in clusters_grouped:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(group['Distance_to_Mean'], bins=10, edgecolor='black')\n",
    "    plt.title(f'Cluster {cluster}: Distance to Mean Distribution')\n",
    "    plt.xlabel('Distance to Mean')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentile threshold per cluster check\n",
    "def check_percentiles(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return threshold\n",
    "\n",
    "# Function applied to each cluster and print the result\n",
    "for cluster, group in clusters_grouped:\n",
    "    threshold = check_percentiles(group)\n",
    "    print(f\"Cluster {cluster}: 90th percentile threshold = {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster, calculate the 70th percentile of distances and filter accordingly\n",
    "for cluster, group in clusters_grouped:\n",
    "    # Calculate the 70th percentile threshold for the current cluster\n",
    "    threshold = np.percentile(group['Distance_to_Mean'], 70)\n",
    "    \n",
    "    # Filter centroids based on the 70th percentile\n",
    "    filtered_group = group[group['Distance_to_Mean'] <= threshold]\n",
    "    \n",
    "    # Print the size of the group before and after filtering\n",
    "    print(f\"Cluster {cluster}: Original size = {len(group)}, Filtered size = {len(filtered_group)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance matrix n=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Mean Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance matrix**: elemnt d_{ij} has the distance between the center of cluster i and cluster j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store distance matrices for each run\n",
    "distance_matrices_10_35 = []\n",
    "\n",
    "# Iterate over all runs and calculate the distance matrix for each run\n",
    "for run_centroids in kmeans_centroids_10:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix_10_35 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    distance_matrices_10_35.append(distance_matrix_10_35)\n",
    "\n",
    "# Convert the list of distance matrices to a numpy array (35 runs, 10x10 distance matrices)\n",
    "distance_matrices_10_35 = np.array(distance_matrices_10_35)\n",
    "\n",
    "# Calculate the mean distance matrix across all runs\n",
    "mean_distance_matrix_10_35 = np.mean(distance_matrices_10_35, axis=0)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_10_35 = (mean_distance_matrix_10_35 - np.min(mean_distance_matrix_10_35)) / (np.max(mean_distance_matrix_10_35) - np.min(mean_distance_matrix_5_35))\n",
    "\n",
    "# Plot of the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=10)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_10_all_runs.npy', distance_matrices_10_35)\n",
    "np.save('mean_distance_matrix_neighbors_10_35.npy', mean_distance_matrix_10_35)\n",
    "\n",
    "# Mean distance matrix\n",
    "print(f\"Mean distance matrix across all runs:\\n{mean_distance_matrix_10_35}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_distance_matrix_10_35= np.load(f'mean_distance_matrix_neighbors_10_35.npy')\n",
    "# mean_distance_matrix_10_35=np.round(mean_distance_matrix_10_35,3)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_10_35 = (mean_distance_matrix_10_35 - np.min(mean_distance_matrix_10_35)) / (np.max(mean_distance_matrix_10_35) - np.min(mean_distance_matrix_10_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_10_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_10_35,3))\n",
    "np.save('G_10_35.npy',G_10_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_10_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_10_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_10_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_10_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum spanning tree\n",
    "\n",
    "# Compute the minimum spanning tree of the graph\n",
    "mst_10_35 = nx.minimum_spanning_tree(G_10_35)\n",
    "np.save('mst_10_35.npy',mst_10_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_10_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_10_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_10_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_10_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST - n_neighbors=10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Std. dev. Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the pairwise distance matrix for the standard deviations\n",
    "distance_matrix_std_10_35 = cdist(centroid_std_10_35, centroid_std_10_35, metric='euclidean')\n",
    "\n",
    "# Normalize the distance matrix\n",
    "normalized_distance_matrix_std_10_35 = (distance_matrix_std_10_35 - np.min(distance_matrix_std_10_35)) / (np.max(distance_matrix_std_10_35) - np.min(distance_matrix_std_10_35))\n",
    "\n",
    "# Visualize the normalized distance matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(normalized_distance_matrix_std_10_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=10)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrix for later analysis\n",
    "np.save(\"distance_matrix_std_10_35.npy\", distance_matrix_std_10_35)\n",
    "np.save(\"normalized_distance_matrix_std_10_35.npy\", normalized_distance_matrix_std_10_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_std_10_35 = nx.from_numpy_array(np.round(normalized_distance_matrix_std_10_35,3))\n",
    "np.save('G_std_10_35.npy', G_std_10_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_std_10_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_std_10_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_std_10_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_std_10_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_std_10_35 = nx.minimum_spanning_tree(G_std_10_35)\n",
    "np.save('mst_std_10_35.npy', mst_std_10_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos_std_10_35 = nx.spring_layout(mst_std_10_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_std_10_35, pos_std_10_35, with_labels=True, node_color='lightyellow', edge_color='green', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels_std_10_35 = nx.get_edge_attributes(mst_std_10_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_10_35, pos_std_10_35, edge_labels=edge_labels_std_10_35, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST Std. Deviation - n_neighbors=10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_10_35, \"MST - Mean Distances\", axes[0], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_10_35, \"MST - Lower Limit\", axes[1], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_10_35, \"MST - Upper Limit\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(matrix, title, xlabel, ylabel, figsize=(10, 8), cmap=\"viridis\", annot=True):\n",
    "    \"\"\"\n",
    "    Plots a heatmap for a given matrix with customizable parameters.\n",
    "\n",
    "    Args:\n",
    "        matrix (ndarray): The 2D matrix to plot as a heatmap.\n",
    "        title (str): Title of the heatmap.\n",
    "        xlabel (str): Label for the x-axis.\n",
    "        ylabel (str): Label for the y-axis.\n",
    "        figsize (tuple): Size of the figure (default: (10, 8)).\n",
    "        cmap (str): Color map to use (default: \"viridis\").\n",
    "        annot (bool): Whether to annotate cells with their values (default: True).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(matrix, annot=annot, cmap=cmap, fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function for both heatmaps\n",
    "plot_heatmap(\n",
    "    normalized_distance_matrix_std_10_35,\n",
    "    title=\"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=10)\",\n",
    "    xlabel=\"Cluster\",\n",
    "    ylabel=\"Cluster\",\n",
    "    figsize=(8, 6)\n",
    ")\n",
    "\n",
    "plot_heatmap(\n",
    "    normalized_mean_distance_matrix_10_35,\n",
    "    title=\"Normalized Mean Distance Matrix (k=10, n_neighbors=10)\",\n",
    "    xlabel=\"Cluster\",\n",
    "    ylabel=\"Cluster\",\n",
    "    figsize=(8, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create heatmaps\n",
    "def plot_heatmaps_side_by_side(matrices, titles, figsize=(16, 8), cmap=\"viridis\", annot=True):\n",
    "    \"\"\"\n",
    "    Plots multiple heatmaps side by side for given matrices and titles.\n",
    "\n",
    "    Args:\n",
    "        matrices (list): List of 2D matrices to plot as heatmaps.\n",
    "        titles (list): List of titles corresponding to each matrix.\n",
    "        figsize (tuple): Size of the entire figure (default: (16, 8)).\n",
    "        cmap (str): Color map to use for all heatmaps (default: \"viridis\").\n",
    "        annot (bool): Whether to annotate cells with their values (default: True).\n",
    "    \"\"\"\n",
    "    n = len(matrices)  # Number of heatmaps\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "\n",
    "    for i, (matrix, title) in enumerate(zip(matrices, titles)):\n",
    "        sns.heatmap(matrix, annot=annot, cmap=cmap, fmt=\".2f\", linewidths=0.5, ax=axes[i])\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_xlabel(\"Cluster\")\n",
    "        axes[i].set_ylabel(\"Cluster\" if i == 0 else \"\")  # Only label y-axis for the first plot\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the two heatmaps\n",
    "plot_heatmaps_side_by_side(\n",
    "    matrices=[\n",
    "        normalized_distance_matrix_std_10_35,\n",
    "        normalized_mean_distance_matrix_10_35\n",
    "    ],\n",
    "    titles=[\n",
    "        \"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=5)\",\n",
    "        \"Normalized Mean Distance Matrix (k=10, n_neighbors=5)\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for \"low\" values\n",
    "threshold = 0.65\n",
    "\n",
    "# Identify pairs of clusters with low values in both matrices\n",
    "low_low_pairs = []\n",
    "for i in range(normalized_mean_distance_matrix_10_35.shape[0]):\n",
    "    for j in range(normalized_mean_distance_matrix_10_35.shape[1]):\n",
    "        if i != j:  # Skip diagonal\n",
    "            mean_value = normalized_mean_distance_matrix_10_35[i, j]\n",
    "            std_value = normalized_distance_matrix_std_10_35[i, j]\n",
    "            if mean_value < threshold and std_value < threshold:\n",
    "                low_low_pairs.append((i, j, mean_value, std_value))\n",
    "\n",
    "# Display the results\n",
    "for pair in low_low_pairs:\n",
    "    print(f\"Clusters {pair[0]} and {pair[1]}: Mean Distance = {pair[2]:.2f}, Std Distance = {pair[3]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.65 can seem like a high value, since it is on the upper-mid range.\n",
    "\n",
    "Depending on the goal of the analysis we can think of it as:\n",
    "- If the aim is to identify the strongest relationships between clusters, a lower threshold would make more sense.\n",
    "- If we want to explore the broader connections, then it is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Replace with your cluster pairs from low_low_pairs\n",
    "low_low_pairs = [(0, 9, 0.64, 0.18), (0, 6, 0.64, 0.23),(7, 9, 0.62, 0.26)]  # Example cluster pairs\n",
    "\n",
    "# UMAP projections and cluster labels (replace with your actual data)\n",
    "umap_projections = np.load(\"umap_projections_neighbors_10.npy\")\n",
    "kmeans_labels = np.load(\"kmeans_labels_list_10_35.npy\")  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Function to plot clusters\n",
    "def plot_clusters(umap_projection, labels, cluster_pair, run_idx):\n",
    "    cluster_a, cluster_b = cluster_pair\n",
    "    points_a = umap_projection[labels == cluster_a]\n",
    "    points_b = umap_projection[labels == cluster_b]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(points_a[:, 0], points_a[:, 1], color=\"blue\", label=f\"Cluster {cluster_a}\", alpha=0.6)\n",
    "    plt.scatter(points_b[:, 0], points_b[:, 1], color=\"orange\", label=f\"Cluster {cluster_b}\", alpha=0.6)\n",
    "    plt.title(f\"Run {run_idx}: Cluster {cluster_a} vs. Cluster {cluster_b}\")\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze each cluster pair\n",
    "for cluster_pair in low_low_pairs:\n",
    "    cluster_a, cluster_b = cluster_pair[0], cluster_pair[1]\n",
    "    print(f\"Analyzing Cluster Pair: {cluster_a} and {cluster_b}\")\n",
    "    \n",
    "    # For simplicity, visualize them in a specific UMAP run (e.g., the first run)\n",
    "    run_idx = 0  # Use the first run for visualization\n",
    "    plot_clusters(umap_projections[run_idx], kmeans_labels[run_idx], (cluster_a, cluster_b), run_idx)\n",
    "\n",
    "    # Calculate additional statistics if needed\n",
    "    distances_a_to_b = np.linalg.norm(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a].mean(axis=0) - \n",
    "                                      umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b].mean(axis=0))\n",
    "    print(f\"Mean Centroid Distance (Run {run_idx}): {distances_a_to_b:.2f}\")\n",
    "\n",
    "    # Variability comparison\n",
    "    cluster_a_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a], axis=0)\n",
    "    cluster_b_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b], axis=0)\n",
    "    print(f\"Cluster {cluster_a} Std Dev: {cluster_a_std}\")\n",
    "    print(f\"Cluster {cluster_b} Std Dev: {cluster_b_std}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 1 and Cluster 8** have a moderate spatial relationship with visible overlap in the UMAP space. Their differing variability patterns suggest distinct structures, but the overlap points might represent shared features or transitions between the clusters.\n",
    "The large spatial separation between their centroids suggests they represent distinct structures or classes in the data.\n",
    "\n",
    "**Cluster 0 and Cluster 9** 9 appears more compact and stable, while Cluster 0 is larger and more variable.\n",
    "Their distinct regions in the UMAP space and differing standard deviations reinforce their meaningful separation.\n",
    "Insights from Variability:\n",
    "\n",
    "The variability of Cluster 0 could indicate sensitivity to UMAP parameters or noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_mnist_overlap(umap_projection, kmeans_labels, mnist_labels, cluster_pair, run_idx):\n",
    "    cluster_a, cluster_b = cluster_pair\n",
    "\n",
    "    # Get points in Cluster A and Cluster B\n",
    "    points_a_indices = np.where(kmeans_labels == cluster_a)[0]\n",
    "    points_b_indices = np.where(kmeans_labels == cluster_b)[0]\n",
    "\n",
    "    # Find the overlapping points (indices)\n",
    "    overlap_indices = np.intersect1d(points_a_indices, points_b_indices)\n",
    "\n",
    "    # Get the original labels of overlapping points\n",
    "    overlap_labels = np.array(mnist_labels)[overlap_indices]\n",
    "\n",
    "    # Analyze the original labels\n",
    "    overlap_label_counts = pd.Series(overlap_labels).value_counts()\n",
    "\n",
    "    # Display the overlap statistics\n",
    "    print(f\"Overlap between Cluster {cluster_a} and Cluster {cluster_b} (Run {run_idx}):\")\n",
    "    print(f\"Number of overlapping points: {len(overlap_indices)}\")\n",
    "    print(f\"Original label distribution of overlapping points:\\n{overlap_label_counts}\")\n",
    "\n",
    "    # Visualize the overlap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(umap_projection[points_a_indices, 0], umap_projection[points_a_indices, 1], color=\"blue\", label=f\"Cluster {cluster_a}\", alpha=0.5)\n",
    "    plt.scatter(umap_projection[points_b_indices, 0], umap_projection[points_b_indices, 1], color=\"orange\", label=f\"Cluster {cluster_b}\", alpha=0.5)\n",
    "    if len(overlap_indices) > 0:\n",
    "        plt.scatter(umap_projection[overlap_indices, 0], umap_projection[overlap_indices, 1], color=\"red\", label=\"Overlap\", alpha=0.7)\n",
    "    plt.title(f\"Cluster Overlap: Cluster {cluster_a} vs. Cluster {cluster_b} (Run {run_idx})\")\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Load your MNIST data\n",
    "dataloader = MnistDataloader(\n",
    "    training_images_filepath=\"train-images.idx3-ubyte\",\n",
    "    training_labels_filepath=\"train-labels.idx1-ubyte\",\n",
    "    test_images_filepath=\"t10k-images.idx3-ubyte\",\n",
    "    test_labels_filepath=\"t10k-labels.idx1-ubyte\"\n",
    ")\n",
    "\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = dataloader.load_data()\n",
    "\n",
    "# Flatten the training images for UMAP (if needed for alignment with projections)\n",
    "x_train_flattened = np.array([np.array(img).flatten() for img in x_train])\n",
    "\n",
    "# Example variables (replace these with your actual data)\n",
    "run_idx = 0  # Analyze the first UMAP run\n",
    "cluster_pair = (1, 8)  # Compare Cluster 1 and Cluster 8\n",
    "umap_projection = umap_projections[run_idx]  # UMAP projection for the given run\n",
    "kmeans_labels = kmeans_labels[run_idx]  # KMeans labels for the given run\n",
    "\n",
    "# Examine overlap\n",
    "examine_mnist_overlap(umap_projection, kmeans_labels, y_train, cluster_pair, run_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interval of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_10_35 = distance_matrix_std_10_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_10_35 = z_score * sem_matrix_10_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_10_35 = mean_distance_matrix_10_35 - margin_of_error_matrix_10_35\n",
    "upper_limit_intconf_matrix_10_35 = mean_distance_matrix_10_35 + margin_of_error_matrix_10_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_10_35 = np.maximum(lower_limit_intconf_matrix_10_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_10_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_10_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_10_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_10_35.npy', lower_limit_intconf_matrix_10_35)\n",
    "np.save('upper_limit_intconf_matrix_10_35.npy', upper_limit_intconf_matrix_10_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_10_35 = normalize_matrix(lower_limit_intconf_matrix_10_35)\n",
    "norm_upper_limit_intconf_matrix_10_35 = normalize_matrix(upper_limit_intconf_matrix_10_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_10_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=10)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=10)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_10_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=10)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intra class evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "radius = 0.5\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_10_35 = []\n",
    "neighbor_counts_10_35 = []\n",
    "kmeans_labels_list_10_35 = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_10):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_10_35 = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_10_35.append(kmeans_labels_10_35)\n",
    "    \n",
    "    run_max_distances_10_35 = []\n",
    "    run_neighbor_counts_10_35 = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_10_35 == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_10_35 = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_10_35.append(max_distance_10_35)\n",
    "        \n",
    "        # Calculate number of neighbors within the radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_10_35 = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_10_35.append(neighbors_within_radius_10_35)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_10_35.append(run_max_distances_10_35)\n",
    "    neighbor_counts_10_35.append(run_neighbor_counts_10_35)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_10_35 = np.array(max_distances_10_35)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_10_35 = np.array(neighbor_counts_10_35)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_10_35 = np.array(kmeans_labels_list_10_35)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_10_35.npy', max_distances_10_35)\n",
    "np.save('neighbor_counts_within_radius_10_35.npy', neighbor_counts_10_35)\n",
    "np.save('kmeans_labels_list_10_35 .npy', kmeans_labels_list_10_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_10_35)\n",
    "print(\"\\nNeighbor counts within radius for each run and each cluster:\\n\", neighbor_counts_10_35)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_distances_10_35_d= np.load(f'max_intra_cluster_distances_dynamic_10_35.npy')\n",
    "neighbor_counts_10_35_d= np.load(f'neighbor_counts_within_dynamic_radius_10_35.npy')\n",
    "kmeans_labels_list_10_35_d= np.load(f'kmeans_labels_list_10_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to track the minimum distance and corresponding clusters\n",
    "overall_min_distance_10 = float('inf')\n",
    "min_distance_clusters_10 = None\n",
    "min_distance_run_idx_10= None\n",
    "\n",
    "for run_idx, run_centroids in enumerate(kmeans_centroids_10):\n",
    "    # Compute pairwise distances between centroids\n",
    "    pairwise_distances_10 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    \n",
    "    # Get the indices of the minimum non-zero distance\n",
    "    np.fill_diagonal(pairwise_distances_10, np.inf)  # Ignore zero distances (self-comparisons)\n",
    "    min_distance = np.min(pairwise_distances_10)\n",
    "    if min_distance < overall_min_distance_10:\n",
    "        overall_min_distance_10 = min_distance\n",
    "        # Find the indices of the clusters corresponding to the minimum distance\n",
    "        cluster_indices = np.unravel_index(np.argmin(pairwise_distances_10), pairwise_distances_10.shape)\n",
    "        min_distance_clusters_10 = cluster_indices\n",
    "        min_distance_run_idx_10 = run_idx\n",
    "\n",
    "# Calculate dynamic radius\n",
    "dynamic_radius_10_35 = overall_min_distance_10 / 2\n",
    "print(f\"Dynamic radius: {dynamic_radius_10_35}\")\n",
    "print(f\"Minimum distance: {overall_min_distance_10}\")\n",
    "print(f\"Clusters contributing to minimum distance: {min_distance_clusters_10}\")\n",
    "print(f\"Run index: {min_distance_run_idx_10}\")\n",
    "\n",
    "# Save dynamic radius\n",
    "np.save('dynamic_radius_results_10_35.npy', dynamic_radius_10_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "\n",
    "# Dynamic radius, previously calculated\n",
    "radius = dynamic_radius_10_35\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_10_35_d = []\n",
    "neighbor_counts_10_35_d = []\n",
    "kmeans_labels_list_10_35_d = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_10):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_10_35_d = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_10_35_d.append(kmeans_labels_10_35_d)\n",
    "    \n",
    "    run_max_distances_10_35_d = []\n",
    "    run_neighbor_counts_10_35_d = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_10_35_d == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_10_35_d = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_10_35_d.append(max_distance_10_35_d)\n",
    "        \n",
    "        # Calculate number of neighbors within the dynamic radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_10_35_d = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_10_35_d.append(neighbors_within_radius_10_35_d)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_10_35_d.append(run_max_distances_10_35_d)\n",
    "    neighbor_counts_10_35_d.append(run_neighbor_counts_10_35_d)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_10_35_d = np.array(max_distances_10_35_d)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_10_35_d = np.array(neighbor_counts_10_35_d)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_10_35_d = np.array(kmeans_labels_list_10_35_d)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_dynamic_10_35.npy', max_distances_10_35_d)\n",
    "np.save('neighbor_counts_within_dynamic_radius_10_35.npy', neighbor_counts_10_35_d)\n",
    "np.save('kmeans_labels_list_10_35.npy', kmeans_labels_list_10_35_d)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_10_35_d)\n",
    "print(\"\\nNeighbor counts within dynamic radius for each run and each cluster:\\n\", neighbor_counts_10_35_d)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot neighbor counts for each cluster across all runs\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Iterate through each cluster\n",
    "for cluster_idx in range(neighbor_counts_10_35_d.shape[1]):\n",
    "    plt.subplot(2, 5, cluster_idx + 1)  # Create subplots for 10 clusters (2 rows, 5 columns)\n",
    "    plt.plot(range(1, neighbor_counts_10_35_d.shape[0] + 1), neighbor_counts_10_35_d[:, cluster_idx], marker='o')\n",
    "    plt.title(f'Cluster {cluster_idx}')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Neighbor Count')\n",
    "    plt.xticks(range(1, neighbor_counts_10_35_d.shape[0] + 1, 5))  # Show every 5th run on the x-axis for clarity\n",
    "    plt.grid(True)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.suptitle('N=10 Neighbor Counts per Cluster Across Runs', y=1.02, fontsize=16)  # Add a global title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and max values across clusters for each run\n",
    "mean_neighbors_10 = np.mean(neighbor_counts_10_35_d, axis=1)  # Shape: (n_runs,)\n",
    "max_neighbors_10 = np.max(neighbor_counts_10_35_d, axis=1)    # Shape: (n_runs,)\n",
    "\n",
    "# Compute trend lines for mean and max\n",
    "runs = np.arange(1, len(mean_neighbors_10) + 1)\n",
    "mean_slope, mean_intercept, _, _, _ = linregress(runs, mean_neighbors_10)\n",
    "max_slope, max_intercept, _, _, _ = linregress(runs, max_neighbors_10)\n",
    "\n",
    "# Calculate trend line values\n",
    "mean_trend_10 = mean_slope * runs + mean_intercept\n",
    "max_trend_10 = max_slope * runs + max_intercept\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Mean neighbor counts\n",
    "plt.plot(runs, mean_neighbors_10, label='Mean Neighbor Count', marker='o', color='blue')\n",
    "\n",
    "# Max neighbor counts\n",
    "plt.plot(runs, max_neighbors_10, label='Max Neighbor Count', marker='s', color='orange')\n",
    "\n",
    "# Trend lines\n",
    "plt.plot(runs, mean_trend_10, linestyle='--', color='green',label='Mean Trend Line')\n",
    "plt.plot(runs, max_trend_10, linestyle='--', color='green', label='Max Trend Line')\n",
    "\n",
    "# Add labels, legend, and title\n",
    "plt.title('N=10 Neighbor Counts Across Runs (Mean vs. Max with Trend Lines)', fontsize=16)\n",
    "plt.xlabel('Run', fontsize=12)\n",
    "plt.ylabel('Neighbor Count', fontsize=12)\n",
    "plt.xticks(range(1, len(mean_neighbors_10) + 1, 5))  # Show every 5th run for readability\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(f'neighbor_counts_plot_n_10_35.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_neighbours= 20, n_runs = 35, n_clusters = 10 (for KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the n_neighbors Analysis\n",
    "umap_projections_20 = np.load(f'umap_projections_neighbors_20.npy')\n",
    "centroid_mean_20_35= np.load(f'centroid_mean_20_35.npy')\n",
    "centroid_std_20_35= np.load(f'centroid_std_20_35.npy')\n",
    "kmeans_centroids_20 = np.load(f\"kmeans_centroids_neighbors_20.npy\")\n",
    "df_results_v2=pd.read_csv('result_table_neighbors_20_35.csv')\n",
    "mean_distance_matrix_20_35= np.load(f'mean_distance_matrix_neighbors_20_35.npy')\n",
    "distance_matrix_std_20_35= np.load(f\"distance_matrix_std_20_35.npy\")\n",
    "normalized_distance_matrix_std_20_35= np.load(f'normalized_distance_matrix_std_20_35.npy')\n",
    "normalized_mean_distance_matrix_20_35= np.load(f'normalized_mean_distance_matrix_20_35.npy')\n",
    "mst_std_20_35= np.load(f\"mst_std_20_35.npy\")\n",
    "mst_20_35= np.load(f\"mst_20_35.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of UMAP and KMeans runs\n",
    "n_runs = 35\n",
    "n_clusters = 10  # Set the number of clusters (for KMeans)\n",
    "n_neighbors = 20\n",
    "\n",
    "# Store UMAP and KMeans results for each run\n",
    "umap_projections_20_35 = []\n",
    "kmeans_centroids_list_20_35 = []  # Use this to store centroids for each run\n",
    "\n",
    "# Define a helper function to calculate the centroid of each cluster\n",
    "def calculate_centroids(kmeans, x_umap):\n",
    "    centroids = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = x_umap[kmeans.labels_ == i]\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        centroids.append(centroid)\n",
    "    return np.array(centroids)\n",
    "\n",
    "# Run UMAP and KMeans multiple times\n",
    "for run in range(n_runs):\n",
    "    # Apply UMAP with the same parameters for each run\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=0.1, n_components=2, random_state=None)  # No random_state to allow randomness, use random_state=None\n",
    "    x_train_umap = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Apply KMeans clustering on the UMAP projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_train_umap)\n",
    "\n",
    "    # Calculate centroids for this run\n",
    "    centroids = calculate_centroids(kmeans, x_train_umap)\n",
    "    \n",
    "    # Store the UMAP projections and KMeans models\n",
    "    umap_projections_20_35.append(x_train_umap)\n",
    "    kmeans_centroids_list_20_35.append(centroids)\n",
    "\n",
    "# Now we calculate the mean and standard deviation of the centroids across all runs\n",
    "kmeans_centroids_20_35 = np.array(kmeans_centroids_list_20_35)  \n",
    "\n",
    "# Calculate mean and std deviation for centroids' coordinates\n",
    "centroid_mean_20_35 = np.mean(kmeans_centroids_20_35, axis=0)\n",
    "centroid_std_20_35 = np.std(kmeans_centroids_20_35, axis=0)\n",
    "\n",
    "# Save the UMAP projections and KMeans centroids using NumPy\n",
    "np.save(f'umap_projections_neighbors_{n_neighbors}.npy', np.array(umap_projections_20_35))\n",
    "np.save(f'kmeans_centroids_neighbors_{n_neighbors}.npy', np.array(kmeans_centroids_list_20_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the UMAP projections\n",
    "umap_projections_20_35 = np.load(f'umap_projections_neighbors_20.npy')\n",
    "\n",
    "# To see the contents of the UMAP projections\n",
    "print(umap_projections_20_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "\n",
    "# Save the centroid_mean and centroid_std\n",
    "np.save(f'centroid_mean_{n_neighbors}_35.npy', np.array(centroid_mean_20_35))\n",
    "np.save(f'centroid_std_{n_neighbors}_35.npy', np.array(centroid_std_20_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_mean_20_35= np.load(f'centroid_mean_20_35.npy')\n",
    "centroid_std_20_35= np.load(f'centroid_std_20_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_centroids_20_35 = np.load(f\"kmeans_centroids_neighbors_20.npy\")  # Load the saved centroids data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_20_35 = np.zeros(10)\n",
    "std_dev_y_20_35 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_20_35[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_20_35[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_20_35[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_20_35[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_20_35)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_20_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_20_35 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_20_35[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_20_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x_20_35[cluster], mean_x + 2 * std_dev_x_20_35[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y_20_35[cluster], mean_y + 2 * std_dev_y_20_35[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_20_35.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_20_35 = pd.DataFrame(data_20_35, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true_20_35 = df_results_20_35.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true_20_35 = trials_all_true_20_35[trials_all_true_20_35].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true_20_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false_20_35 = trials_all_true_20_35[~trials_all_true_20_35].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false_20_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Save the result table to a CSV file\n",
    "df_results_20_35.to_csv(f'result_table_neighbors_v2_{20}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal outliers process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_20_35=pd.read_csv('result_table_neighbors_v2_20_35.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array into a DataFrame with 'Cluster', 'x_mean', and 'y_mean'\n",
    "centroid_mean_20_35_df = pd.DataFrame(centroid_mean_20_35, columns=['x_mean', 'y_mean'])\n",
    "centroid_mean_20_35_df['Cluster'] = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y coordinates\n",
    "df_results_20_35[['x', 'y']] = pd.DataFrame(df_results_20_35['Centroid Coord'].tolist(), index=df_results_20_35.index)\n",
    "\n",
    "# Merge the mean centroids dataframe with the results dataframe on 'Cluster'\n",
    "df_merged_20_35 = pd.merge(df_results_20_35, centroid_mean_20_35_df, on='Cluster', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot changes in X-coordinate for each cluster over all runs\n",
    "n_runs = 35\n",
    "n_clusters = 10\n",
    "n_neighbors = 5\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_20_35[:, cluster, 0], marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Cluster {cluster} X-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('X Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot changes in Y-coordinate for each cluster over all runs\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_20_35[:, cluster, 1], marker='o', linestyle='-', color='g')\n",
    "    plt.title(f'Cluster {cluster} Y-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Y Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance from each centroid to its cluster's mean\n",
    "df_merged_20_35['Distance_to_Mean'] = np.sqrt((df_merged_20_35['x'] - df_merged_20_35['x_mean'])**2 + (df_merged_20_35['y'] - df_merged_20_35['y_mean'])**2)\n",
    "\n",
    "# Apply an outlier threshold (e.g., 90th percentile of the distance per cluster)\n",
    "def filter_outliers(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return df[df['Distance_to_Mean'] <= threshold]\n",
    "\n",
    "# Apply the filtering function for each cluster\n",
    "df_no_outliers_20_35 = df_merged_20_35.groupby('Cluster').apply(filter_outliers).reset_index(drop=True)\n",
    "\n",
    "# Step 7: Drop unnecessary columns if needed (like 'x' and 'y' if only the distance matters)\n",
    "df_no_outliers_cleaned_20_35 = df_no_outliers_20_35.drop(columns=['x', 'y', 'x_mean', 'y_mean'])\n",
    "\n",
    "# Step 8: Check the size of the resulting dataframe\n",
    "print(f\"Original DataFrame size: {df_merged_20_35.shape}\")\n",
    "print(f\"DataFrame size after removing outliers: {df_no_outliers_cleaned_20_35.shape}\")\n",
    "\n",
    "# Display the final dataframe to the user\n",
    "df_no_outliers_cleaned_20_35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by 'Cluster'\n",
    "clusters_grouped_20_35 = df_no_outliers_cleaned_20_35.groupby('Cluster')\n",
    "\n",
    "# Create a dictionary to store arrays for each cluster's centroids\n",
    "clusters_centroids_20_35 = {}\n",
    "\n",
    "# Loop through each group (cluster) and store the centroids in arrays\n",
    "for cluster, group in clusters_grouped_20_35:\n",
    "    # Extract centroids (x, y) as a NumPy array\n",
    "    centroids_array = np.array(group['Centroid Coord'].tolist())  # Assuming 'Centroid Coord' contains [x, y] pairs\n",
    "    clusters_centroids_20_35[cluster] = centroids_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the size of each cluster\n",
    "cluster_sizes_20_35 = {cluster: len(centroids) for cluster, centroids in clusters_centroids_20_35.items()}\n",
    "\n",
    "# Print the size of each cluster\n",
    "for cluster, size in cluster_sizes_20_35.items():\n",
    "    print(f\"Cluster {cluster} has {size} centroids considered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to verify that if it is fine to have all clusters with the same number of centroids after filtering out outliers. This must be due to:\n",
    "- The Distance Distributions are Likely Very Similar\n",
    "- Uniform Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each cluster and plot the distribution of distances\n",
    "for cluster, group in clusters_grouped_20_35:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(group['Distance_to_Mean'], bins=10, edgecolor='black')\n",
    "    plt.title(f'Cluster {cluster}: Distance to Mean Distribution')\n",
    "    plt.xlabel('Distance to Mean')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentile threshold per cluster check\n",
    "def check_percentiles(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return threshold\n",
    "\n",
    "# Function applied to each cluster and print the result\n",
    "for cluster, group in clusters_grouped_20_35:\n",
    "    threshold = check_percentiles(group)\n",
    "    print(f\"Cluster {cluster}: 90th percentile threshold = {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster, calculate the 70th percentile of distances and filter accordingly\n",
    "for cluster, group in clusters_grouped_20_35:\n",
    "    # Calculate the 70th percentile threshold for the current cluster\n",
    "    threshold = np.percentile(group['Distance_to_Mean'], 70)\n",
    "    \n",
    "    # Filter centroids based on the 70th percentile\n",
    "    filtered_group = group[group['Distance_to_Mean'] <= threshold]\n",
    "    \n",
    "    # Print the size of the group before and after filtering\n",
    "    print(f\"Cluster {cluster}: Original size = {len(group)}, Filtered size = {len(filtered_group)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Distance matrix n=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Mean matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance matrix**: elemnt d_{ij} has the distance between the center of cluster i and cluster j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store distance matrices for each run\n",
    "distance_matrices_20_35 = []\n",
    "\n",
    "# Iterate over all runs and calculate the distance matrix for each run\n",
    "for run_centroids in kmeans_centroids_20_35:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    dist_matrix = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    distance_matrices_20_35.append(dist_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a numpy array (35 runs, 10x10 distance matrices)\n",
    "distance_matrices_20_35 = np.array(distance_matrices_20_35)\n",
    "\n",
    "# Calculate the mean distance matrix across all runs\n",
    "mean_distance_matrix_20_35 = np.mean(distance_matrices_20_35, axis=0)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_20_35 = (mean_distance_matrix_20_35 - np.min(mean_distance_matrix_20_35)) / (np.max(mean_distance_matrix_20_35) - np.min(mean_distance_matrix_20_35))\n",
    "\n",
    "# Plot of the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_20_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=20)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_20_all_runs.npy', distance_matrices_20_35)\n",
    "np.save('mean_distance_matrix_neighbors_20_35.npy', mean_distance_matrix_20_35)\n",
    "\n",
    "# Mean distance matrix\n",
    "print(f\"Mean distance matrix across all runs:\\n{mean_distance_matrix_20_35}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_distance_matrix_20_35= np.load(f'mean_distance_matrix_neighbors_20_35.npy')\n",
    "# mean_distance_matrix_20_35=np.round(mean_distance_matrix_20_35,3)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_20_35 = (mean_distance_matrix_20_35 - np.min(mean_distance_matrix_20_35)) / (np.max(mean_distance_matrix_20_35) - np.min(mean_distance_matrix_20_35))\n",
    "np.save('normalized_mean_distance_matrix_20_35.npy', normalized_mean_distance_matrix_20_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_20_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_20_35,3))\n",
    "np.save('G_20_35.npy', G_20_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_20_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_20_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_20_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_20_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum spanning tree\n",
    "\n",
    "# Compute the minimum spanning tree of the graph\n",
    "mst_20_35 = nx.minimum_spanning_tree(G_20_35)\n",
    "np.save(\"mst_20_35.npy\",mst_20_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_20_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_20_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_20_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_20_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST - n_neighbors=20\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance Std. dev. Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Std. dev. Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the pairwise distance matrix for the standard deviations\n",
    "distance_matrix_std_20_35 = cdist(centroid_std_20_35, centroid_std_20_35, metric='euclidean')\n",
    "\n",
    "# Normalize the distance matrix\n",
    "normalized_distance_matrix_std_20_35 = (distance_matrix_std_20_35 - np.min(distance_matrix_std_20_35)) / (np.max(distance_matrix_std_20_35) - np.min(distance_matrix_std_20_35))\n",
    "\n",
    "# Visualize the normalized distance matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(normalized_distance_matrix_std_20_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=20)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrix for later analysis\n",
    "np.save(\"distance_matrix_std_20_35.npy\", distance_matrix_std_20_35)\n",
    "np.save(\"normalized_distance_matrix_std_20_35.npy\", normalized_distance_matrix_std_20_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_std_20_35 = nx.from_numpy_array(np.round(normalized_distance_matrix_std_20_35,3))\n",
    "np.save('G_std_20_35.npy',G_std_20_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_std_20_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_std_20_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_std_20_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_std_20_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_std_20_35 = nx.minimum_spanning_tree(G_std_20_35)\n",
    "np.save('mst_std_20_35.npy',G_std_20_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos_std_20_35 = nx.spring_layout(mst_std_20_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_std_20_35, pos_std_20_35, with_labels=True, node_color='lightyellow', edge_color='green', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels_std_20_35 = nx.get_edge_attributes(mst_std_20_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_20_35, pos_std_20_35, edge_labels=edge_labels_std_20_35, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST Std. Deviation - n_neighbors=20\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create heatmaps\n",
    "def plot_heatmaps_side_by_side(matrices, titles, figsize=(16, 8), cmap=\"viridis\", annot=True):\n",
    "    \"\"\"\n",
    "    Plots multiple heatmaps side by side for given matrices and titles.\n",
    "\n",
    "    Args:\n",
    "        matrices (list): List of 2D matrices to plot as heatmaps.\n",
    "        titles (list): List of titles corresponding to each matrix.\n",
    "        figsize (tuple): Size of the entire figure (default: (16, 8)).\n",
    "        cmap (str): Color map to use for all heatmaps (default: \"viridis\").\n",
    "        annot (bool): Whether to annotate cells with their values (default: True).\n",
    "    \"\"\"\n",
    "    n = len(matrices)  # Number of heatmaps\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "\n",
    "    for i, (matrix, title) in enumerate(zip(matrices, titles)):\n",
    "        sns.heatmap(matrix, annot=annot, cmap=cmap, fmt=\".2f\", linewidths=0.5, ax=axes[i])\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_xlabel(\"Cluster\")\n",
    "        axes[i].set_ylabel(\"Cluster\" if i == 0 else \"\")  # Only label y-axis for the first plot\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the two heatmaps\n",
    "plot_heatmaps_side_by_side(\n",
    "    matrices=[\n",
    "        normalized_distance_matrix_std_20_35,\n",
    "        normalized_mean_distance_matrix_20_35\n",
    "    ],\n",
    "    titles=[\n",
    "        \"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=20)\",\n",
    "        \"Normalized Mean Distance Matrix (k=10, n_neighbors=20)\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for \"low\" values\n",
    "threshold = 0.6\n",
    "\n",
    "# Identify pairs of clusters with low values in both matrices\n",
    "low_low_pairs = []\n",
    "for i in range(normalized_mean_distance_matrix_20_35.shape[0]):\n",
    "    for j in range(normalized_mean_distance_matrix_20_35.shape[1]):\n",
    "        if i != j:  # Skip diagonal\n",
    "            mean_value = normalized_mean_distance_matrix_20_35[i, j]\n",
    "            std_value = normalized_distance_matrix_std_20_35[i, j]\n",
    "            if mean_value < threshold and std_value < threshold:\n",
    "                low_low_pairs.append((i, j, mean_value, std_value))\n",
    "\n",
    "# Display the results\n",
    "for pair in low_low_pairs:\n",
    "    print(f\"Clusters {pair[0]} and {pair[1]}: Mean Distance = {pair[2]:.2f}, Std Distance = {pair[3]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.6 is the lowest threshold so far.\n",
    "\n",
    "Depending on the goal of the analysis we can think of it as:\n",
    "- If the aim is to identify the strongest relationships between clusters, a lower threshold would make more sense.\n",
    "- If we want to explore the broader connections, then it is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Replace with your cluster pairs from low_low_pairs\n",
    "low_low_pairs = [(0, 4, 0.58, 0.57), (3, 8, 0.58, 0.59), (6, 8, 0.60, 0.35), (7, 8, 0.60, 0.33), (7, 9, 0.58, 0.37)]\n",
    "\n",
    "# UMAP projections and cluster labels (replace with your actual data)\n",
    "umap_projections = np.load(\"umap_projections_neighbors_20.npy\")\n",
    "kmeans_labels = np.load(\"kmeans_labels_list_20_35.npy\")  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Function to plot clusters\n",
    "def plot_clusters(umap_projection, labels, cluster_pair, run_idx):\n",
    "    cluster_a, cluster_b = cluster_pair\n",
    "    points_a = umap_projection[labels == cluster_a]\n",
    "    points_b = umap_projection[labels == cluster_b]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(points_a[:, 0], points_a[:, 1], color=\"blue\", label=f\"Cluster {cluster_a}\", alpha=0.6)\n",
    "    plt.scatter(points_b[:, 0], points_b[:, 1], color=\"orange\", label=f\"Cluster {cluster_b}\", alpha=0.6)\n",
    "    plt.title(f\"Run {run_idx}: Cluster {cluster_a} vs. Cluster {cluster_b}\")\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze each cluster pair\n",
    "for cluster_pair in low_low_pairs:\n",
    "    cluster_a, cluster_b = cluster_pair[0], cluster_pair[1]\n",
    "    print(f\"Analyzing Cluster Pair: {cluster_a} and {cluster_b}\")\n",
    "    \n",
    "    # For simplicity, visualize them in a specific UMAP run (e.g., the first run)\n",
    "    run_idx = 0  # Use the first run for visualization\n",
    "    plot_clusters(umap_projections[run_idx], kmeans_labels[run_idx], (cluster_a, cluster_b), run_idx)\n",
    "\n",
    "    # Calculate additional statistics if needed\n",
    "    distances_a_to_b = np.linalg.norm(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a].mean(axis=0) - \n",
    "                                      umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b].mean(axis=0))\n",
    "    print(f\"Mean Centroid Distance (Run {run_idx}): {distances_a_to_b:.2f}\")\n",
    "\n",
    "    # Variability comparison\n",
    "    cluster_a_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a], axis=0)\n",
    "    cluster_b_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b], axis=0)\n",
    "    print(f\"Cluster {cluster_a} Std Dev: {cluster_a_std}\")\n",
    "    print(f\"Cluster {cluster_b} Std Dev: {cluster_b_std}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 1 and Cluster 8** have a moderate spatial relationship with visible overlap in the UMAP space. Their differing variability patterns suggest distinct structures, but the overlap points might represent shared features or transitions between the clusters.\n",
    "The large spatial separation between their centroids suggests they represent distinct structures or classes in the data.\n",
    "\n",
    "**Cluster 0 and Cluster 9** 9 appears more compact and stable, while Cluster 0 is larger and more variable.\n",
    "Their distinct regions in the UMAP space and differing standard deviations reinforce their meaningful separation.\n",
    "Insights from Variability:\n",
    "\n",
    "The variability of Cluster 0 could indicate sensitivity to UMAP parameters or noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interval of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_20_35 = distance_matrix_std_20_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_20_35 = z_score * sem_matrix_20_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_20_35 = mean_distance_matrix_20_35 - margin_of_error_matrix_20_35\n",
    "upper_limit_intconf_matrix_20_35 = mean_distance_matrix_20_35 + margin_of_error_matrix_20_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_20_35 = np.maximum(lower_limit_intconf_matrix_20_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_20_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_20_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_20_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_20_35.npy', lower_limit_intconf_matrix_20_35)\n",
    "np.save('upper_limit_intconf_matrix_20_35.npy', upper_limit_intconf_matrix_20_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_20_35 = normalize_matrix(lower_limit_intconf_matrix_20_35)\n",
    "norm_upper_limit_intconf_matrix_20_35 = normalize_matrix(upper_limit_intconf_matrix_20_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_20_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=20)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_20_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=20)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_20_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=20)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_20_35, \"MST - Mean Distances\", axes[0], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_20_35, \"MST - Lower Limit\", axes[1], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_20_35, \"MST - Upper Limit\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intra class evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "radius = 0.5\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_20_35 = []\n",
    "neighbor_counts_20_35 = []\n",
    "kmeans_labels_list_20_35 = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_20_35):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_20_35 = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_20_35.append(kmeans_labels_20_35)\n",
    "    \n",
    "    run_max_distances_20_35 = []\n",
    "    run_neighbor_counts_20_35 = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_20_35 == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_20_35 = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_20_35.append(max_distance_20_35)\n",
    "        \n",
    "        # Calculate number of neighbors within the radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_20_35 = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_20_35.append(neighbors_within_radius_20_35)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_20_35.append(run_max_distances_20_35)\n",
    "    neighbor_counts_20_35.append(run_neighbor_counts_20_35)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_20_35 = np.array(max_distances_20_35)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_20_35 = np.array(neighbor_counts_20_35)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_20_35 = np.array(kmeans_labels_list_20_35)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_5_35.npy', max_distances_20_35)\n",
    "np.save('neighbor_counts_within_radius_5_35.npy', neighbor_counts_20_35)\n",
    "np.save('kmeans_labels_list_5_35 .npy', kmeans_labels_list_20_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_20_35)\n",
    "print(\"\\nNeighbor counts within radius for each run and each cluster:\\n\", neighbor_counts_20_35)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_distances_20_35_d= np.load(f'max_intra_cluster_distances_dynamic_20_35.npy')\n",
    "neighbor_counts_20_35_d= np.load(f'neighbor_counts_within_dynamic_radius_20_35.npy')\n",
    "kmeans_labels_list_20_35_d= np.load(f'kmeans_labels_list_20_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to track the minimum distance and corresponding clusters\n",
    "overall_min_distance_20 = float('inf')\n",
    "min_distance_clusters_20 = None\n",
    "min_distance_run_idx_20= None\n",
    "\n",
    "for run_idx, run_centroids in enumerate(kmeans_centroids_20_35):\n",
    "    # Compute pairwise distances between centroids\n",
    "    pairwise_distances_20 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    \n",
    "    # Get the indices of the minimum non-zero distance\n",
    "    np.fill_diagonal(pairwise_distances_20, np.inf)  # Ignore zero distances (self-comparisons)\n",
    "    min_distance = np.min(pairwise_distances_20)\n",
    "    if min_distance < overall_min_distance_20:\n",
    "        overall_min_distance_20 = min_distance\n",
    "        # Find the indices of the clusters corresponding to the minimum distance\n",
    "        cluster_indices = np.unravel_index(np.argmin(pairwise_distances_20), pairwise_distances_20.shape)\n",
    "        min_distance_clusters_20 = cluster_indices\n",
    "        min_distance_run_idx_20 = run_idx\n",
    "\n",
    "# Calculate dynamic radius\n",
    "dynamic_radius_20_35 = overall_min_distance_20 / 2\n",
    "print(f\"Dynamic radius: {dynamic_radius_20_35}\")\n",
    "print(f\"Minimum distance: {overall_min_distance_20}\")\n",
    "print(f\"Clusters contributing to minimum distance: {min_distance_clusters_20}\")\n",
    "print(f\"Run index: {min_distance_run_idx_20}\")\n",
    "\n",
    "# Save dynamic radius\n",
    "np.save('dynamic_radius_results_20_35.npy', dynamic_radius_20_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "\n",
    "# Dynamic radius, previously calculated\n",
    "radius = dynamic_radius_20_35\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_20_35_d = []\n",
    "neighbor_counts_20_35_d = []\n",
    "kmeans_labels_list_20_35_d = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_20_35):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_20_35_d = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_20_35_d.append(kmeans_labels_20_35_d)\n",
    "    \n",
    "    run_max_distances_20_35_d = []\n",
    "    run_neighbor_counts_20_35_d = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_20_35_d == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_20_35_d = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_20_35_d.append(max_distance_20_35_d)\n",
    "        \n",
    "        # Calculate number of neighbors within the dynamic radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_20_35_d = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_20_35_d.append(neighbors_within_radius_20_35_d)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_20_35_d.append(run_max_distances_20_35_d)\n",
    "    neighbor_counts_20_35_d.append(run_neighbor_counts_20_35_d)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_20_35_d = np.array(max_distances_20_35_d)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_20_35_d = np.array(neighbor_counts_20_35_d)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_20_35_d = np.array(kmeans_labels_list_20_35_d)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_dynamic_20_35.npy', max_distances_20_35_d)\n",
    "np.save('neighbor_counts_within_dynamic_radius_20_35.npy', neighbor_counts_20_35_d)\n",
    "np.save('kmeans_labels_list_20_35.npy', kmeans_labels_list_20_35_d)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_20_35_d)\n",
    "print(\"\\nNeighbor counts within dynamic radius for each run and each cluster:\\n\", neighbor_counts_20_35_d)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot neighbor counts for each cluster across all runs\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Iterate through each cluster\n",
    "for cluster_idx in range(neighbor_counts_20_35_d.shape[1]):\n",
    "    plt.subplot(2, 5, cluster_idx + 1)  # Create subplots for 10 clusters (2 rows, 5 columns)\n",
    "    plt.plot(range(1, neighbor_counts_20_35_d.shape[0] + 1), neighbor_counts_20_35_d[:, cluster_idx], marker='o')\n",
    "    plt.title(f'Cluster {cluster_idx}')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Neighbor Count')\n",
    "    plt.xticks(range(1, neighbor_counts_20_35_d.shape[0] + 1, 5))  # Show every 5th run on the x-axis for clarity\n",
    "    plt.grid(True)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.suptitle('N=20 Neighbor Counts per Cluster Across Runs', y=1.02, fontsize=16)  # Add a global title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and max values across clusters for each run\n",
    "mean_neighbors_20 = np.mean(neighbor_counts_20_35_d, axis=1)  # Shape: (n_runs,)\n",
    "max_neighbors_20 = np.max(neighbor_counts_20_35_d, axis=1)    # Shape: (n_runs,)\n",
    "\n",
    "# Compute trend lines for mean and max\n",
    "runs = np.arange(1, len(mean_neighbors_20) + 1)\n",
    "mean_slope, mean_intercept, _, _, _ = linregress(runs, mean_neighbors_20)\n",
    "max_slope, max_intercept, _, _, _ = linregress(runs, max_neighbors_20)\n",
    "\n",
    "# Calculate trend line values\n",
    "mean_trend_20 = mean_slope * runs + mean_intercept\n",
    "max_trend_20 = max_slope * runs + max_intercept\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Mean neighbor counts\n",
    "plt.plot(runs, mean_neighbors_20, label='Mean Neighbor Count', marker='o', color='blue')\n",
    "\n",
    "# Max neighbor counts\n",
    "plt.plot(runs, max_neighbors_20, label='Max Neighbor Count', marker='s', color='orange')\n",
    "\n",
    "# Trend lines\n",
    "plt.plot(runs, mean_trend_20, linestyle='--', color='green',label='Mean Trend Line')\n",
    "plt.plot(runs, max_trend_20, linestyle='--', color='green', label='Max Trend Line')\n",
    "\n",
    "# Add labels, legend, and title\n",
    "plt.title('N=20 Neighbor Counts Across Runs (Mean vs. Max with Trend Lines)', fontsize=16)\n",
    "plt.xlabel('Run', fontsize=12)\n",
    "plt.ylabel('Neighbor Count', fontsize=12)\n",
    "plt.xticks(range(1, len(mean_neighbors_20) + 1, 5))  # Show every 5th run for readability\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(f'neighbor_counts_plot_n_20_35.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_neighbors = 30, n_runs = 35, n_clusters = 10 (for KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the n_neighbors Analysis\n",
    "umap_projections_30 = np.load(f'umap_projections_neighbors_30.npy')\n",
    "centroid_mean_30_35= np.load(f'centroid_mean_30_35.npy')\n",
    "centroid_std_30_35= np.load(f'centroid_std_30_35.npy')\n",
    "kmeans_centroids_30 = np.load(f\"kmeans_centroids_neighbors_30.npy\")\n",
    "df_results_v2=pd.read_csv(f'result_table_neighbors_30_35.csv')\n",
    "mean_distance_matrix_30_35= np.load(f'mean_distance_matrix_neighbors_30_35.npy')\n",
    "distance_matrix_std_30_35= np.load(f\"distance_matrix_std_30_35.npy\")\n",
    "normalized_distance_matrix_std_30_35= np.load(f'normalized_distance_matrix_std_30_35.npy')\n",
    "normalized_mean_distance_matrix_30_35= np.load(f'normalized_mean_distance_matrix_neighbors_30_35.npy')\n",
    "mst_std_30_35= np.load(f'mst_std_30_35.npy')\n",
    "mst_30_35= np.load(f'mst_30_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "\n",
    "# Define the number of UMAP and KMeans runs\n",
    "n_runs = 35\n",
    "n_clusters = 10  # Set the number of clusters (for KMeans)\n",
    "n_neighbors = 30\n",
    "\n",
    "# Store UMAP and KMeans results for each run\n",
    "umap_projections_30_35 = []\n",
    "kmeans_centroids_list_30_35 = []  # Use this to store centroids for each run\n",
    "\n",
    "# Define a helper function to calculate the centroid of each cluster\n",
    "def calculate_centroids(kmeans, x_umap):\n",
    "    centroids = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = x_umap[kmeans.labels_ == i]\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        centroids.append(centroid)\n",
    "    return np.array(centroids)\n",
    "\n",
    "# Run UMAP and KMeans multiple times\n",
    "for run in range(n_runs):\n",
    "    # Apply UMAP with the same parameters for each run\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=0.1, n_components=2, random_state=None)  # No random_state to allow randomness, use random_state=None\n",
    "    x_train_umap = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Apply KMeans clustering on the UMAP projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_train_umap)\n",
    "\n",
    "    # Calculate centroids for this run\n",
    "    centroids = calculate_centroids(kmeans, x_train_umap)\n",
    "    \n",
    "    # Store the UMAP projections and KMeans models\n",
    "    umap_projections_30_35.append(x_train_umap)\n",
    "    kmeans_centroids_list_30_35.append(centroids)\n",
    "\n",
    "# Now we calculate the mean and standard deviation of the centroids across all runs\n",
    "kmeans_centroids = np.array(kmeans_centroids_list_30_35)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "# Calculate mean and std deviation for centroids' coordinates\n",
    "centroid_mean = np.mean(kmeans_centroids, axis=0)\n",
    "centroid_std = np.std(kmeans_centroids, axis=0)\n",
    "\n",
    "# Save the UMAP projections and KMeans centroids\n",
    "np.save(f'umap_projections_neighbors_{n_neighbors}.npy', np.array(umap_projections_30_35))\n",
    "np.save(f'kmeans_centroids_neighbors_{n_neighbors}.npy', np.array(kmeans_centroids_list_30_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the UMAP projections\n",
    "umap_projections_30_35 = np.load(f'umap_projections_neighbors_30.npy')\n",
    "\n",
    "# To see the contents of the UMAP projections\n",
    "print(umap_projections_30_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "\n",
    "# Save the centroid_mean and centroid_std\n",
    "np.save(f'centroid_mean_{n_neighbors}_35.npy', np.array(centroid_mean))\n",
    "np.save(f'centroid_std_{n_neighbors}_35.npy', np.array(centroid_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_mean_30_35= np.load(f'centroid_mean_30_35.npy')\n",
    "centroid_std_30_35= np.load(f'centroid_std_30_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_centroids_30_35 = np.load(f\"kmeans_centroids_neighbors_30.npy\")  # Load the saved centroids data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_30_35 = np.zeros(10)\n",
    "std_dev_y_30_35 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_30_35[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_30_35[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_30_35[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_30_35[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_30_35)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_30_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_30_35 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_30_35[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_30_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x_30_35[cluster], mean_x + 2 * std_dev_x_30_35[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y_30_35[cluster], mean_y + 2 * std_dev_y_30_35[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_30_35.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_30_35 = pd.DataFrame(data_30_35, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true_30_35 = df_results_30_35.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true_30_35 = trials_all_true_30_35[trials_all_true_30_35].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true_30_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false_30_35 = trials_all_true_30_35[~trials_all_true_30_35].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false_30_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Save the result table to a CSV file\n",
    "df_results_30_35.to_csv(f'result_table_neighbors_30_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal outliers process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_30_35=pd.read_csv('result_table_neighbors_30_35.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array into a DataFrame with 'Cluster', 'x_mean', and 'y_mean'\n",
    "centroid_mean_neighbors_30_35_df = pd.DataFrame(centroid_mean_30_35, columns=['x_mean', 'y_mean'])\n",
    "centroid_mean_neighbors_30_35_df['Cluster'] = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Add commas between numbers in 'Centroid Coord' entries if they are missing\n",
    "df_results_30_35['Centroid Coord'] = df_results_30_35['Centroid Coord'].str.replace(\n",
    "    r'(\\-?\\d+\\.\\d+)\\s+(\\-?\\d+\\.\\d+)', r'\\1, \\2', regex=True\n",
    ")\n",
    "\n",
    "# Step 2: Convert 'Centroid Coord' from string to list\n",
    "df_results_30_35['Centroid Coord'] = df_results_30_35['Centroid Coord'].apply(ast.literal_eval)\n",
    "\n",
    "# Step 3: Verify if each entry in 'Centroid Coord' is a list of length 2\n",
    "invalid_rows = df_results_30_35[df_results_30_35['Centroid Coord'].apply(lambda x: not (isinstance(x, list) and len(x) == 2))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y coordinates\n",
    "df_results_30_35[['x', 'y']] = pd.DataFrame(df_results_30_35['Centroid Coord'].tolist(), index=df_results_30_35.index)\n",
    "\n",
    "# Merge the mean centroids dataframe with the results dataframe on 'Cluster'\n",
    "df_merged_30_35 = pd.merge(df_results_30_35, centroid_mean_neighbors_30_35_df, on='Cluster', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot changes in X-coordinate for each cluster over all runs\n",
    "n_runs = 35\n",
    "n_clusters = 10\n",
    "n_neighbors = 30\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_30_35[:, cluster, 0], marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Cluster {cluster} X-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('X Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot changes in Y-coordinate for each cluster over all runs\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_30_35[:, cluster, 1], marker='o', linestyle='-', color='g')\n",
    "    plt.title(f'Cluster {cluster} Y-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Y Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance from each centroid to its cluster's mean\n",
    "df_merged_30_35['Distance_to_Mean'] = np.sqrt((df_merged_30_35['x'] - df_merged_30_35['x_mean'])**2 + (df_merged_30_35['y'] - df_merged_30_35['y_mean'])**2)\n",
    "\n",
    "# Apply an outlier threshold (e.g., 90th percentile of the distance per cluster)\n",
    "def filter_outliers(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return df[df['Distance_to_Mean'] <= threshold]\n",
    "\n",
    "# Apply the filtering function for each cluster\n",
    "df_no_outliers = df_merged_30_35.groupby('Cluster').apply(filter_outliers).reset_index(drop=True)\n",
    "\n",
    "# Step 7: Drop unnecessary columns if needed (like 'x' and 'y' if only the distance matters)\n",
    "df_no_outliers_cleaned_30_35 = df_no_outliers.drop(columns=['x', 'y', 'x_mean', 'y_mean'])\n",
    "\n",
    "# Step 8: Check the size of the resulting dataframe\n",
    "print(f\"Original DataFrame size: {df_merged_30_35.shape}\")\n",
    "print(f\"DataFrame size after removing outliers: {df_no_outliers_cleaned_30_35.shape}\")\n",
    "\n",
    "# Display the final dataframe to the user\n",
    "df_no_outliers_cleaned_30_35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by 'Cluster'\n",
    "clusters_grouped_30_35 = df_no_outliers_cleaned_30_35.groupby('Cluster')\n",
    "\n",
    "# Create a dictionary to store arrays for each cluster's centroids\n",
    "clusters_centroids_30_35 = {}\n",
    "\n",
    "# Loop through each group (cluster) and store the centroids in arrays\n",
    "for cluster, group in clusters_grouped_30_35:\n",
    "    # Extract centroids (x, y) as a NumPy array\n",
    "    centroids_array = np.array(group['Centroid Coord'].tolist())  # Assuming 'Centroid Coord' contains [x, y] pairs\n",
    "    clusters_centroids_30_35[cluster] = centroids_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the size of each cluster\n",
    "cluster_sizes = {cluster: len(centroids) for cluster, centroids in clusters_centroids_30_35.items()}\n",
    "\n",
    "# Print the size of each cluster\n",
    "for cluster, size in cluster_sizes.items():\n",
    "    print(f\"Cluster {cluster} has {size} centroids considered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to verify that if it is fine to have all clusters with the same number of centroids after filtering out outliers. This must be due to:\n",
    "- The Distance Distributions are Likely Very Similar\n",
    "- Uniform Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each cluster and plot the distribution of distances\n",
    "for cluster, group in clusters_grouped_30_35:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(group['Distance_to_Mean'], bins=10, edgecolor='black')\n",
    "    plt.title(f'Cluster {cluster}: Distance to Mean Distribution')\n",
    "    plt.xlabel('Distance to Mean')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentile threshold per cluster check\n",
    "def check_percentiles(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return threshold\n",
    "\n",
    "# Function applied to each cluster and print the result\n",
    "for cluster, group in clusters_grouped_30_35:\n",
    "    threshold = check_percentiles(group)\n",
    "    print(f\"Cluster {cluster}: 90th percentile threshold = {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster, calculate the 70th percentile of distances and filter accordingly\n",
    "for cluster, group in clusters_grouped_30_35:\n",
    "    # Calculate the 70th percentile threshold for the current cluster\n",
    "    threshold = np.percentile(group['Distance_to_Mean'], 70)\n",
    "    \n",
    "    # Filter centroids based on the 70th percentile\n",
    "    filtered_group = group[group['Distance_to_Mean'] <= threshold]\n",
    "    \n",
    "    # Print the size of the group before and after filtering\n",
    "    print(f\"Cluster {cluster}: Original size = {len(group)}, Filtered size = {len(filtered_group)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Distance matrix n= 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Mean matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance matrix**: elemnt d_{ij} has the distance between the center of cluster i and cluster j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store distance matrices for each run\n",
    "distance_matrices_30_35 = []\n",
    "\n",
    "# Iterate over all runs and calculate the distance matrix for each run\n",
    "for run_centroids in kmeans_centroids_30_35:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix_30_35 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    distance_matrices_30_35.append(distance_matrix_30_35)\n",
    "\n",
    "# Convert the list of distance matrices to a numpy array (35 runs, 10x10 distance matrices)\n",
    "distance_matrices_30_35 = np.array(distance_matrices_30_35)\n",
    "\n",
    "# Calculate the mean distance matrix across all runs\n",
    "mean_distance_matrix_30_35 = np.mean(distance_matrices_30_35, axis=0)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_30_35 = (mean_distance_matrix_30_35 - np.min(mean_distance_matrix_30_35)) / (np.max(mean_distance_matrix_30_35) - np.min(mean_distance_matrix_30_35))\n",
    "\n",
    "# Plot of the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_30_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=30)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_30_all_runs.npy', distance_matrices_30_35)\n",
    "np.save('mean_distance_matrix_neighbors_30_35.npy', mean_distance_matrix_30_35)\n",
    "np.save('normalized_mean_distance_matrix_neighbors_30_35.npy', normalized_mean_distance_matrix_30_35)\n",
    "# Mean distance matrix\n",
    "print(f\"Mean distance matrix across all runs:\\n{mean_distance_matrix_30_35}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_distance_matrix_30_35= np.load(f'mean_distance_matrix_neighbors_30_35.npy')\n",
    "# mean_distance_matrix_30_35=np.round(mean_distance_matrix_30_35,3)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_30_35 = (mean_distance_matrix_30_35 - np.min(mean_distance_matrix_30_35)) / (np.max(mean_distance_matrix_30_35) - np.min(mean_distance_matrix_30_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_30_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_30_35,3))\n",
    "np.save('G_30_35.npy', G_30_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_30_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_30_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_30_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_30_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_30_35 = nx.minimum_spanning_tree(G_30_35)\n",
    "np.save('mst_30_35.npy', mst_30_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_30_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_30_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_30_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_30_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST - n_neighbors=30\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance Std. dev. Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Std. dev. Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the pairwise distance matrix for the standard deviations\n",
    "distance_matrix_std_30_35 = cdist(centroid_std_30_35, centroid_std_30_35, metric='euclidean')\n",
    "\n",
    "# Normalize the distance matrix\n",
    "normalized_distance_matrix_std_30_35 = (distance_matrix_std_30_35 - np.min(distance_matrix_std_30_35)) / (np.max(distance_matrix_std_30_35) - np.min(distance_matrix_std_30_35))\n",
    "\n",
    "# Visualize the normalized distance matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(normalized_distance_matrix_std_30_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=30)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrix for later analysis\n",
    "np.save(\"distance_matrix_std_30_35.npy\", distance_matrix_std_30_35)\n",
    "np.save(\"normalized_distance_matrix_std_30_35.npy\", normalized_distance_matrix_std_30_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_std_30_35 = nx.from_numpy_array(np.round(normalized_distance_matrix_std_30_35,3))\n",
    "np.save('G_std_30_35.npy', G_std_30_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_std_30_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_std_30_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_std_30_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_std_30_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_std_30_35 = nx.minimum_spanning_tree(G_std_30_35)\n",
    "np.save('mst_std_30_35.npy', mst_std_30_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos_std_30_35 = nx.spring_layout(mst_std_30_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_std_30_35, pos_std_30_35, with_labels=True, node_color='lightyellow', edge_color='green', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels_std_30_35 = nx.get_edge_attributes(mst_std_30_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_30_35, pos_std_30_35, edge_labels=edge_labels_std_30_35, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST Std. Deviation - n_neighbors=30\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create heatmaps\n",
    "def plot_heatmaps_side_by_side(matrices, titles, figsize=(16, 8), cmap=\"viridis\", annot=True):\n",
    "    \"\"\"\n",
    "    Plots multiple heatmaps side by side for given matrices and titles.\n",
    "\n",
    "    Args:\n",
    "        matrices (list): List of 2D matrices to plot as heatmaps.\n",
    "        titles (list): List of titles corresponding to each matrix.\n",
    "        figsize (tuple): Size of the entire figure (default: (16, 8)).\n",
    "        cmap (str): Color map to use for all heatmaps (default: \"viridis\").\n",
    "        annot (bool): Whether to annotate cells with their values (default: True).\n",
    "    \"\"\"\n",
    "    n = len(matrices)  # Number of heatmaps\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "\n",
    "    for i, (matrix, title) in enumerate(zip(matrices, titles)):\n",
    "        sns.heatmap(matrix, annot=annot, cmap=cmap, fmt=\".2f\", linewidths=0.5, ax=axes[i])\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_xlabel(\"Cluster\")\n",
    "        axes[i].set_ylabel(\"Cluster\" if i == 0 else \"\")  # Only label y-axis for the first plot\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the two heatmaps\n",
    "plot_heatmaps_side_by_side(\n",
    "    matrices=[\n",
    "        normalized_distance_matrix_std_30_35,\n",
    "        normalized_mean_distance_matrix_30_35\n",
    "    ],\n",
    "    titles=[\n",
    "        \"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=30)\",\n",
    "        \"Normalized Mean Distance Matrix (k=10, n_neighbors=30)\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for \"low\" values\n",
    "threshold = 0.55\n",
    "\n",
    "# Identify pairs of clusters with low values in both matrices\n",
    "low_low_pairs = []\n",
    "for i in range(normalized_mean_distance_matrix_30_35.shape[0]):\n",
    "    for j in range(normalized_mean_distance_matrix_30_35.shape[1]):\n",
    "        if i != j:  # Skip diagonal\n",
    "            mean_value = normalized_mean_distance_matrix_30_35[i, j]\n",
    "            std_value = normalized_distance_matrix_std_30_35[i, j]\n",
    "            if mean_value < threshold and std_value < threshold:\n",
    "                low_low_pairs.append((i, j, mean_value, std_value))\n",
    "\n",
    "# Display the results\n",
    "for pair in low_low_pairs:\n",
    "    print(f\"Clusters {pair[0]} and {pair[1]}: Mean Distance = {pair[2]:.2f}, Std Distance = {pair[3]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0.55  lowest threshold so far.**\n",
    "\n",
    "Depending on the goal of the analysis we can think of it as:\n",
    "- If the aim is to identify the strongest relationships between clusters, a lower threshold would make more sense.\n",
    "- If we want to explore the broader connections, then it is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Replace with your cluster pairs from low_low_pairs\n",
    "low_low_pairs = [(7, 8, 0.53, 0.11), (7, 9, 0.55, 0.31)]\n",
    "\n",
    "# UMAP projections and cluster labels (replace with your actual data)\n",
    "umap_projections = np.load(\"umap_projections_neighbors_30.npy\")\n",
    "kmeans_labels = np.load(\"kmeans_labels_list_30_35.npy\")  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Function to plot clusters\n",
    "def plot_clusters(umap_projection, labels, cluster_pair, run_idx):\n",
    "    cluster_a, cluster_b = cluster_pair\n",
    "    points_a = umap_projection[labels == cluster_a]\n",
    "    points_b = umap_projection[labels == cluster_b]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(points_a[:, 0], points_a[:, 1], color=\"blue\", label=f\"Cluster {cluster_a}\", alpha=0.6)\n",
    "    plt.scatter(points_b[:, 0], points_b[:, 1], color=\"orange\", label=f\"Cluster {cluster_b}\", alpha=0.6)\n",
    "    plt.title(f\"Run {run_idx}: Cluster {cluster_a} vs. Cluster {cluster_b}\")\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze each cluster pair\n",
    "for cluster_pair in low_low_pairs:\n",
    "    cluster_a, cluster_b = cluster_pair[0], cluster_pair[1]\n",
    "    print(f\"Analyzing Cluster Pair: {cluster_a} and {cluster_b}\")\n",
    "    \n",
    "    # For simplicity, visualize them in a specific UMAP run (e.g., the first run)\n",
    "    run_idx = 0  # Use the first run for visualization\n",
    "    plot_clusters(umap_projections[run_idx], kmeans_labels[run_idx], (cluster_a, cluster_b), run_idx)\n",
    "\n",
    "    # Calculate additional statistics if needed\n",
    "    distances_a_to_b = np.linalg.norm(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a].mean(axis=0) - \n",
    "                                      umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b].mean(axis=0))\n",
    "    print(f\"Mean Centroid Distance (Run {run_idx}): {distances_a_to_b:.2f}\")\n",
    "\n",
    "    # Variability comparison\n",
    "    cluster_a_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a], axis=0)\n",
    "    cluster_b_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b], axis=0)\n",
    "    print(f\"Cluster {cluster_a} Std Dev: {cluster_a_std}\")\n",
    "    print(f\"Cluster {cluster_b} Std Dev: {cluster_b_std}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 1 and Cluster 8** have a moderate spatial relationship with visible overlap in the UMAP space. Their differing variability patterns suggest distinct structures, but the overlap points might represent shared features or transitions between the clusters.\n",
    "The large spatial separation between their centroids suggests they represent distinct structures or classes in the data.\n",
    "\n",
    "**Cluster 0 and Cluster 9** 9 appears more compact and stable, while Cluster 0 is larger and more variable.\n",
    "Their distinct regions in the UMAP space and differing standard deviations reinforce their meaningful separation.\n",
    "Insights from Variability:\n",
    "\n",
    "The variability of Cluster 0 could indicate sensitivity to UMAP parameters or noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interval of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_30_35 = distance_matrix_std_30_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_30_35 = z_score * sem_matrix_30_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_30_35 = mean_distance_matrix_30_35 - margin_of_error_matrix_30_35\n",
    "upper_limit_intconf_matrix_30_35 = mean_distance_matrix_30_35 + margin_of_error_matrix_30_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_30_35 = np.maximum(lower_limit_intconf_matrix_30_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_30_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_30_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_30_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_30_35.npy', lower_limit_intconf_matrix_30_35)\n",
    "np.save('upper_limit_intconf_matrix_30_35.npy', upper_limit_intconf_matrix_30_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_30_35 = normalize_matrix(lower_limit_intconf_matrix_30_35)\n",
    "norm_upper_limit_intconf_matrix_30_35 = normalize_matrix(upper_limit_intconf_matrix_30_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(normalized_mean_distance_matrix_30_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=30)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_30_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=30)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_30_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=30)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_30_35, \"MST - Mean Distances\", axes[0], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_30_35, \"MST - Lower Limit\", axes[1], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_30_35, \"MST - Upper Limit\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intra class evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "radius = 0.5\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_30_35 = []\n",
    "neighbor_counts_30_35 = []\n",
    "kmeans_labels_list_30_35 = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_30_35):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_30_35 = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_30_35.append(kmeans_labels_5_35)\n",
    "    \n",
    "    run_max_distances_30_35 = []\n",
    "    run_neighbor_counts_30_35 = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_30_35 == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_30_35 = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_30_35.append(max_distance_30_35)\n",
    "        \n",
    "        # Calculate number of neighbors within the radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_30_35 = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_30_35.append(neighbors_within_radius_30_35)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_30_35.append(run_max_distances_30_35)\n",
    "    neighbor_counts_30_35.append(run_neighbor_counts_30_35)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_30_35 = np.array(max_distances_30_35)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_30_35 = np.array(neighbor_counts_30_35)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_30_35 = np.array(kmeans_labels_list_30_35)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_30_35.npy', max_distances_30_35)\n",
    "np.save('neighbor_counts_within_radius_30_35.npy', neighbor_counts_30_35)\n",
    "np.save('kmeans_labels_list_30_35 .npy', kmeans_labels_list_30_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_30_35)\n",
    "print(\"\\nNeighbor counts within radius for each run and each cluster:\\n\", neighbor_counts_30_35)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_distances_30_35_d= np.load(f'max_intra_cluster_distances_dynamic_30_35.npy')\n",
    "neighbor_counts_30_35_d= np.load(f'neighbor_counts_within_dynamic_radius_30_35.npy')\n",
    "kmeans_labels_list_30_35_d= np.load(f'kmeans_labels_list_30_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to track the minimum distance and corresponding clusters\n",
    "overall_min_distance_30 = float('inf')\n",
    "min_distance_clusters_30 = None\n",
    "min_distance_run_idx_30= None\n",
    "\n",
    "for run_idx, run_centroids in enumerate(kmeans_centroids_30_35):\n",
    "    # Compute pairwise distances between centroids\n",
    "    pairwise_distances_30 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    \n",
    "    # Get the indices of the minimum non-zero distance\n",
    "    np.fill_diagonal(pairwise_distances_30, np.inf)  # Ignore zero distances (self-comparisons)\n",
    "    min_distance = np.min(pairwise_distances_30)\n",
    "    if min_distance < overall_min_distance_30:\n",
    "        overall_min_distance_30 = min_distance\n",
    "        # Find the indices of the clusters corresponding to the minimum distance\n",
    "        cluster_indices = np.unravel_index(np.argmin(pairwise_distances_30), pairwise_distances_30.shape)\n",
    "        min_distance_clusters_30 = cluster_indices\n",
    "        min_distance_run_idx_30 = run_idx\n",
    "\n",
    "# Calculate dynamic radius\n",
    "dynamic_radius_30_35 = overall_min_distance_30 / 2\n",
    "print(f\"Dynamic radius: {dynamic_radius_30_35}\")\n",
    "print(f\"Minimum distance: {overall_min_distance_30}\")\n",
    "print(f\"Clusters contributing to minimum distance: {min_distance_clusters_30}\")\n",
    "print(f\"Run index: {min_distance_run_idx_30}\")\n",
    "\n",
    "# Save dynamic radius\n",
    "np.save('dynamic_radius_results_30_35.npy', dynamic_radius_30_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "\n",
    "# Dynamic radius, previously calculated\n",
    "radius = dynamic_radius_30_35\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_30_35_d = []\n",
    "neighbor_counts_30_35_d = []\n",
    "kmeans_labels_list_30_35_d = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_30_35):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_30_35_d = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_30_35_d.append(kmeans_labels_30_35_d)\n",
    "    \n",
    "    run_max_distances_30_35_d = []\n",
    "    run_neighbor_counts_30_35_d = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_30_35_d == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_30_35_d = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_30_35_d.append(max_distance_30_35_d)\n",
    "        \n",
    "        # Calculate number of neighbors within the dynamic radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_30_35_d = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_30_35_d.append(neighbors_within_radius_30_35_d)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_30_35_d.append(run_max_distances_30_35_d)\n",
    "    neighbor_counts_30_35_d.append(run_neighbor_counts_30_35_d)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_30_35_d = np.array(max_distances_30_35_d)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_30_35_d = np.array(neighbor_counts_30_35_d)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_30_35_d = np.array(kmeans_labels_list_30_35_d)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_dynamic_30_35.npy', max_distances_30_35_d)\n",
    "np.save('neighbor_counts_within_dynamic_radius_30_35.npy', neighbor_counts_30_35_d)\n",
    "np.save('kmeans_labels_list_30_35.npy', kmeans_labels_list_30_35_d)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_30_35_d)\n",
    "print(\"\\nNeighbor counts within dynamic radius for each run and each cluster:\\n\", neighbor_counts_30_35_d)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot neighbor counts for each cluster across all runs\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Iterate through each cluster\n",
    "for cluster_idx in range(neighbor_counts_30_35_d.shape[1]):\n",
    "    plt.subplot(2, 5, cluster_idx + 1)  # Create subplots for 10 clusters (2 rows, 5 columns)\n",
    "    plt.plot(range(1, neighbor_counts_30_35_d.shape[0] + 1), neighbor_counts_30_35_d[:, cluster_idx], marker='o')\n",
    "    plt.title(f'Cluster {cluster_idx}')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Neighbor Count')\n",
    "    plt.xticks(range(1, neighbor_counts_30_35_d.shape[0] + 1, 5))  # Show every 5th run on the x-axis for clarity\n",
    "    plt.grid(True)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.suptitle('N=30 Neighbor Counts per Cluster Across Runs', y=1.02, fontsize=16)  # Add a global title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and max values across clusters for each run\n",
    "mean_neighbors_30 = np.mean(neighbor_counts_30_35_d, axis=1)  # Shape: (n_runs,)\n",
    "max_neighbors_30 = np.max(neighbor_counts_30_35_d, axis=1)    # Shape: (n_runs,)\n",
    "\n",
    "# Compute trend lines for mean and max\n",
    "runs = np.arange(1, len(mean_neighbors_30) + 1)\n",
    "mean_slope, mean_intercept, _, _, _ = linregress(runs, mean_neighbors_30)\n",
    "max_slope, max_intercept, _, _, _ = linregress(runs, max_neighbors_30)\n",
    "\n",
    "# Calculate trend line values\n",
    "mean_trend_30 = mean_slope * runs + mean_intercept\n",
    "max_trend_30 = max_slope * runs + max_intercept\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Mean neighbor counts\n",
    "plt.plot(runs, mean_neighbors_30, label='Mean Neighbor Count', marker='o', color='blue')\n",
    "\n",
    "# Max neighbor counts\n",
    "plt.plot(runs, max_neighbors_30, label='Max Neighbor Count', marker='s', color='orange')\n",
    "\n",
    "# Trend lines\n",
    "plt.plot(runs, mean_trend_30, linestyle='--', color='green',label='Mean Trend Line')\n",
    "plt.plot(runs, max_trend_30, linestyle='--', color='green', label='Max Trend Line')\n",
    "\n",
    "# Add labels, legend, and title\n",
    "plt.title('N=30 Neighbor Counts Across Runs (Mean vs. Max with Trend Lines)', fontsize=16)\n",
    "plt.xlabel('Run', fontsize=12)\n",
    "plt.ylabel('Neighbor Count', fontsize=12)\n",
    "plt.xticks(range(1, len(mean_neighbors_30) + 1, 5))  # Show every 5th run for readability\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(f'neighbor_counts_plot_n_30_35.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_neighbors = 50, n_runs = 35, n_clusters = 10 (for KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the n_neighbors Analysis\n",
    "umap_projections_50 = np.load(f'umap_projections_neighbors_50.npy')\n",
    "centroid_mean_50_35= np.load(f'centroid_mean_50_35.npy')\n",
    "centroid_std_50_35= np.load(f'centroid_std_50_35.npy')\n",
    "kmeans_centroids_50_35 = np.load(f\"kmeans_centroids_neighbors_50.npy\")\n",
    "df_results_v2=pd.read_csv('result_table_neighbors_50_35.csv')\n",
    "mean_distance_matrix_50_35= np.load(f'mean_distance_matrix_neighbors_50_35.npy')\n",
    "distance_matrix_std_50_35= np.load(f\"distance_matrix_std_50_35.npy\")\n",
    "normalized_distance_matrix_std_50_35= np.load(f'normalized_distance_matrix_std_50_35.npy')\n",
    "normalized_mean_distance_matrix_50_35= np.load(f'normalized_mean_distance_matrix_50_35.npy')\n",
    "mst_50_35= np.load(f'mst_50_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "\n",
    "# Define the number of UMAP and KMeans runs\n",
    "n_runs = 35\n",
    "n_clusters = 10  # Set the number of clusters (for KMeans)\n",
    "n_neighbors = 50\n",
    "\n",
    "# Store UMAP and KMeans results for each run\n",
    "umap_projections_50_35 = []\n",
    "kmeans_centroids_list_50_35 = []  # Use this to store centroids for each run\n",
    "\n",
    "# Define a helper function to calculate the centroid of each cluster\n",
    "def calculate_centroids(kmeans, x_umap):\n",
    "    centroids = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = x_umap[kmeans.labels_ == i]\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        centroids.append(centroid)\n",
    "    return np.array(centroids)\n",
    "\n",
    "# Run UMAP and KMeans multiple times\n",
    "for run in range(n_runs):\n",
    "    # Apply UMAP with the same parameters for each run\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=0.1, n_components=2, random_state=None)  # No random_state to allow randomness, use random_state=None\n",
    "    x_train_umap = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Apply KMeans clustering on the UMAP projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_train_umap)\n",
    "\n",
    "    # Calculate centroids for this run\n",
    "    centroids = calculate_centroids(kmeans, x_train_umap)\n",
    "    \n",
    "    # Store the UMAP projections and KMeans models\n",
    "    umap_projections_50_35.append(x_train_umap)\n",
    "    kmeans_centroids_list_50_35.append(centroids)\n",
    "\n",
    "# Now we calculate the mean and standard deviation of the centroids across all runs\n",
    "kmeans_centroids = np.array(kmeans_centroids_list_50_35)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "# Calculate mean and std deviation for centroids' coordinates\n",
    "centroid_mean = np.mean(kmeans_centroids, axis=0)\n",
    "centroid_std = np.std(kmeans_centroids, axis=0)\n",
    "\n",
    "# Save the UMAP projections and KMeans centroids\n",
    "np.save(f'umap_projections_neighbors_{n_neighbors}.npy', np.array(umap_projections_50_35))\n",
    "np.save(f'kmeans_centroids_neighbors_{n_neighbors}.npy', np.array(kmeans_centroids_list_50_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the UMAP projections\n",
    "umap_projections_50_35 = np.load(f'umap_projections_neighbors_50.npy')\n",
    "\n",
    "# To see the contents of the UMAP projections\n",
    "print(umap_projections_50_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "\n",
    "# Save the centroid_mean and centroid_std\n",
    "np.save(f'centroid_mean_50_35.npy', np.array(centroid_mean))\n",
    "np.save(f'centroid_std_50_35.npy', np.array(centroid_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_mean_50_35= np.load(f'centroid_mean_50_35.npy')\n",
    "centroid_std_50_35= np.load(f'centroid_std_50_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_centroids_50_35 = np.load(f\"kmeans_centroids_neighbors_50.npy\")  # Load the saved centroids data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_50_35 = np.zeros(10)\n",
    "std_dev_y_50_35 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_50_35[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_50_35[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_50_35[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_50_35[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_50_35)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_50_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_50_35 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_50_35[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_50_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x_50_35[cluster], mean_x + 2 * std_dev_x_50_35[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y_50_35[cluster], mean_y + 2 * std_dev_y_50_35[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_50_35.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_50_35 = pd.DataFrame(data_50_35, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true_50_35 = df_results_50_35.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true_50_35 = trials_all_true_50_35[trials_all_true_50_35].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true_50_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false_50_35 = trials_all_true_50_35[~trials_all_true_50_35].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false_50_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Save the result table to a CSV file\n",
    "df_results_50_35.to_csv(f'result_table_neighbors_50_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal outliers process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_50_35=pd.read_csv('result_table_neighbors_50_35.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array into a DataFrame with 'Cluster', 'x_mean', and 'y_mean'\n",
    "centroid_mean_neighbors_50_35_df = pd.DataFrame(centroid_mean_50_35, columns=['x_mean', 'y_mean'])\n",
    "centroid_mean_neighbors_50_35_df['Cluster'] = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Add commas between numbers in 'Centroid Coord' entries if they are missing\n",
    "df_results_50_35['Centroid Coord'] = df_results_50_35['Centroid Coord'].str.replace(\n",
    "    r'(\\-?\\d+\\.\\d+)\\s+(\\-?\\d+\\.\\d+)', r'\\1, \\2', regex=True\n",
    ")\n",
    "\n",
    "# Step 2: Convert 'Centroid Coord' from string to list\n",
    "df_results_50_35['Centroid Coord'] = df_results_50_35['Centroid Coord'].apply(ast.literal_eval)\n",
    "\n",
    "# Step 3: Verify if each entry in 'Centroid Coord' is a list of length 2\n",
    "invalid_rows = df_results_50_35[df_results_50_35['Centroid Coord'].apply(lambda x: not (isinstance(x, list) and len(x) == 2))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y coordinates\n",
    "df_results_50_35[['x', 'y']] = pd.DataFrame(df_results_50_35['Centroid Coord'].tolist(), index=df_results_50_35.index)\n",
    "\n",
    "# Merge the mean centroids dataframe with the results dataframe on 'Cluster'\n",
    "df_merged_50_35 = pd.merge(df_results_50_35, centroid_mean_neighbors_50_35_df, on='Cluster', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot changes in X-coordinate for each cluster over all runs\n",
    "n_runs = 35\n",
    "n_clusters = 10\n",
    "n_neighbors = 50\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_50_35[:, cluster, 0], marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Cluster {cluster} X-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('X Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot changes in Y-coordinate for each cluster over all runs\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_50_35[:, cluster, 1], marker='o', linestyle='-', color='g')\n",
    "    plt.title(f'Cluster {cluster} Y-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Y Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance from each centroid to its cluster's mean\n",
    "df_merged_50_35['Distance_to_Mean'] = np.sqrt((df_merged_50_35['x'] - df_merged_50_35['x_mean'])**2 + (df_merged_50_35['y'] - df_merged_50_35['y_mean'])**2)\n",
    "\n",
    "# Apply an outlier threshold (e.g., 90th percentile of the distance per cluster)\n",
    "def filter_outliers(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return df[df['Distance_to_Mean'] <= threshold]\n",
    "\n",
    "# Apply the filtering function for each cluster\n",
    "df_no_outliers = df_merged_50_35.groupby('Cluster').apply(filter_outliers).reset_index(drop=True)\n",
    "\n",
    "# Step 7: Drop unnecessary columns if needed (like 'x' and 'y' if only the distance matters)\n",
    "df_no_outliers_cleaned_50_35 = df_no_outliers.drop(columns=['x', 'y', 'x_mean', 'y_mean'])\n",
    "\n",
    "# Step 8: Check the size of the resulting dataframe\n",
    "print(f\"Original DataFrame size: {df_merged_50_35.shape}\")\n",
    "print(f\"DataFrame size after removing outliers: {df_no_outliers_cleaned_50_35.shape}\")\n",
    "\n",
    "# Display the final dataframe to the user\n",
    "df_no_outliers_cleaned_50_35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by 'Cluster'\n",
    "clusters_grouped_50_35 = df_no_outliers_cleaned_50_35.groupby('Cluster')\n",
    "\n",
    "# Create a dictionary to store arrays for each cluster's centroids\n",
    "clusters_centroids_50_35 = {}\n",
    "\n",
    "# Loop through each group (cluster) and store the centroids in arrays\n",
    "for cluster, group in clusters_grouped_50_35:\n",
    "    # Extract centroids (x, y) as a NumPy array\n",
    "    centroids_array = np.array(group['Centroid Coord'].tolist())  # Assuming 'Centroid Coord' contains [x, y] pairs\n",
    "    clusters_centroids_50_35[cluster] = centroids_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the size of each cluster\n",
    "cluster_sizes = {cluster: len(centroids) for cluster, centroids in clusters_centroids_50_35.items()}\n",
    "\n",
    "# Print the size of each cluster\n",
    "for cluster, size in cluster_sizes.items():\n",
    "    print(f\"Cluster {cluster} has {size} centroids considered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to verify that if it is fine to have all clusters with the same number of centroids after filtering out outliers. This must be due to:\n",
    "- The Distance Distributions are Likely Very Similar\n",
    "- Uniform Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each cluster and plot the distribution of distances\n",
    "for cluster, group in clusters_grouped_50_35:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(group['Distance_to_Mean'], bins=10, edgecolor='black')\n",
    "    plt.title(f'Cluster {cluster}: Distance to Mean Distribution')\n",
    "    plt.xlabel('Distance to Mean')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentile threshold per cluster check\n",
    "def check_percentiles(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return threshold\n",
    "\n",
    "# Function applied to each cluster and print the result\n",
    "for cluster, group in clusters_grouped_50_35:\n",
    "    threshold = check_percentiles(group)\n",
    "    print(f\"Cluster {cluster}: 90th percentile threshold = {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster, calculate the 70th percentile of distances and filter accordingly\n",
    "for cluster, group in clusters_grouped_50_35:\n",
    "    # Calculate the 70th percentile threshold for the current cluster\n",
    "    threshold = np.percentile(group['Distance_to_Mean'], 70)\n",
    "    \n",
    "    # Filter centroids based on the 70th percentile\n",
    "    filtered_group = group[group['Distance_to_Mean'] <= threshold]\n",
    "    \n",
    "    # Print the size of the group before and after filtering\n",
    "    print(f\"Cluster {cluster}: Original size = {len(group)}, Filtered size = {len(filtered_group)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Distance matrix n= 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Mean matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance matrix**: elemnt d_{ij} has the distance between the center of cluster i and cluster j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store distance matrices for each run\n",
    "distance_matrices_50_35 = []\n",
    "\n",
    "# Iterate over all runs and calculate the distance matrix for each run\n",
    "for run_centroids in kmeans_centroids_50_35:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix_50_35 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    distance_matrices_50_35.append(distance_matrix_50_35)\n",
    "\n",
    "# Convert the list of distance matrices to a numpy array (35 runs, 10x10 distance matrices)\n",
    "distance_matrices_50_35 = np.array(distance_matrices_50_35)\n",
    "\n",
    "# Calculate the mean distance matrix across all runs\n",
    "mean_distance_matrix_50_35 = np.mean(distance_matrices_50_35, axis=0)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_50_35 = (mean_distance_matrix_50_35 - np.min(mean_distance_matrix_50_35)) / (np.max(mean_distance_matrix_50_35) - np.min(mean_distance_matrix_50_35))\n",
    "\n",
    "# Plot of the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_50_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=50)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_50_all_runs.npy', distance_matrices_50_35)\n",
    "np.save('mean_distance_matrix_neighbors_50_35.npy', mean_distance_matrix_50_35)\n",
    "np.save('normalized_mean_distance_matrix_50_35.npy', normalized_mean_distance_matrix_50_35)\n",
    "# Mean distance matrix\n",
    "print(f\"Mean distance matrix across all runs:\\n{mean_distance_matrix_50_35}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_distance_matrix_50_35= np.load(f'mean_distance_matrix_neighbors_50_35.npy')\n",
    "# mean_distance_matrix_50_35=np.round(mean_distance_matrix_50_35,3)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_50_35 = (mean_distance_matrix_50_35 - np.min(mean_distance_matrix_50_35)) / (np.max(mean_distance_matrix_50_35) - np.min(mean_distance_matrix_50_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_50_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_50_35,3))\n",
    "np.save('G_50_35.npy', G_50_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_50_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_50_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_50_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_50_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_50_35 = nx.minimum_spanning_tree(G_50_35)\n",
    "np.save('mst_50_35.npy', mst_50_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_50_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_50_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_50_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_50_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST - n_neighbors=50\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Std. dev. Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the pairwise distance matrix for the standard deviations\n",
    "distance_matrix_std_50_35 = cdist(centroid_std_50_35, centroid_std_50_35, metric='euclidean')\n",
    "\n",
    "# Normalize the distance matrix\n",
    "normalized_distance_matrix_std_50_35 = (distance_matrix_std_50_35 - np.min(distance_matrix_std_50_35)) / (np.max(distance_matrix_std_50_35) - np.min(distance_matrix_std_50_35))\n",
    "\n",
    "# Visualize the normalized distance matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(normalized_distance_matrix_std_50_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=50)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrix for later analysis\n",
    "np.save(\"distance_matrix_std_50_35.npy\", distance_matrix_std_50_35)\n",
    "np.save(\"normalized_distance_matrix_std_50_35.npy\", normalized_distance_matrix_std_50_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_std_50_35 = nx.from_numpy_array(np.round(normalized_distance_matrix_std_50_35,3))\n",
    "np.save('G_std_50_35.npy', G_std_50_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_std_50_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_std_50_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_std_50_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_std_50_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_std_50_35 = nx.minimum_spanning_tree(G_std_50_35)\n",
    "np.save('mst_std_50_35.npy', mst_std_50_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos_std_50_35 = nx.spring_layout(mst_std_50_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_std_50_35, pos_std_50_35, with_labels=True, node_color='lightyellow', edge_color='green', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels_std_50_35 = nx.get_edge_attributes(mst_std_50_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_50_35, pos_std_50_35, edge_labels=edge_labels_std_50_35, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST Std. Deviation - n_neighbors=50\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create heatmaps\n",
    "def plot_heatmaps_side_by_side(matrices, titles, figsize=(16, 8), cmap=\"viridis\", annot=True):\n",
    "    \"\"\"\n",
    "    Plots multiple heatmaps side by side for given matrices and titles.\n",
    "\n",
    "    Args:\n",
    "        matrices (list): List of 2D matrices to plot as heatmaps.\n",
    "        titles (list): List of titles corresponding to each matrix.\n",
    "        figsize (tuple): Size of the entire figure (default: (16, 8)).\n",
    "        cmap (str): Color map to use for all heatmaps (default: \"viridis\").\n",
    "        annot (bool): Whether to annotate cells with their values (default: True).\n",
    "    \"\"\"\n",
    "    n = len(matrices)  # Number of heatmaps\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "\n",
    "    for i, (matrix, title) in enumerate(zip(matrices, titles)):\n",
    "        sns.heatmap(matrix, annot=annot, cmap=cmap, fmt=\".2f\", linewidths=0.5, ax=axes[i])\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_xlabel(\"Cluster\")\n",
    "        axes[i].set_ylabel(\"Cluster\" if i == 0 else \"\")  # Only label y-axis for the first plot\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the two heatmaps\n",
    "plot_heatmaps_side_by_side(\n",
    "    matrices=[\n",
    "        normalized_distance_matrix_std_50_35,\n",
    "        normalized_mean_distance_matrix_50_35\n",
    "    ],\n",
    "    titles=[\n",
    "        \"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=50)\",\n",
    "        \"Normalized Mean Distance Matrix (k=10, n_neighbors=50)\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for \"low\" values\n",
    "threshold = 0.65\n",
    "\n",
    "# Identify pairs of clusters with low values in both matrices\n",
    "low_low_pairs = []\n",
    "for i in range(normalized_mean_distance_matrix_50_35.shape[0]):\n",
    "    for j in range(normalized_mean_distance_matrix_50_35.shape[1]):\n",
    "        if i != j:  # Skip diagonal\n",
    "            mean_value = normalized_mean_distance_matrix_50_35[i, j]\n",
    "            std_value = normalized_distance_matrix_std_50_35[i, j]\n",
    "            if mean_value < threshold and std_value < threshold:\n",
    "                low_low_pairs.append((i, j, mean_value, std_value))\n",
    "\n",
    "# Display the results\n",
    "for pair in low_low_pairs:\n",
    "    print(f\"Clusters {pair[0]} and {pair[1]}: Mean Distance = {pair[2]:.2f}, Std Distance = {pair[3]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0.55  lowest threshold so far.**\n",
    "\n",
    "Depending on the goal of the analysis we can think of it as:\n",
    "- If the aim is to identify the strongest relationships between clusters, a lower threshold would make more sense.\n",
    "- If we want to explore the broader connections, then it is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Replace with your cluster pairs from low_low_pairs\n",
    "low_low_pairs = [(0, 8, 0.65, 0.54), (7, 8, 0.62, 0.17), (7, 9, 0.60, 0.25)]\n",
    "\n",
    "# UMAP projections and cluster labels (replace with your actual data)\n",
    "umap_projections = np.load(\"umap_projections_neighbors_50.npy\")\n",
    "kmeans_labels = np.load(\"kmeans_labels_list_50_35.npy\")  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Function to plot clusters\n",
    "def plot_clusters(umap_projection, labels, cluster_pair, run_idx):\n",
    "    cluster_a, cluster_b = cluster_pair\n",
    "    points_a = umap_projection[labels == cluster_a]\n",
    "    points_b = umap_projection[labels == cluster_b]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(points_a[:, 0], points_a[:, 1], color=\"blue\", label=f\"Cluster {cluster_a}\", alpha=0.6)\n",
    "    plt.scatter(points_b[:, 0], points_b[:, 1], color=\"orange\", label=f\"Cluster {cluster_b}\", alpha=0.6)\n",
    "    plt.title(f\"Run {run_idx}: Cluster {cluster_a} vs. Cluster {cluster_b}\")\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze each cluster pair\n",
    "for cluster_pair in low_low_pairs:\n",
    "    cluster_a, cluster_b = cluster_pair[0], cluster_pair[1]\n",
    "    print(f\"Analyzing Cluster Pair: {cluster_a} and {cluster_b}\")\n",
    "    \n",
    "    # For simplicity, visualize them in a specific UMAP run (e.g., the first run)\n",
    "    run_idx = 0  # Use the first run for visualization\n",
    "    plot_clusters(umap_projections[run_idx], kmeans_labels[run_idx], (cluster_a, cluster_b), run_idx)\n",
    "\n",
    "    # Calculate additional statistics if needed\n",
    "    distances_a_to_b = np.linalg.norm(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a].mean(axis=0) - \n",
    "                                      umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b].mean(axis=0))\n",
    "    print(f\"Mean Centroid Distance (Run {run_idx}): {distances_a_to_b:.2f}\")\n",
    "\n",
    "    # Variability comparison\n",
    "    cluster_a_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a], axis=0)\n",
    "    cluster_b_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b], axis=0)\n",
    "    print(f\"Cluster {cluster_a} Std Dev: {cluster_a_std}\")\n",
    "    print(f\"Cluster {cluster_b} Std Dev: {cluster_b_std}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 1 and Cluster 8** have a moderate spatial relationship with visible overlap in the UMAP space. Their differing variability patterns suggest distinct structures, but the overlap points might represent shared features or transitions between the clusters.\n",
    "The large spatial separation between their centroids suggests they represent distinct structures or classes in the data.\n",
    "\n",
    "**Cluster 0 and Cluster 9** 9 appears more compact and stable, while Cluster 0 is larger and more variable.\n",
    "Their distinct regions in the UMAP space and differing standard deviations reinforce their meaningful separation.\n",
    "Insights from Variability:\n",
    "\n",
    "The variability of Cluster 0 could indicate sensitivity to UMAP parameters or noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interval of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_50_35 = distance_matrix_std_50_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_50_35 = z_score * sem_matrix_50_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_50_35 = mean_distance_matrix_50_35 - margin_of_error_matrix_50_35\n",
    "upper_limit_intconf_matrix_50_35 = mean_distance_matrix_50_35 + margin_of_error_matrix_50_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_50_35 = np.maximum(lower_limit_intconf_matrix_50_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_50_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_50_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_50_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_50_35.npy', lower_limit_intconf_matrix_50_35)\n",
    "np.save('upper_limit_intconf_matrix_50_35.npy', upper_limit_intconf_matrix_50_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_50_35 = normalize_matrix(lower_limit_intconf_matrix_50_35)\n",
    "norm_upper_limit_intconf_matrix_50_35 = normalize_matrix(upper_limit_intconf_matrix_50_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_50_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=50)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_50_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=50)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_50_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=50)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_50_35, \"MST - Mean Distances\", axes[0], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_50_35, \"MST - Lower Limit\", axes[1], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_50_35, \"MST - Upper Limit\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intra class evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_distances_50_35_d= np.load(f'max_intra_cluster_distances_dynamic_50_35.npy')\n",
    "neighbor_counts_50_35_d= np.load(f'neighbor_counts_within_dynamic_radius_50_35.npy')\n",
    "kmeans_labels_list_50_35_d= np.load(f'kmeans_labels_list_50_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to track the minimum distance and corresponding clusters\n",
    "overall_min_distance_50 = float('inf')\n",
    "min_distance_clusters_50 = None\n",
    "min_distance_run_idx_50= None\n",
    "\n",
    "for run_idx, run_centroids in enumerate(kmeans_centroids_50_35):\n",
    "    # Compute pairwise distances between centroids\n",
    "    pairwise_distances_50 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    \n",
    "    # Get the indices of the minimum non-zero distance\n",
    "    np.fill_diagonal(pairwise_distances_50, np.inf)  # Ignore zero distances (self-comparisons)\n",
    "    min_distance = np.min(pairwise_distances_50)\n",
    "    if min_distance < overall_min_distance_50:\n",
    "        overall_min_distance_50 = min_distance\n",
    "        # Find the indices of the clusters corresponding to the minimum distance\n",
    "        cluster_indices = np.unravel_index(np.argmin(pairwise_distances_50), pairwise_distances_50.shape)\n",
    "        min_distance_clusters_50 = cluster_indices\n",
    "        min_distance_run_idx_50 = run_idx\n",
    "\n",
    "# Calculate dynamic radius\n",
    "dynamic_radius_50_35 = overall_min_distance_50 / 2\n",
    "print(f\"Dynamic radius: {dynamic_radius_50_35}\")\n",
    "print(f\"Minimum distance: {overall_min_distance_50}\")\n",
    "print(f\"Clusters contributing to minimum distance: {min_distance_clusters_50}\")\n",
    "print(f\"Run index: {min_distance_run_idx_50}\")\n",
    "\n",
    "# Save dynamic radius\n",
    "np.save('dynamic_radius_results_50_35.npy', dynamic_radius_50_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "\n",
    "# Dynamic radius, previously calculated\n",
    "radius = dynamic_radius_50_35\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_50_35_d = []\n",
    "neighbor_counts_50_35_d = []\n",
    "kmeans_labels_list_50_35_d = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_50_35):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_50_35_d = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_50_35_d.append(kmeans_labels_50_35_d)\n",
    "    \n",
    "    run_max_distances_50_35_d = []\n",
    "    run_neighbor_counts_50_35_d = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_50_35_d == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_50_35_d = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_50_35_d.append(max_distance_50_35_d)\n",
    "        \n",
    "        # Calculate number of neighbors within the dynamic radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_50_35_d = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_50_35_d.append(neighbors_within_radius_50_35_d)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_50_35_d.append(run_max_distances_50_35_d)\n",
    "    neighbor_counts_50_35_d.append(run_neighbor_counts_50_35_d)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_50_35_d = np.array(max_distances_50_35_d)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_50_35_d = np.array(neighbor_counts_50_35_d)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_50_35_d = np.array(kmeans_labels_list_50_35_d)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_dynamic_50_35.npy', max_distances_50_35_d)\n",
    "np.save('neighbor_counts_within_dynamic_radius_50_35.npy', neighbor_counts_50_35_d)\n",
    "np.save('kmeans_labels_list_50_35.npy', kmeans_labels_list_50_35_d)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_50_35_d)\n",
    "print(\"\\nNeighbor counts within dynamic radius for each run and each cluster:\\n\", neighbor_counts_50_35_d)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot neighbor counts for each cluster across all runs\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Iterate through each cluster\n",
    "for cluster_idx in range(neighbor_counts_50_35_d.shape[1]):\n",
    "    plt.subplot(2, 5, cluster_idx + 1)  # Create subplots for 10 clusters (2 rows, 5 columns)\n",
    "    plt.plot(range(1, neighbor_counts_50_35_d.shape[0] + 1), neighbor_counts_50_35_d[:, cluster_idx], marker='o')\n",
    "    plt.title(f'Cluster {cluster_idx}')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Neighbor Count')\n",
    "    plt.xticks(range(1, neighbor_counts_50_35_d.shape[0] + 1, 5))  # Show every 5th run on the x-axis for clarity\n",
    "    plt.grid(True)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.suptitle('N=30 Neighbor Counts per Cluster Across Runs', y=1.02, fontsize=16)  # Add a global title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and max values across clusters for each run\n",
    "mean_neighbors_50 = np.mean(neighbor_counts_50_35_d, axis=1)  # Shape: (n_runs,)\n",
    "max_neighbors_50 = np.max(neighbor_counts_50_35_d, axis=1)    # Shape: (n_runs,)\n",
    "\n",
    "# Compute trend lines for mean and max\n",
    "runs = np.arange(1, len(mean_neighbors_50) + 1)\n",
    "mean_slope, mean_intercept, _, _, _ = linregress(runs, mean_neighbors_50)\n",
    "max_slope, max_intercept, _, _, _ = linregress(runs, max_neighbors_50)\n",
    "\n",
    "# Calculate trend line values\n",
    "mean_trend_50 = mean_slope * runs + mean_intercept\n",
    "max_trend_50 = max_slope * runs + max_intercept\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Mean neighbor counts\n",
    "plt.plot(runs, mean_neighbors_50, label='Mean Neighbor Count', marker='o', color='blue')\n",
    "\n",
    "# Max neighbor counts\n",
    "plt.plot(runs, max_neighbors_50, label='Max Neighbor Count', marker='s', color='orange')\n",
    "\n",
    "# Trend lines\n",
    "plt.plot(runs, mean_trend_50, linestyle='--', color='green',label='Mean Trend Line')\n",
    "plt.plot(runs, max_trend_50, linestyle='--', color='green', label='Max Trend Line')\n",
    "\n",
    "# Add labels, legend, and title\n",
    "plt.title('N=50 Neighbor Counts Across Runs (Mean vs. Max with Trend Lines)', fontsize=16)\n",
    "plt.xlabel('Run', fontsize=12)\n",
    "plt.ylabel('Neighbor Count', fontsize=12)\n",
    "plt.xticks(range(1, len(mean_neighbors_50) + 1, 5))  # Show every 5th run for readability\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(f'neighbor_counts_plot_n_50_35.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "radius = 0.5\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_50_35 = []\n",
    "neighbor_counts_50_35 = []\n",
    "kmeans_labels_list_50_35 = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_50_35):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_50_35 = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_50_35.append(kmeans_labels_5_35)\n",
    "    \n",
    "    run_max_distances_50_35 = []\n",
    "    run_neighbor_counts_50_35 = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_50_35 == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_50_35 = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_50_35.append(max_distance_50_35)\n",
    "        \n",
    "        # Calculate number of neighbors within the radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_50_35 = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_50_35.append(neighbors_within_radius_50_35)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_50_35.append(run_max_distances_50_35)\n",
    "    neighbor_counts_50_35.append(run_neighbor_counts_50_35)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_50_35 = np.array(max_distances_50_35)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_50_35 = np.array(neighbor_counts_50_35)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_50_35 = np.array(kmeans_labels_list_50_35)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_5_35.npy', max_distances_50_35)\n",
    "np.save('neighbor_counts_within_radius_5_35.npy', neighbor_counts_50_35)\n",
    "np.save('kmeans_labels_list_5_35 .npy', kmeans_labels_list_50_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_50_35)\n",
    "print(\"\\nNeighbor counts within radius for each run and each cluster:\\n\", neighbor_counts_50_35)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_neighbors = 100, n_runs = 35, n_clusters = 10 (for KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the n_neighbors Analysis\n",
    "umap_projections_100 = np.load(f'umap_projections_neighbors_100.npy')\n",
    "centroid_mean_100_35= np.load(f'centroid_mean_100_35.npy')\n",
    "centroid_std_100_35= np.load(f'centroid_std_100_35.npy')\n",
    "kmeans_centroids_100_35 = np.load(f\"kmeans_centroids_neighbors_100.npy\")\n",
    "df_results_v2=pd.read_csv('result_table_neighbors_100_35.csv')\n",
    "mean_distance_matrix_100_35= np.load(f'mean_distance_matrix_neighbors_100_35.npy')\n",
    "distance_matrix_std_100_35= np.load(f\"distance_matrix_std_100_35.npy\")\n",
    "normalized_distance_matrix_std_100_35= np.load(f'normalized_distance_matrix_std_100_35.npy')\n",
    "normalized_mean_distance_matrix_100_35= np.load(f'normalized_mean_distance_matrix_100_35.npy')\n",
    "mst_std_100_35= np.load(f'mst_std_100_35.npy')\n",
    "mst_100_35= np.load(f'mst_100_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "\n",
    "# Define the number of UMAP and KMeans runs\n",
    "n_runs = 35\n",
    "n_clusters = 10  # Set the number of clusters (for KMeans)\n",
    "n_neighbors = 100\n",
    "\n",
    "# Store UMAP and KMeans results for each run\n",
    "umap_projections_100_35 = []\n",
    "kmeans_centroids_list_100_35 = []  # Use this to store centroids for each run\n",
    "\n",
    "# Define a helper function to calculate the centroid of each cluster\n",
    "def calculate_centroids(kmeans, x_umap):\n",
    "    centroids = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = x_umap[kmeans.labels_ == i]\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        centroids.append(centroid)\n",
    "    return np.array(centroids)\n",
    "\n",
    "# Run UMAP and KMeans multiple times\n",
    "for run in range(n_runs):\n",
    "    # Apply UMAP with the same parameters for each run\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=0.1, n_components=2, random_state=None)  # No random_state to allow randomness, use random_state=None\n",
    "    x_train_umap = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Apply KMeans clustering on the UMAP projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_train_umap)\n",
    "\n",
    "    # Calculate centroids for this run\n",
    "    centroids = calculate_centroids(kmeans, x_train_umap)\n",
    "    \n",
    "    # Store the UMAP projections and KMeans models\n",
    "    umap_projections_100_35.append(x_train_umap)\n",
    "    kmeans_centroids_list_100_35.append(centroids)\n",
    "\n",
    "# Now we calculate the mean and standard deviation of the centroids across all runs\n",
    "kmeans_centroids = np.array(kmeans_centroids_list_100_35)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "# Calculate mean and std deviation for centroids' coordinates\n",
    "centroid_mean = np.mean(kmeans_centroids, axis=0)\n",
    "centroid_std = np.std(kmeans_centroids, axis=0)\n",
    "\n",
    "# Save the UMAP projections and KMeans centroids\n",
    "np.save(f'umap_projections_neighbors_{n_neighbors}.npy', np.array(umap_projections_100_35))\n",
    "np.save(f'kmeans_centroids_neighbors_{n_neighbors}.npy', np.array(kmeans_centroids_list_100_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the UMAP projections\n",
    "umap_projections_100_35 = np.load(f'umap_projections_neighbors_100.npy')\n",
    "\n",
    "# To see the contents of the UMAP projections\n",
    "print(umap_projections_100_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "\n",
    "# Save the centroid_mean and centroid_std\n",
    "np.save(f'centroid_mean_100_35.npy', np.array(centroid_mean))\n",
    "np.save(f'centroid_std_100_35.npy', np.array(centroid_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_mean_100_35= np.load(f'centroid_mean_100_35.npy')\n",
    "centroid_std_100_35= np.load(f'centroid_std_100_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_centroids_100_35 = np.load(f\"kmeans_centroids_neighbors_100.npy\")  # Load the saved centroids data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_100_35 = np.zeros(10)\n",
    "std_dev_y_100_35 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_100_35[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_100_35[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_100_35[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_100_35[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_100_35)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_100_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_100_35 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_100_35[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_100_35[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x_100_35[cluster], mean_x + 2 * std_dev_x_100_35[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y_100_35[cluster], mean_y + 2 * std_dev_y_100_35[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_100_35.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_100_35 = pd.DataFrame(data_100_35, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true_100_35 = df_results_100_35.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true_100_35 = trials_all_true_100_35[trials_all_true_100_35].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true_100_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false_100_35 = trials_all_true_100_35[~trials_all_true_100_35].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false_100_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Save the result table to a CSV file\n",
    "df_results_100_35.to_csv(f'result_table_neighbors_100_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal outliers process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_100_35=pd.read_csv('result_table_neighbors_100_35.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array into a DataFrame with 'Cluster', 'x_mean', and 'y_mean'\n",
    "centroid_mean_neighbors_100_35_df = pd.DataFrame(centroid_mean_100_35, columns=['x_mean', 'y_mean'])\n",
    "centroid_mean_neighbors_100_35_df['Cluster'] = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Add commas between numbers in 'Centroid Coord' entries if they are missing\n",
    "df_results_100_35['Centroid Coord'] = df_results_100_35['Centroid Coord'].str.replace(\n",
    "    r'(\\-?\\d+\\.\\d+)\\s+(\\-?\\d+\\.\\d+)', r'\\1, \\2', regex=True\n",
    ")\n",
    "\n",
    "# Step 2: Convert 'Centroid Coord' from string to list\n",
    "df_results_100_35['Centroid Coord'] = df_results_100_35['Centroid Coord'].apply(ast.literal_eval)\n",
    "\n",
    "# Step 3: Verify if each entry in 'Centroid Coord' is a list of length 2\n",
    "invalid_rows = df_results_100_35[df_results_100_35['Centroid Coord'].apply(lambda x: not (isinstance(x, list) and len(x) == 2))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y coordinates\n",
    "df_results_100_35[['x', 'y']] = pd.DataFrame(df_results_100_35['Centroid Coord'].tolist(), index=df_results_100_35.index)\n",
    "\n",
    "# Merge the mean centroids dataframe with the results dataframe on 'Cluster'\n",
    "df_merged_100_35 = pd.merge(df_results_100_35, centroid_mean_neighbors_100_35_df, on='Cluster', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot changes in X-coordinate for each cluster over all runs\n",
    "n_runs = 35\n",
    "n_clusters = 10\n",
    "n_neighbors = 100\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_100_35[:, cluster, 0], marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Cluster {cluster} X-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('X Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot changes in Y-coordinate for each cluster over all runs\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_100_35[:, cluster, 1], marker='o', linestyle='-', color='g')\n",
    "    plt.title(f'Cluster {cluster} Y-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Y Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance from each centroid to its cluster's mean\n",
    "df_merged_100_35['Distance_to_Mean'] = np.sqrt((df_merged_100_35['x'] - df_merged_100_35['x_mean'])**2 + (df_merged_100_35['y'] - df_merged_100_35['y_mean'])**2)\n",
    "\n",
    "# Apply an outlier threshold (e.g., 90th percentile of the distance per cluster)\n",
    "def filter_outliers(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return df[df['Distance_to_Mean'] <= threshold]\n",
    "\n",
    "# Apply the filtering function for each cluster\n",
    "df_no_outliers = df_merged_100_35.groupby('Cluster').apply(filter_outliers).reset_index(drop=True)\n",
    "\n",
    "# Step 7: Drop unnecessary columns if needed (like 'x' and 'y' if only the distance matters)\n",
    "df_no_outliers_cleaned_100_35 = df_no_outliers.drop(columns=['x', 'y', 'x_mean', 'y_mean'])\n",
    "\n",
    "# Step 8: Check the size of the resulting dataframe\n",
    "print(f\"Original DataFrame size: {df_merged_100_35.shape}\")\n",
    "print(f\"DataFrame size after removing outliers: {df_no_outliers_cleaned_100_35.shape}\")\n",
    "\n",
    "# Display the final dataframe to the user\n",
    "df_no_outliers_cleaned_100_35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by 'Cluster'\n",
    "clusters_grouped_100_35 = df_no_outliers_cleaned_100_35.groupby('Cluster')\n",
    "\n",
    "# Create a dictionary to store arrays for each cluster's centroids\n",
    "clusters_centroids_100_35 = {}\n",
    "\n",
    "# Loop through each group (cluster) and store the centroids in arrays\n",
    "for cluster, group in clusters_grouped_100_35:\n",
    "    # Extract centroids (x, y) as a NumPy array\n",
    "    centroids_array = np.array(group['Centroid Coord'].tolist())  # Assuming 'Centroid Coord' contains [x, y] pairs\n",
    "    clusters_centroids_100_35[cluster] = centroids_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the size of each cluster\n",
    "cluster_sizes = {cluster: len(centroids) for cluster, centroids in clusters_centroids_100_35.items()}\n",
    "\n",
    "# Print the size of each cluster\n",
    "for cluster, size in cluster_sizes.items():\n",
    "    print(f\"Cluster {cluster} has {size} centroids considered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to verify that if it is fine to have all clusters with the same number of centroids after filtering out outliers. This must be due to:\n",
    "- The Distance Distributions are Likely Very Similar\n",
    "- Uniform Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each cluster and plot the distribution of distances\n",
    "for cluster, group in clusters_grouped_100_35:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(group['Distance_to_Mean'], bins=10, edgecolor='black')\n",
    "    plt.title(f'Cluster {cluster}: Distance to Mean Distribution')\n",
    "    plt.xlabel('Distance to Mean')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentile threshold per cluster check\n",
    "def check_percentiles(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return threshold\n",
    "\n",
    "# Function applied to each cluster and print the result\n",
    "for cluster, group in clusters_grouped_100_35:\n",
    "    threshold = check_percentiles(group)\n",
    "    print(f\"Cluster {cluster}: 90th percentile threshold = {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster, calculate the 70th percentile of distances and filter accordingly\n",
    "for cluster, group in clusters_grouped_100_35:\n",
    "    # Calculate the 70th percentile threshold for the current cluster\n",
    "    threshold = np.percentile(group['Distance_to_Mean'], 70)\n",
    "    \n",
    "    # Filter centroids based on the 70th percentile\n",
    "    filtered_group = group[group['Distance_to_Mean'] <= threshold]\n",
    "    \n",
    "    # Print the size of the group before and after filtering\n",
    "    print(f\"Cluster {cluster}: Original size = {len(group)}, Filtered size = {len(filtered_group)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Distance matrix n= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Mean matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance matrix**: elemnt d_{ij} has the distance between the center of cluster i and cluster j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store distance matrices for each run\n",
    "distance_matrices_100_35 = []\n",
    "\n",
    "# Iterate over all runs and calculate the distance matrix for each run\n",
    "for run_centroids in kmeans_centroids_100_35:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix_100_35 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    distance_matrices_100_35.append(distance_matrix_100_35)\n",
    "\n",
    "# Convert the list of distance matrices to a numpy array (35 runs, 10x10 distance matrices)\n",
    "distance_matrices_100_35 = np.array(distance_matrices_100_35)\n",
    "\n",
    "# Calculate the mean distance matrix across all runs\n",
    "mean_distance_matrix_100_35 = np.mean(distance_matrices_100_35, axis=0)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_100_35 = (mean_distance_matrix_100_35 - np.min(mean_distance_matrix_100_35)) / (np.max(mean_distance_matrix_100_35) - np.min(mean_distance_matrix_100_35))\n",
    "\n",
    "# Plot of the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=100)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_100_all_runs.npy', distance_matrices_100_35)\n",
    "np.save('mean_distance_matrix_neighbors_100_35.npy', mean_distance_matrix_100_35)\n",
    "np.save('normalized_mean_distance_matrix_100_35.npy', normalized_mean_distance_matrix_100_35)\n",
    "# Mean distance matrix\n",
    "print(f\"Mean distance matrix across all runs:\\n{mean_distance_matrix_100_35}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_distance_matrix_100_35= np.load(f'mean_distance_matrix_neighbors_100_35.npy')\n",
    "# mean_distance_matrix_100_35=np.round(mean_distance_matrix_100_35,3)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_100_35 = (mean_distance_matrix_100_35 - np.min(mean_distance_matrix_100_35)) / (np.max(mean_distance_matrix_100_35) - np.min(mean_distance_matrix_100_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_100_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_100_35,3))\n",
    "np.save('G_100_35.npy', G_100_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_100_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_100_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_100_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_100_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_100_35 = nx.minimum_spanning_tree(G_100_35)\n",
    "np.save(\"mst_100_35.npy\", mst_100_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_100_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_100_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_100_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_100_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST - n_neighbors=100\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Std. dev. Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the pairwise distance matrix for the standard deviations\n",
    "distance_matrix_std_100_35 = cdist(centroid_std_100_35, centroid_std_100_35, metric='euclidean')\n",
    "\n",
    "# Normalize the distance matrix\n",
    "normalized_distance_matrix_std_100_35 = (distance_matrix_std_100_35 - np.min(distance_matrix_std_100_35)) / (np.max(distance_matrix_std_100_35) - np.min(distance_matrix_std_100_35))\n",
    "\n",
    "# Visualize the normalized distance matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(normalized_distance_matrix_std_100_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=100)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrix for later analysis\n",
    "np.save(\"distance_matrix_std_100_35.npy\", distance_matrix_std_100_35)\n",
    "np.save(\"normalized_distance_matrix_std_100_35.npy\", normalized_distance_matrix_std_100_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_std_100_35 = nx.from_numpy_array(np.round(normalized_distance_matrix_std_100_35,3))\n",
    "np.save('G_std_100_35', G_std_100_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_std_100_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_std_100_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_std_100_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_std_100_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_std_100_35 = nx.minimum_spanning_tree(G_std_100_35)\n",
    "np.save('mst_std_100_35.npy', mst_std_100_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos_std_100_35 = nx.spring_layout(mst_std_100_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_std_100_35, pos_std_100_35, with_labels=True, node_color='lightyellow', edge_color='green', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels_std_100_35 = nx.get_edge_attributes(mst_std_100_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_100_35, pos_std_100_35, edge_labels=edge_labels_std_100_35, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST Std. Deviation - n_neighbors=100\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create heatmaps\n",
    "def plot_heatmaps_side_by_side(matrices, titles, figsize=(16, 8), cmap=\"viridis\", annot=True):\n",
    "    \"\"\"\n",
    "    Plots multiple heatmaps side by side for given matrices and titles.\n",
    "\n",
    "    Args:\n",
    "        matrices (list): List of 2D matrices to plot as heatmaps.\n",
    "        titles (list): List of titles corresponding to each matrix.\n",
    "        figsize (tuple): Size of the entire figure (default: (16, 8)).\n",
    "        cmap (str): Color map to use for all heatmaps (default: \"viridis\").\n",
    "        annot (bool): Whether to annotate cells with their values (default: True).\n",
    "    \"\"\"\n",
    "    n = len(matrices)  # Number of heatmaps\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "\n",
    "    for i, (matrix, title) in enumerate(zip(matrices, titles)):\n",
    "        sns.heatmap(matrix, annot=annot, cmap=cmap, fmt=\".2f\", linewidths=0.5, ax=axes[i])\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_xlabel(\"Cluster\")\n",
    "        axes[i].set_ylabel(\"Cluster\" if i == 0 else \"\")  # Only label y-axis for the first plot\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the two heatmaps\n",
    "plot_heatmaps_side_by_side(\n",
    "    matrices=[\n",
    "        normalized_distance_matrix_std_100_35,\n",
    "        normalized_mean_distance_matrix_100_35\n",
    "    ],\n",
    "    titles=[\n",
    "        \"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=100)\",\n",
    "        \"Normalized Mean Distance Matrix (k=10, n_neighbors=190)\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for \"low\" values\n",
    "threshold = 0.6\n",
    "\n",
    "# Identify pairs of clusters with low values in both matrices\n",
    "low_low_pairs = []\n",
    "for i in range(normalized_mean_distance_matrix_100_35.shape[0]):\n",
    "    for j in range(normalized_mean_distance_matrix_100_35.shape[1]):\n",
    "        if i != j:  # Skip diagonal\n",
    "            mean_value = normalized_mean_distance_matrix_100_35[i, j]\n",
    "            std_value = normalized_distance_matrix_std_100_35[i, j]\n",
    "            if mean_value < threshold and std_value < threshold:\n",
    "                low_low_pairs.append((i, j, mean_value, std_value))\n",
    "\n",
    "# Display the results\n",
    "for pair in low_low_pairs:\n",
    "    print(f\"Clusters {pair[0]} and {pair[1]}: Mean Distance = {pair[2]:.2f}, Std Distance = {pair[3]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.6 is the lowest threshold so far.\n",
    "\n",
    "Depending on the goal of the analysis we can think of it as:\n",
    "- If the aim is to identify the strongest relationships between clusters, a lower threshold would make more sense.\n",
    "- If we want to explore the broader connections, then it is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Replace with your cluster pairs from low_low_pairs\n",
    "low_low_pairs = [(0, 8, 0.59, 0.55), (3, 8, 0.59, 0.25), (7, 8, 0.58, 0.24)]\n",
    "\n",
    "# UMAP projections and cluster labels (replace with your actual data)\n",
    "umap_projections = np.load(\"umap_projections_neighbors_100.npy\")\n",
    "kmeans_labels = np.load(\"kmeans_labels_list_100_35.npy\")  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Function to plot clusters\n",
    "def plot_clusters(umap_projection, labels, cluster_pair, run_idx):\n",
    "    cluster_a, cluster_b = cluster_pair\n",
    "    points_a = umap_projection[labels == cluster_a]\n",
    "    points_b = umap_projection[labels == cluster_b]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(points_a[:, 0], points_a[:, 1], color=\"blue\", label=f\"Cluster {cluster_a}\", alpha=0.6)\n",
    "    plt.scatter(points_b[:, 0], points_b[:, 1], color=\"orange\", label=f\"Cluster {cluster_b}\", alpha=0.6)\n",
    "    plt.title(f\"Run {run_idx}: Cluster {cluster_a} vs. Cluster {cluster_b}\")\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze each cluster pair\n",
    "for cluster_pair in low_low_pairs:\n",
    "    cluster_a, cluster_b = cluster_pair[0], cluster_pair[1]\n",
    "    print(f\"Analyzing Cluster Pair: {cluster_a} and {cluster_b}\")\n",
    "    \n",
    "    # For simplicity, visualize them in a specific UMAP run (e.g., the first run)\n",
    "    run_idx = 0  # Use the first run for visualization\n",
    "    plot_clusters(umap_projections[run_idx], kmeans_labels[run_idx], (cluster_a, cluster_b), run_idx)\n",
    "\n",
    "    # Calculate additional statistics if needed\n",
    "    distances_a_to_b = np.linalg.norm(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a].mean(axis=0) - \n",
    "                                      umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b].mean(axis=0))\n",
    "    print(f\"Mean Centroid Distance (Run {run_idx}): {distances_a_to_b:.2f}\")\n",
    "\n",
    "    # Variability comparison\n",
    "    cluster_a_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a], axis=0)\n",
    "    cluster_b_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b], axis=0)\n",
    "    print(f\"Cluster {cluster_a} Std Dev: {cluster_a_std}\")\n",
    "    print(f\"Cluster {cluster_b} Std Dev: {cluster_b_std}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 1 and Cluster 8** have a moderate spatial relationship with visible overlap in the UMAP space. Their differing variability patterns suggest distinct structures, but the overlap points might represent shared features or transitions between the clusters.\n",
    "The large spatial separation between their centroids suggests they represent distinct structures or classes in the data.\n",
    "\n",
    "**Cluster 0 and Cluster 9** 9 appears more compact and stable, while Cluster 0 is larger and more variable.\n",
    "Their distinct regions in the UMAP space and differing standard deviations reinforce their meaningful separation.\n",
    "Insights from Variability:\n",
    "\n",
    "The variability of Cluster 0 could indicate sensitivity to UMAP parameters or noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interval of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_100_35 = distance_matrix_std_100_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_100_35 = z_score * sem_matrix_100_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_100_35 = mean_distance_matrix_100_35 - margin_of_error_matrix_100_35\n",
    "upper_limit_intconf_matrix_100_35 = mean_distance_matrix_100_35 + margin_of_error_matrix_100_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_100_35 = np.maximum(lower_limit_intconf_matrix_100_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_100_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_100_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_100_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_100_35.npy', lower_limit_intconf_matrix_100_35)\n",
    "np.save('upper_limit_intconf_matrix_100_35.npy', upper_limit_intconf_matrix_100_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_100_35 = normalize_matrix(lower_limit_intconf_matrix_100_35)\n",
    "norm_upper_limit_intconf_matrix_100_35 = normalize_matrix(upper_limit_intconf_matrix_100_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_100_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=100)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=100)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_100_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=100)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_100_35, \"MST - Mean Distances\", axes[0], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_100_35, \"MST - Lower Limit\", axes[1], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_100_35, \"MST - Upper Limit\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intra class evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "radius = 0.5\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_50_35 = []\n",
    "neighbor_counts_50_35 = []\n",
    "kmeans_labels_list_50_35 = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_50_35):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_50_35 = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_50_35.append(kmeans_labels_5_35)\n",
    "    \n",
    "    run_max_distances_50_35 = []\n",
    "    run_neighbor_counts_50_35 = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_50_35 == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_50_35 = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_50_35.append(max_distance_50_35)\n",
    "        \n",
    "        # Calculate number of neighbors within the radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_50_35 = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_50_35.append(neighbors_within_radius_50_35)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_50_35.append(run_max_distances_50_35)\n",
    "    neighbor_counts_50_35.append(run_neighbor_counts_50_35)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_50_35 = np.array(max_distances_50_35)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_50_35 = np.array(neighbor_counts_50_35)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_50_35 = np.array(kmeans_labels_list_50_35)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_5_35.npy', max_distances_50_35)\n",
    "np.save('neighbor_counts_within_radius_5_35.npy', neighbor_counts_50_35)\n",
    "np.save('kmeans_labels_list_5_35 .npy', kmeans_labels_list_50_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_50_35)\n",
    "print(\"\\nNeighbor counts within radius for each run and each cluster:\\n\", neighbor_counts_50_35)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_distances_100_35_d= np.load(f'max_intra_cluster_distances_dynamic_100_35.npy')\n",
    "neighbor_counts_100_35_d= np.load(f'neighbor_counts_within_dynamic_radius_100_35.npy')\n",
    "kmeans_labels_list_100_35_d= np.load(f'kmeans_labels_list_100_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to track the minimum distance and corresponding clusters\n",
    "overall_min_distance_100 = float('inf')\n",
    "min_distance_clusters_100 = None\n",
    "min_distance_run_idx_100= None\n",
    "\n",
    "for run_idx, run_centroids in enumerate(kmeans_centroids_100_35):\n",
    "    # Compute pairwise distances between centroids\n",
    "    pairwise_distances_100 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    \n",
    "    # Get the indices of the minimum non-zero distance\n",
    "    np.fill_diagonal(pairwise_distances_100, np.inf)  # Ignore zero distances (self-comparisons)\n",
    "    min_distance = np.min(pairwise_distances_100)\n",
    "    if min_distance < overall_min_distance_100:\n",
    "        overall_min_distance_100 = min_distance\n",
    "        # Find the indices of the clusters corresponding to the minimum distance\n",
    "        cluster_indices = np.unravel_index(np.argmin(pairwise_distances_100), pairwise_distances_100.shape)\n",
    "        min_distance_clusters_100 = cluster_indices\n",
    "        min_distance_run_idx_100 = run_idx\n",
    "\n",
    "# Calculate dynamic radius\n",
    "dynamic_radius_100_35 = overall_min_distance_100 / 2\n",
    "print(f\"Dynamic radius: {dynamic_radius_100_35}\")\n",
    "print(f\"Minimum distance: {overall_min_distance_100}\")\n",
    "print(f\"Clusters contributing to minimum distance: {min_distance_clusters_100}\")\n",
    "print(f\"Run index: {min_distance_run_idx_100}\")\n",
    "\n",
    "# Save dynamic radius\n",
    "np.save('dynamic_radius_results_100_35.npy', dynamic_radius_100_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "\n",
    "# Dynamic radius, previously calculated\n",
    "radius = dynamic_radius_100_35\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_100_35_d = []\n",
    "neighbor_counts_100_35_d = []\n",
    "kmeans_labels_list_100_35_d = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_100_35):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_100_35_d = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_100_35_d.append(kmeans_labels_100_35_d)\n",
    "    \n",
    "    run_max_distances_100_35_d = []\n",
    "    run_neighbor_counts_100_35_d = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_100_35_d == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_100_35_d = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_100_35_d.append(max_distance_100_35_d)\n",
    "        \n",
    "        # Calculate number of neighbors within the dynamic radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_100_35_d = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_100_35_d.append(neighbors_within_radius_100_35_d)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_100_35_d.append(run_max_distances_100_35_d)\n",
    "    neighbor_counts_100_35_d.append(run_neighbor_counts_100_35_d)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_100_35_d = np.array(max_distances_100_35_d)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_100_35_d = np.array(neighbor_counts_100_35_d)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_100_35_d = np.array(kmeans_labels_list_100_35_d)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_dynamic_100_35.npy', max_distances_100_35_d)\n",
    "np.save('neighbor_counts_within_dynamic_radius_100_35.npy', neighbor_counts_100_35_d)\n",
    "np.save('kmeans_labels_list_100_35.npy', kmeans_labels_list_100_35_d)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_100_35_d)\n",
    "print(\"\\nNeighbor counts within dynamic radius for each run and each cluster:\\n\", neighbor_counts_100_35_d)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot neighbor counts for each cluster across all runs\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Iterate through each cluster\n",
    "for cluster_idx in range(neighbor_counts_100_35_d.shape[1]):\n",
    "    plt.subplot(2, 5, cluster_idx + 1)  # Create subplots for 10 clusters (2 rows, 5 columns)\n",
    "    plt.plot(range(1, neighbor_counts_100_35_d.shape[0] + 1), neighbor_counts_100_35_d[:, cluster_idx], marker='o')\n",
    "    plt.title(f'Cluster {cluster_idx}')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Neighbor Count')\n",
    "    plt.xticks(range(1, neighbor_counts_100_35_d.shape[0] + 1, 5))  # Show every 5th run on the x-axis for clarity\n",
    "    plt.grid(True)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.suptitle('N=100 Neighbor Counts per Cluster Across Runs', y=1.02, fontsize=16)  # Add a global title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and max values across clusters for each run\n",
    "mean_neighbors_100 = np.mean(neighbor_counts_100_35_d, axis=1)  # Shape: (n_runs,)\n",
    "max_neighbors_100 = np.max(neighbor_counts_100_35_d, axis=1)    # Shape: (n_runs,)\n",
    "\n",
    "# Compute trend lines for mean and max\n",
    "runs = np.arange(1, len(mean_neighbors_100) + 1)\n",
    "mean_slope, mean_intercept, _, _, _ = linregress(runs, mean_neighbors_100)\n",
    "max_slope, max_intercept, _, _, _ = linregress(runs, max_neighbors_100)\n",
    "\n",
    "# Calculate trend line values\n",
    "mean_trend_100 = mean_slope * runs + mean_intercept\n",
    "max_trend_100 = max_slope * runs + max_intercept\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Mean neighbor counts\n",
    "plt.plot(runs, mean_neighbors_100, label='Mean Neighbor Count', marker='o', color='blue')\n",
    "\n",
    "# Max neighbor counts\n",
    "plt.plot(runs, max_neighbors_100, label='Max Neighbor Count', marker='s', color='orange')\n",
    "\n",
    "# Trend lines\n",
    "plt.plot(runs, mean_trend_100, linestyle='--', color='green',label='Mean Trend Line')\n",
    "plt.plot(runs, max_trend_100, linestyle='--', color='green', label='Max Trend Line')\n",
    "\n",
    "# Add labels, legend, and title\n",
    "plt.title('N=100 Neighbor Counts Across Runs (Mean vs. Max with Trend Lines)', fontsize=16)\n",
    "plt.xlabel('Run', fontsize=12)\n",
    "plt.ylabel('Neighbor Count', fontsize=12)\n",
    "plt.xticks(range(1, len(mean_neighbors_100) + 1, 5))  # Show every 5th run for readability\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(f'neighbor_counts_plot_n_100_35.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean, min, and max across clusters for each run\n",
    "mean_intra_cluster_distances = np.mean(max_distances_100_35_d, axis=1)  # Mean across clusters\n",
    "min_intra_cluster_distances = np.min(max_distances_100_35_d, axis=1)    # Min across clusters\n",
    "max_intra_cluster_distances = np.max(max_distances_100_35_d, axis=1)    # Max across clusters\n",
    "\n",
    "mean_neighbor_counts = np.mean(neighbor_counts_100_35_d, axis=1)  # Mean across clusters\n",
    "min_neighbor_counts = np.min(neighbor_counts_100_35_d, axis=1)    # Min across clusters\n",
    "max_neighbor_counts = np.max(neighbor_counts_100_35_d, axis=1)    # Max across clusters\n",
    "\n",
    "# Smoothing using rolling average (optional, if data is noisy)\n",
    "# window_size = 3\n",
    "# mean_intra_cluster_distances = np.convolve(mean_intra_cluster_distances, np.ones(window_size)/window_size, mode='valid')\n",
    "# mean_neighbor_counts = np.convolve(mean_neighbor_counts, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Plot Intra-Cluster Distances\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_intra_cluster_distances, label='Mean Intra-Cluster Distance', marker='o', color='blue')\n",
    "plt.fill_between(\n",
    "    range(len(mean_intra_cluster_distances)),\n",
    "    min_intra_cluster_distances,\n",
    "    max_intra_cluster_distances,\n",
    "    color='blue',\n",
    "    alpha=0.2,\n",
    "    label='Range (Min-Max)'\n",
    ")\n",
    "plt.title('Intra-Cluster Distance Statistics Across Runs (n_neighbors=100)', fontsize=16)\n",
    "plt.xlabel('Run', fontsize=12)\n",
    "plt.ylabel('Distance', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Neighbor Counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_neighbor_counts, label='Mean Neighbor Count', marker='o', color='orange')\n",
    "plt.fill_between(\n",
    "    range(len(mean_neighbor_counts)),\n",
    "    min_neighbor_counts,\n",
    "    max_neighbor_counts,\n",
    "    color='orange',\n",
    "    alpha=0.2,\n",
    "    label='Range (Min-Max)'\n",
    ")\n",
    "plt.title('Neighbor Counts Statistics Within Dynamic Radius Across Runs (n_neighbors=100)', fontsize=16)\n",
    "plt.xlabel('Run', fontsize=12)\n",
    "plt.ylabel('Neighbor Count', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the MNIST dataset to a flattened format suitable for UMAP\n",
    "x_train_flattened = np.array([np.array(img).flatten() for img in x_train])\n",
    "x_test_flattened = np.array([np.array(img).flatten() for img in x_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_5_01_35= np.load(f'umap_projections_5_01_35.npy')\n",
    "mean_projection_5_01_35= np.load(f'mean_projection_5_01_35.npy')\n",
    "std_projection_5_01_35= np.load(f'std_projection_5_01_35.npy')\n",
    "lower_limit_intconf_matrix_5_01_35= np.load(f'lower_limit_intconf_matrix_5_01_35.npy')\n",
    "upper_limit_intconf_matrix_5_01_35= np.load(f'upper_limit_intconf_matrix_5_01_35.npy')\n",
    "distance_matrices_5_01_35=np.load(f'distance_matrices_neighbors_5_01_35.npy')\n",
    "mean_distance_matrix_5_01_35=np.load(f'mean_distance_matrix_neighbors_5_01_35.npy')\n",
    "norm_lower_limit_intconf_matrix_5_01_35=np.load(f'norm_lower_limit_intconf_matrix_5_01_35.npy')\n",
    "norm_upper_limit_intconf_matrix_5_01_35=np.load(f'norm_upper_limit_intconf_matrix_5_01_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 5\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_5_01_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_5_01_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_5_01_35 = np.array(umap_projections_5_01_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_5_01_35 = np.mean(umap_projections_5_01_35, axis=0)\n",
    "std_projection_5_01_35 = np.std(umap_projections_5_01_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_5_01_35.npy', umap_projections_5_01_35)\n",
    "np.save('mean_projection_5_01_35.npy', mean_projection_5_01_35)\n",
    "np.save('std_projection_5_01_35.npy', std_projection_5_01_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_5_01_35'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_5_01_35 = np.sqrt(np.sum((umap_projections_5_01_35 - mean_projection_5_01_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_5_01_35, axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"y_train type: {type(y_train)}\")  # Should now be <class 'numpy.ndarray'>\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_5_01_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_5_01 = np.zeros((n_runs, n_clusters, umap_projections_5_01_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_5_01_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_5_01[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_5_01 = np.zeros(10)\n",
    "std_dev_y_5_01 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_5_01[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_5_01[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_5_01[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_5_01[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_5_01)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_5_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids for each valid run\n",
    "cluster_centroids_per_run_kmeans = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids using KMeans\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_5_01_35[run]  # Shape: (n_samples, 2)\n",
    "    \n",
    "    # Perform KMeans clustering to calculate centroids\n",
    "    kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "    kmeans.fit(projections)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    # Store centroids for this run\n",
    "    cluster_centroids_per_run_kmeans.append(centroids)\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_5_01_35_kmeans = []\n",
    "for centroids in cluster_centroids_per_run_kmeans:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_5_01_35_kmeans.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_5_01_35_kmeans = np.array(distance_matrices_5_01_35_kmeans)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_5_01_35_kmeans = np.mean(distance_matrices_5_01_35_kmeans, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_5_01_35_kmeans = (\n",
    "    mean_distance_matrix_5_01_35_kmeans - np.min(mean_distance_matrix_5_01_35_kmeans)\n",
    ") / (np.max(mean_distance_matrix_5_01_35_kmeans) - np.min(mean_distance_matrix_5_01_35_kmeans))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_01_35_kmeans, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (KMeans, k=10, n_neighbors=5)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_5_01_35_kmeans.npy', distance_matrices_5_01_35_kmeans)\n",
    "np.save('mean_distance_matrix_neighbors_5_01_35_kmeans.npy', mean_distance_matrix_5_01_35_kmeans)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs (KMeans):\\n{mean_distance_matrix_5_01_35_kmeans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_5_01_35[run]  # Shape: (n_samples, 2)\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_5_01_35 = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_5_01_35.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_5_01_35 = np.array(distance_matrices_5_01_35)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_5_01_35 = np.mean(distance_matrices_5_01_35, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_5_01_35 = (mean_distance_matrix_5_01_35 - np.min(mean_distance_matrix_5_01_35)) / (np.max(mean_distance_matrix_5_01_35) - np.min(mean_distance_matrix_5_01_35))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=5)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_5_01_35.npy', distance_matrices_5_01_35)\n",
    "np.save('mean_distance_matrix_neighbors_5_01_35.npy', mean_distance_matrix_5_01_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_5_01_35}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_5_01_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_5_01_35,3))\n",
    "np.save('G_5_01_35.npy',G_5_01_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_5_01_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_5_01_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_5_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_5_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_5_01_35 = nx.minimum_spanning_tree(G_5_01_35)\n",
    "np.save('mst_5_01_35.npy', mst_5_01_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_5_01_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_5_01_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_5_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_5_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=5, min_dist=0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. MST UMAP**\n",
    "\n",
    "Structure:\n",
    "\n",
    "The graph represents a Minimum Spanning Tree (MST) based on the normalized mean distance matrix derived purely from UMAP projections.\n",
    "Each node corresponds to a cluster, and the edges represent the shortest path between clusters with the edge weight being the normalized distance.\n",
    "Insights:\n",
    "\n",
    "The clusters are more tightly interconnected, with nodes such as 3, 5, and 7 serving as central points, connecting multiple clusters.\n",
    "The distances (weights on the edges) are relatively lower, indicating smaller normalized pairwise distances between clusters.\n",
    "This suggests that the UMAP embedding preserves local neighborhood structures, where distances reflect proximity in the reduced-dimensional space.\n",
    "\n",
    "**2. MST UMAP + KMeans**\n",
    "\n",
    "Structure:\n",
    "\n",
    "This graph represents an MST derived from the combination of UMAP for dimensionality reduction and KMeans for clustering.\n",
    "KMeans introduces an additional layer of abstraction, grouping data into clusters and then calculating centroids for these clusters.\n",
    "Insights:\n",
    "\n",
    "The structure of this MST is more linear, with node 1 acting as a central hub connecting multiple clusters.\n",
    "The edge weights (distances) are consistently higher, suggesting that the clustering step has created more separation between cluster centroids compared to distances derived from UMAP alone.\n",
    "This linear structure likely reflects the influence of KMeans clustering, which tends to impose a more rigid partitioning of the data.\n",
    "\n",
    "**Comparison**\n",
    "\n",
    "Cluster Connectivity:\n",
    "\n",
    "UMAP: Shows a more decentralized network with multiple hubs (3, 5, 7).\n",
    "UMAP + KMeans: Displays a more centralized structure with one dominant hub (1).\n",
    "Edge Weights:\n",
    "\n",
    "UMAP: Lower edge weights suggest tighter groupings and shorter distances between clusters.\n",
    "UMAP + KMeans: Higher edge weights reflect the increased separation introduced by the clustering process.\n",
    "Interpretation of Distances:\n",
    "\n",
    "UMAP: Distances directly reflect the UMAP embedding, which prioritizes local neighborhood preservation.\n",
    "UMAP + KMeans: Distances represent the centroids derived from KMeans clustering, which may distort or amplify separation compared to the original embedding.\n",
    "\n",
    "Flexibility vs. Structure:\n",
    "UMAP: More flexible, as it is based on pairwise distances without enforcing strict cluster boundaries.\n",
    "UMAP + KMeans: More structured due to KMeans, which enforces boundaries and distances are centroid-based.\n",
    "\n",
    "**Conclusion**\n",
    "The MST UMAP graph provides a more natural representation of cluster relationships, capturing the local proximities and overlaps between clusters.\n",
    "The MST UMAP + KMeans graph introduces a more structured interpretation of the data, but at the cost of potentially oversimplifying relationships between clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_5_01_35 = np.std(distance_matrices_5_01_35, axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_5_01_35.npy\", distance_matrix_std_5_01_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_01_35):\\n\", distance_matrix_std_5_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_5_01_35 = distance_matrix_std_5_01_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_5_01_35 = z_score * sem_matrix_5_01_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_5_01_35 = mean_distance_matrix_5_01_35 - margin_of_error_matrix_5_01_35\n",
    "upper_limit_intconf_matrix_5_01_35 = mean_distance_matrix_5_01_35 + margin_of_error_matrix_5_01_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_5_01_35 = np.maximum(lower_limit_intconf_matrix_5_01_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_5_01_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_5_01_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_5_01_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_5_01_35.npy', lower_limit_intconf_matrix_5_01_35)\n",
    "np.save('upper_limit_intconf_matrix_5_01_35.npy', upper_limit_intconf_matrix_5_01_35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interval of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_5_01_35 = normalize_matrix(lower_limit_intconf_matrix_5_01_35)\n",
    "norm_upper_limit_intconf_matrix_5_01_35 = normalize_matrix(upper_limit_intconf_matrix_5_01_35)\n",
    "np.save('norm_lower_limit_intconf_matrix_5_01_35.npy', norm_lower_limit_intconf_matrix_5_01_35)\n",
    "np.save('norm_upper_limit_intconf_matrix_5_01_35.npy', norm_upper_limit_intconf_matrix_5_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_5_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=5, min_dist=0.1)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=5, min_dist=0.1)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_5_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=5, min_dist=0.1)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_5_01_35, \"UMAP MST - Mean Distances - n_neighbors=5 min_dist = 0.1\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_5_01_35, \"UMAP MST - Lower Limit - n_neighbors=5 min_dist = 0.1\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_5_01_35, \"UMAP MST - Upper Limit - n_neighbors=5 min_dist = 0.1\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Could be nice to show but, analysis at the end not used\n",
    "# Calculate the standard deviation of distances across runs for each sample\n",
    "std_distances_per_point = np.std(distances_to_mean_5_01_35, axis=0)  # Shape: (n_samples,)\n",
    "\n",
    "# Plot the distribution of standard deviations\n",
    "plt.hist(std_distances_per_point, bins=50)\n",
    "plt.xlabel('Standard Deviation of Distances')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Distance Variability Across Runs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above graph show that distances vary across runs and it might be necessary to increase UMAP parameters or fixed random_state.\n",
    "From histogram most points have low standard deviation, which suggests the variability is relatively low across runs for a majority of points.However, there is a long tail where some points exhibit much higher variability, which could be influencing the filtering logic in stricter approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FINAL VERSION - IN USED ###\n",
    "\n",
    "# Assuming `umap_projections_5_01_35` has the shape (n_runs, n_points, 2)\n",
    "# # and `y_train` has the labels for the dataset.\n",
    "\n",
    "# # Step 1: Calculate Cluster Centers for Full Dataset\n",
    "# n_clusters = 10  # Number of clusters (digits 0-9)\n",
    "# cluster_centers_full = []\n",
    "\n",
    "# for run_idx, x_umap in enumerate(umap_projections_5_01_35):  # Iterate over all runs\n",
    "#     cluster_centers_run = []\n",
    "#     for label in np.unique(y_train):  # Iterate over all labels\n",
    "#         cluster_points = x_umap[y_train == label]\n",
    "#         if len(cluster_points) > 0:\n",
    "#             cluster_center = np.mean(cluster_points, axis=0)\n",
    "#             cluster_centers_run.append(cluster_center)\n",
    "#     cluster_centers_full.append(np.array(cluster_centers_run))\n",
    "\n",
    "# cluster_centers_full = np.array(cluster_centers_full)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "# # Step 2: Calculate Dynamic Radius Per Cluster\n",
    "# dynamic_radii_full = []\n",
    "\n",
    "# for run_idx, x_umap in enumerate(umap_projections_5_01_35):\n",
    "#     radii_run = []\n",
    "#     for label, cluster_center in zip(np.unique(y_train), cluster_centers_full[run_idx]):\n",
    "#         cluster_points = x_umap[y_train == label]\n",
    "#         if len(cluster_points) > 0:\n",
    "#             distances_to_center = np.linalg.norm(cluster_points - cluster_center, axis=1)\n",
    "#             dynamic_radius = np.mean(distances_to_center)  # Use the mean distance\n",
    "#             radii_run.append(dynamic_radius)\n",
    "#     dynamic_radii_full.append(radii_run)\n",
    "\n",
    "# dynamic_radii_full = np.array(dynamic_radii_full)  # Shape: (n_runs, n_clusters)\n",
    "\n",
    "# # Step 3: Count Neighbors Within Radius\n",
    "# neighbor_counts_full = []\n",
    "\n",
    "# for run_idx, x_umap in enumerate(umap_projections_5_01_35):\n",
    "#     counts_run = []\n",
    "#     for label, cluster_center, radius in zip(np.unique(y_train), cluster_centers_full[run_idx], dynamic_radii_full[run_idx]):\n",
    "#         distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "#         count = np.sum(distances_to_center <= radius)  # Count points within the radius\n",
    "#         counts_run.append(count)\n",
    "#     neighbor_counts_full.append(counts_run)\n",
    "\n",
    "# neighbor_counts_full = np.array(neighbor_counts_full)  # Shape: (n_runs, n_clusters)\n",
    "\n",
    "# # Step 4: Save Results\n",
    "# np.save(\"neighbor_counts_dynamic_radius_full.npy\", neighbor_counts_full)\n",
    "# np.save(\"dynamic_radii_full.npy\", dynamic_radii_full)\n",
    "# np.save(\"cluster_centers_full.npy\", cluster_centers_full)\n",
    "\n",
    "# # Step 5: Print Results for Verification\n",
    "# print(f\"Dynamic Radii (first run):\\n{dynamic_radii_full[0]}\")\n",
    "# print(f\"Neighbor Counts (first run):\\n{neighbor_counts_full[0]}\")\n",
    "# print(f\"Neighbor Counts Shape: {neighbor_counts_full.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptable Radius Final Version & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THE FINAL - USE THIS DON'T OVERTHINK\n",
    "# FOR min_dist = 0,1.\n",
    "\n",
    "# Function to calculate cluster metrics\n",
    "def calculate_cluster_metrics(umap_projections, y_labels, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Calculate average cluster radii and neighbor counts for each cluster over all runs.\n",
    "    \"\"\"\n",
    "    n_runs = len(umap_projections)  # Number of runs\n",
    "    cluster_centers_full = []\n",
    "    \n",
    "    # Step 1: Calculabte cluster centers for each run\n",
    "    for run_idx, x_umap in enumerate(umap_projections):\n",
    "        cluster_centers_run = []\n",
    "        for label in np.unique(y_labels):\n",
    "            cluster_points = x_umap[y_labels == label]\n",
    "            if len(cluster_points) > 0:\n",
    "                cluster_center = np.mean(cluster_points, axis=0)\n",
    "                cluster_centers_run.append(cluster_center)\n",
    "        cluster_centers_full.append(np.array(cluster_centers_run))\n",
    "    \n",
    "    cluster_centers_full = np.array(cluster_centers_full)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "    # Step 2: Calculate average radii for each cluster\n",
    "    radii_per_cluster = []\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        radii_cluster = []\n",
    "        for run_idx, x_umap in enumerate(umap_projections):\n",
    "            cluster_center = cluster_centers_full[run_idx][cluster_idx]\n",
    "            cluster_points = x_umap[y_labels == cluster_idx]\n",
    "            if len(cluster_points) > 0:\n",
    "                distances_to_center = np.linalg.norm(cluster_points - cluster_center, axis=1)\n",
    "                dynamic_radius = np.mean(distances_to_center)  # Mean distance to center\n",
    "                radii_cluster.append(dynamic_radius)\n",
    "        radii_per_cluster.append(np.mean(radii_cluster))  # Average radius across runs\n",
    "\n",
    "    # Step 3: Calculate neighbor counts for each cluster\n",
    "    neighbor_counts_full = []\n",
    "    for run_idx, x_umap in enumerate(umap_projections):\n",
    "        counts_run = []\n",
    "        for cluster_idx, cluster_center in enumerate(cluster_centers_full[run_idx]):\n",
    "            radius = radii_per_cluster[cluster_idx]  # Use the average radius\n",
    "            distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "            count = np.sum(distances_to_center <= radius)  # Count points within the radius\n",
    "            counts_run.append(count)\n",
    "        neighbor_counts_full.append(counts_run)\n",
    "\n",
    "    neighbor_counts_full = np.array(neighbor_counts_full)  # Shape: (n_runs, n_clusters)\n",
    "    average_neighbor_counts = np.mean(neighbor_counts_full, axis=0)  # Average across runs\n",
    "\n",
    "    return radii_per_cluster, average_neighbor_counts\n",
    "\n",
    "# Define n_neighbors values\n",
    "n_neighbors_values = [5, 10, 20, 30, 50, 100]\n",
    "results = []\n",
    "\n",
    "# Iterate over each n_neighbors value\n",
    "for n_neighbors in n_neighbors_values:\n",
    "    if n_neighbors == 5:\n",
    "        umap_projections = umap_projections_5_01_35\n",
    "    elif n_neighbors == 10:\n",
    "        umap_projections = umap_projections_10_01_35\n",
    "    elif n_neighbors == 20:\n",
    "        umap_projections = umap_projections_20_01_35\n",
    "    elif n_neighbors == 30:\n",
    "        umap_projections = umap_projections_30_01_35\n",
    "    elif n_neighbors == 50:\n",
    "        umap_projections = umap_projections_50_01_35\n",
    "    elif n_neighbors == 100:\n",
    "        umap_projections = umap_projections_100_01_35\n",
    "\n",
    "    # Calculate metrics\n",
    "    radii_per_cluster, average_neighbor_counts = calculate_cluster_metrics(umap_projections, y_train)\n",
    "\n",
    "    # Store results\n",
    "    for cluster_idx in range(len(radii_per_cluster)):\n",
    "        results.append({\n",
    "            \"N\": n_neighbors,\n",
    "            \"Cluster\": cluster_idx,\n",
    "            \"Radius\": np.round(radii_per_cluster[cluster_idx], 3),\n",
    "            \"Number of Neighbors\": np.round(average_neighbor_counts[cluster_idx], 0)\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save results for later use\n",
    "df_results.to_csv(\"radius_neighbor_analysis_merged_MinDist_01.csv\", index=False)\n",
    "\n",
    "# Pivot table for easy visualization\n",
    "pivot_table = df_results.pivot(index=\"Cluster\", columns=\"N\", values=[\"Radius\", \"Number of Neighbors\"])\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_neighbor_counts_across_runs(umap_projections_list, n_neighbors_values, y_labels, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Plot mean number of neighbors across runs for different n_neighbors values.\n",
    "    \"\"\"\n",
    "    neighbor_counts_avg_runs = []\n",
    "\n",
    "    for umap_projections in umap_projections_list:\n",
    "        # Calculate neighbor counts for each run\n",
    "        neighbor_counts_per_run = []\n",
    "        for run_idx, x_umap in enumerate(umap_projections):\n",
    "            cluster_centers = []\n",
    "            radii_per_cluster = []\n",
    "            for cluster_idx in range(n_clusters):\n",
    "                # Compute cluster center\n",
    "                cluster_points = x_umap[y_labels == cluster_idx]\n",
    "                if len(cluster_points) > 0:\n",
    "                    cluster_center = np.mean(cluster_points, axis=0)\n",
    "                    cluster_centers.append(cluster_center)\n",
    "\n",
    "                    # Compute dynamic radius for this cluster\n",
    "                    distances_to_center = np.linalg.norm(cluster_points - cluster_center, axis=1)\n",
    "                    dynamic_radius = np.mean(distances_to_center)\n",
    "                    radii_per_cluster.append(dynamic_radius)\n",
    "                else:\n",
    "                    radii_per_cluster.append(0)\n",
    "                    cluster_centers.append(np.array([0, 0]))\n",
    "\n",
    "            # Compute number of neighbors within radius for each cluster\n",
    "            neighbor_counts = []\n",
    "            for cluster_idx, cluster_center in enumerate(cluster_centers):\n",
    "                if radii_per_cluster[cluster_idx] > 0:  # Avoid empty clusters\n",
    "                    distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "                    neighbor_count = np.sum(distances_to_center <= radii_per_cluster[cluster_idx])\n",
    "                    neighbor_counts.append(neighbor_count)\n",
    "\n",
    "            # Store the mean neighbor count for this run\n",
    "            neighbor_counts_per_run.append(np.mean(neighbor_counts))\n",
    "        \n",
    "        neighbor_counts_avg_runs.append(neighbor_counts_per_run)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, counts in enumerate(neighbor_counts_avg_runs):\n",
    "        plt.plot(range(1, len(counts) + 1), counts, label=f'n_neighbors={n_neighbors_values[i]}', marker='o')\n",
    "\n",
    "    plt.xlabel(\"Run Index\")\n",
    "    plt.ylabel(\"Mean Number of Points\")\n",
    "    plt.title(\"Mean Number of Points Across Runs for min_dist = 0.1\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))  # Adjust legend position\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_neighbor_counts_across_runs(\n",
    "    umap_projections_list=[\n",
    "        umap_projections_5_01_35,\n",
    "        umap_projections_10_01_35,\n",
    "        umap_projections_20_01_35,\n",
    "        umap_projections_30_01_35,\n",
    "        umap_projections_50_01_35,\n",
    "        umap_projections_100_01_35\n",
    "    ],\n",
    "    n_neighbors_values=n_neighbors_values,\n",
    "    y_labels=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading .csv \n",
    "df_results_01 = pd.read_csv('radius_neighbor_analysis_merged_MinDist_01.csv')\n",
    "\n",
    "# Add a density column to df_results\n",
    "df_results_01['Density'] = df_results_01['Number of Neighbors'] / df_results_01['Radius']\n",
    "\n",
    "# Find the row with the maximum density\n",
    "max_density_row = df_results_01.loc[df_results_01['Density'].idxmax()]\n",
    "\n",
    "# Extract the cluster, n_neighbors, and maximum density\n",
    "max_density = max_density_row['Density']\n",
    "max_cluster = max_density_row['Cluster']\n",
    "max_n_neighbors = max_density_row['N']\n",
    "\n",
    "# Print the results\n",
    "print(f\"Highest Density: {max_density:.2f}\")\n",
    "print(f\"Cluster: {int(max_cluster)}\")\n",
    "print(f\"n_neighbors: {int(max_n_neighbors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR min_dist=0,0125.\n",
    "\n",
    "# Function to calculate cluster metrics\n",
    "def calculate_cluster_metrics(umap_projections, y_labels, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Calculate average cluster radii and neighbor counts for each cluster over all runs.\n",
    "    \"\"\"\n",
    "    n_runs = len(umap_projections)  # Number of runs\n",
    "    cluster_centers_full = []\n",
    "    \n",
    "    # Step 1: Calculate cluster centers for each run\n",
    "    for run_idx, x_umap in enumerate(umap_projections):\n",
    "        cluster_centers_run = []\n",
    "        for label in np.unique(y_labels):\n",
    "            cluster_points = x_umap[y_labels == label]\n",
    "            if len(cluster_points) > 0:\n",
    "                cluster_center = np.mean(cluster_points, axis=0)\n",
    "                cluster_centers_run.append(cluster_center)\n",
    "        cluster_centers_full.append(np.array(cluster_centers_run))\n",
    "    \n",
    "    cluster_centers_full = np.array(cluster_centers_full)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "    # Step 2: Calculate average radii for each cluster\n",
    "    radii_per_cluster = []\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        radii_cluster = []\n",
    "        for run_idx, x_umap in enumerate(umap_projections):\n",
    "            cluster_center = cluster_centers_full[run_idx][cluster_idx]\n",
    "            cluster_points = x_umap[y_labels == cluster_idx]\n",
    "            if len(cluster_points) > 0:\n",
    "                distances_to_center = np.linalg.norm(cluster_points - cluster_center, axis=1)\n",
    "                dynamic_radius = np.mean(distances_to_center)  # Mean distance to center\n",
    "                radii_cluster.append(dynamic_radius)\n",
    "        radii_per_cluster.append(np.mean(radii_cluster))  # Average radius across runs\n",
    "\n",
    "    # Step 3: Calculate neighbor counts for each cluster\n",
    "    neighbor_counts_full = []\n",
    "    for run_idx, x_umap in enumerate(umap_projections):\n",
    "        counts_run = []\n",
    "        for cluster_idx, cluster_center in enumerate(cluster_centers_full[run_idx]):\n",
    "            radius = radii_per_cluster[cluster_idx]  # Use the average radius\n",
    "            distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "            count = np.sum(distances_to_center <= radius)  # Count points within the radius\n",
    "            counts_run.append(count)\n",
    "        neighbor_counts_full.append(counts_run)\n",
    "\n",
    "    neighbor_counts_full = np.array(neighbor_counts_full)  # Shape: (n_runs, n_clusters)\n",
    "    average_neighbor_counts = np.mean(neighbor_counts_full, axis=0)  # Average across runs\n",
    "\n",
    "    return radii_per_cluster, average_neighbor_counts\n",
    "\n",
    "# Define n_neighbors values\n",
    "n_neighbors_values = [5, 10, 20, 30, 50, 100]\n",
    "results = []\n",
    "\n",
    "# Iterate over each n_neighbors value\n",
    "for n_neighbors in n_neighbors_values:\n",
    "    if n_neighbors == 5:\n",
    "        umap_projections = umap_projections_5_00125_35\n",
    "    elif n_neighbors == 10:\n",
    "        umap_projections = umap_projections_10_00125_35\n",
    "    elif n_neighbors == 20:\n",
    "        umap_projections = umap_projections_20_00125_35\n",
    "    elif n_neighbors == 30:\n",
    "        umap_projections = umap_projections_30_00125_35\n",
    "    elif n_neighbors == 50:\n",
    "        umap_projections = umap_projections_50_00125_35\n",
    "    elif n_neighbors == 100:\n",
    "        umap_projections = umap_projections_100_00125_35\n",
    "\n",
    "    # Calculate metrics\n",
    "    radii_per_cluster, average_neighbor_counts = calculate_cluster_metrics(umap_projections, y_train)\n",
    "\n",
    "    # Store results\n",
    "    for cluster_idx in range(len(radii_per_cluster)):\n",
    "        results.append({\n",
    "            \"N\": n_neighbors,\n",
    "            \"Cluster\": cluster_idx,\n",
    "            \"Radius\": np.round(radii_per_cluster[cluster_idx], 3),\n",
    "            \"Number of Neighbors\": np.round(average_neighbor_counts[cluster_idx], 0)\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save results for later use\n",
    "df_results.to_csv(\"radius_neighbor_analysis_merged_MinDist_00125.csv\", index=False)\n",
    "\n",
    "# Pivot table for easy visualization\n",
    "pivot_table = df_results.pivot(index=\"Cluster\", columns=\"N\", values=[\"Radius\", \"Number of Neighbors\"])\n",
    "print(pivot_table)\n",
    "\n",
    "# # Visualization of mean neighbor counts across runs\n",
    "# def plot_mean_neighbor_counts_across_runs(umap_projections_list, n_neighbors_values, y_labels):\n",
    "#     \"\"\"\n",
    "#     Plot mean number of neighbors across runs for different n_neighbors values.\n",
    "#     \"\"\"\n",
    "#     neighbor_counts_avg_runs = []\n",
    "    \n",
    "#     for umap_projections in umap_projections_list:\n",
    "#         # Calculate neighbor counts for each run\n",
    "#         neighbor_counts = []\n",
    "#         for run_idx, x_umap in enumerate(umap_projections):\n",
    "#             mean_neighbors = []\n",
    "#             for cluster_idx in np.unique(y_labels):\n",
    "#                 cluster_points = x_umap[y_labels == cluster_idx]\n",
    "#                 cluster_center = np.mean(cluster_points, axis=0)\n",
    "#                 distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "#                 mean_neighbors.append(np.mean(distances_to_center))\n",
    "#             neighbor_counts.append(np.mean(mean_neighbors))  # Mean across clusters for a run\n",
    "#         neighbor_counts_avg_runs.append(neighbor_counts)\n",
    "\n",
    "#     # Plot results\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     for i, counts in enumerate(neighbor_counts_avg_runs):\n",
    "#         plt.plot(range(1, len(counts) + 1), counts, label=f'n_neighbors={n_neighbors_values[i]}', marker='o')\n",
    "    \n",
    "#     plt.xlabel(\"Run Index\")\n",
    "#     plt.ylabel(\"Mean Number of Neighbors\")\n",
    "#     plt.title(\"Mean Number of Neighbors Across Runs\")\n",
    "#     plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))  # Adjust as needed\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "# # Call the visualization function\n",
    "# plot_mean_neighbor_counts_across_runs(\n",
    "#     umap_projections_list=[umap_projections_5_00125_35,umap_projections_10_00125_35,umap_projections_20_00125_35,umap_projections_30_00125_35, umap_projections_50_00125_35, umap_projections_100_00125_35],\n",
    "#     n_neighbors_values=n_neighbors_values,\n",
    "#     y_labels=y_train\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_neighbor_counts_across_runs(umap_projections_list, n_neighbors_values, y_labels, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Plot mean number of neighbors across runs for different n_neighbors values.\n",
    "    \"\"\"\n",
    "    neighbor_counts_avg_runs = []\n",
    "\n",
    "    for umap_projections in umap_projections_list:\n",
    "        # Calculate neighbor counts for each run\n",
    "        neighbor_counts_per_run = []\n",
    "        for run_idx, x_umap in enumerate(umap_projections):\n",
    "            cluster_centers = []\n",
    "            radii_per_cluster = []\n",
    "            for cluster_idx in range(n_clusters):\n",
    "                # Compute cluster center\n",
    "                cluster_points = x_umap[y_labels == cluster_idx]\n",
    "                if len(cluster_points) > 0:\n",
    "                    cluster_center = np.mean(cluster_points, axis=0)\n",
    "                    cluster_centers.append(cluster_center)\n",
    "\n",
    "                    # Compute dynamic radius for this cluster\n",
    "                    distances_to_center = np.linalg.norm(cluster_points - cluster_center, axis=1)\n",
    "                    dynamic_radius = np.mean(distances_to_center)\n",
    "                    radii_per_cluster.append(dynamic_radius)\n",
    "                else:\n",
    "                    radii_per_cluster.append(0)\n",
    "                    cluster_centers.append(np.array([0, 0]))\n",
    "\n",
    "            # Compute number of neighbors within radius for each cluster\n",
    "            neighbor_counts = []\n",
    "            for cluster_idx, cluster_center in enumerate(cluster_centers):\n",
    "                if radii_per_cluster[cluster_idx] > 0:  # Avoid empty clusters\n",
    "                    distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "                    neighbor_count = np.sum(distances_to_center <= radii_per_cluster[cluster_idx])\n",
    "                    neighbor_counts.append(neighbor_count)\n",
    "\n",
    "            # Store the mean neighbor count for this run\n",
    "            neighbor_counts_per_run.append(np.mean(neighbor_counts))\n",
    "        \n",
    "        neighbor_counts_avg_runs.append(neighbor_counts_per_run)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, counts in enumerate(neighbor_counts_avg_runs):\n",
    "        plt.plot(range(1, len(counts) + 1), counts, label=f'n_neighbors={n_neighbors_values[i]}', marker='o')\n",
    "\n",
    "    plt.xlabel(\"Run Index\")\n",
    "    plt.ylabel(\"Mean Number of Points\")\n",
    "    plt.title(\"Mean Number of Points Across Runs for min_dist = 0.0125\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))  # Adjust legend position\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_neighbor_counts_across_runs(\n",
    "    umap_projections_list=[\n",
    "        umap_projections_5_00125_35,\n",
    "        umap_projections_10_00125_35,\n",
    "        umap_projections_20_00125_35,\n",
    "        umap_projections_30_00125_35,\n",
    "        umap_projections_50_00125_35,\n",
    "        umap_projections_100_00125_35\n",
    "    ],\n",
    "    n_neighbors_values=n_neighbors_values,\n",
    "    y_labels=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading .csv \n",
    "df_results_00125 = pd.read_csv('radius_neighbor_analysis_merged_MinDist_00125.csv')\n",
    "\n",
    "# Add a density column to df_results\n",
    "df_results_00125['Density'] = df_results_00125['Number of Neighbors'] / df_results_00125['Radius']\n",
    "\n",
    "# Find the row with the maximum density\n",
    "max_density_row = df_results_00125.loc[df_results_00125['Density'].idxmax()]\n",
    "\n",
    "# Extract the cluster, n_neighbors, and maximum density\n",
    "max_density = max_density_row['Density']\n",
    "max_cluster = max_density_row['Cluster']\n",
    "max_n_neighbors = max_density_row['N']\n",
    "\n",
    "# Print the results\n",
    "print(f\"Highest Density: {max_density:.2f}\")\n",
    "print(f\"Cluster: {int(max_cluster)}\")\n",
    "print(f\"n_neighbors: {int(max_n_neighbors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For min_dist=0,8.\n",
    "\n",
    "# Function to calculate cluster metrics\n",
    "def calculate_cluster_metrics(umap_projections, y_labels, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Calculate average cluster radii and neighbor counts for each cluster over all runs.\n",
    "    \"\"\"\n",
    "    n_runs = len(umap_projections)  # Number of runs\n",
    "    cluster_centers_full = []\n",
    "    \n",
    "    # Step 1: Calculate cluster centers for each run\n",
    "    for run_idx, x_umap in enumerate(umap_projections):\n",
    "        cluster_centers_run = []\n",
    "        for label in np.unique(y_labels):\n",
    "            cluster_points = x_umap[y_labels == label]\n",
    "            if len(cluster_points) > 0:\n",
    "                cluster_center = np.mean(cluster_points, axis=0)\n",
    "                cluster_centers_run.append(cluster_center)\n",
    "        cluster_centers_full.append(np.array(cluster_centers_run))\n",
    "    \n",
    "    cluster_centers_full = np.array(cluster_centers_full)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "    # Step 2: Calculate average radii for each cluster\n",
    "    radii_per_cluster = []\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        radii_cluster = []\n",
    "        for run_idx, x_umap in enumerate(umap_projections):\n",
    "            cluster_center = cluster_centers_full[run_idx][cluster_idx]\n",
    "            cluster_points = x_umap[y_labels == cluster_idx]\n",
    "            if len(cluster_points) > 0:\n",
    "                distances_to_center = np.linalg.norm(cluster_points - cluster_center, axis=1)\n",
    "                dynamic_radius = np.mean(distances_to_center)  # Mean distance to center\n",
    "                radii_cluster.append(dynamic_radius)\n",
    "        radii_per_cluster.append(np.mean(radii_cluster))  # Average radius across runs\n",
    "\n",
    "    # Step 3: Calculate neighbor counts for each cluster\n",
    "    neighbor_counts_full = []\n",
    "    for run_idx, x_umap in enumerate(umap_projections):\n",
    "        counts_run = []\n",
    "        for cluster_idx, cluster_center in enumerate(cluster_centers_full[run_idx]):\n",
    "            radius = radii_per_cluster[cluster_idx]  # Use the average radius\n",
    "            distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "            count = np.sum(distances_to_center <= radius)  # Count points within the radius\n",
    "            counts_run.append(count)\n",
    "        neighbor_counts_full.append(counts_run)\n",
    "\n",
    "    neighbor_counts_full = np.array(neighbor_counts_full)  # Shape: (n_runs, n_clusters)\n",
    "    average_neighbor_counts = np.mean(neighbor_counts_full, axis=0)  # Average across runs\n",
    "\n",
    "    return radii_per_cluster, average_neighbor_counts\n",
    "\n",
    "# Define n_neighbors values\n",
    "n_neighbors_values = [5, 10, 20, 30, 50, 100]\n",
    "results = []\n",
    "\n",
    "# Iterate over each n_neighbors value\n",
    "for n_neighbors in n_neighbors_values:\n",
    "    if n_neighbors == 5:\n",
    "        umap_projections = umap_projections_5_08_35\n",
    "    elif n_neighbors == 10:\n",
    "        umap_projections = umap_projections_10_08_35\n",
    "    elif n_neighbors == 20:\n",
    "        umap_projections = umap_projections_20_08_35\n",
    "    elif n_neighbors == 30:\n",
    "        umap_projections = umap_projections_30_08_35\n",
    "    elif n_neighbors == 50:\n",
    "        umap_projections = umap_projections_50_08_35\n",
    "    elif n_neighbors == 100:\n",
    "        umap_projections = umap_projections_100_08_35\n",
    "\n",
    "    # Calculate metrics\n",
    "    radii_per_cluster, average_neighbor_counts = calculate_cluster_metrics(umap_projections, y_train)\n",
    "\n",
    "    # Store results\n",
    "    for cluster_idx in range(len(radii_per_cluster)):\n",
    "        results.append({\n",
    "            \"N\": n_neighbors,\n",
    "            \"Cluster\": cluster_idx,\n",
    "            \"Radius\": np.round(radii_per_cluster[cluster_idx], 3),\n",
    "            \"Number of Neighbors\": np.round(average_neighbor_counts[cluster_idx], 0)\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save results for later use\n",
    "df_results.to_csv(\"radius_neighbor_analysis_merged_MinDist_08.csv\", index=False)\n",
    "\n",
    "# Pivot table for easy visualization\n",
    "pivot_table = df_results.pivot(index=\"Cluster\", columns=\"N\", values=[\"Radius\", \"Number of Neighbors\"])\n",
    "print(pivot_table)\n",
    "\n",
    "# # Visualization of mean neighbor counts across runs\n",
    "# def plot_mean_neighbor_counts_across_runs(umap_projections_list, n_neighbors_values, y_labels):\n",
    "#     \"\"\"\n",
    "#     Plot mean number of neighbors across runs for different n_neighbors values.\n",
    "#     \"\"\"\n",
    "#     neighbor_counts_avg_runs = []\n",
    "    \n",
    "#     for umap_projections in umap_projections_list:\n",
    "#         # Calculate neighbor counts for each run\n",
    "#         neighbor_counts = []\n",
    "#         for run_idx, x_umap in enumerate(umap_projections):\n",
    "#             mean_neighbors = []\n",
    "#             for cluster_idx in np.unique(y_labels):\n",
    "#                 cluster_points = x_umap[y_labels == cluster_idx]\n",
    "#                 cluster_center = np.mean(cluster_points, axis=0)\n",
    "#                 distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "#                 mean_neighbors.append(np.mean(distances_to_center))\n",
    "#             neighbor_counts.append(np.mean(mean_neighbors))  # Mean across clusters for a run\n",
    "#         neighbor_counts_avg_runs.append(neighbor_counts)\n",
    "\n",
    "#     # Plot results\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     for i, counts in enumerate(neighbor_counts_avg_runs):\n",
    "#         plt.plot(range(1, len(counts) + 1), counts, label=f'n_neighbors={n_neighbors_values[i]}', marker='o')\n",
    "    \n",
    "#     plt.xlabel(\"Run Index\")\n",
    "#     plt.ylabel(\"Mean Number of Neighbors\")\n",
    "#     plt.title(\"Mean Number of Neighbors Across Runs\")\n",
    "#     plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))  # Adjust as needed\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "# # Call the visualization function\n",
    "# plot_mean_neighbor_counts_across_runs(\n",
    "#     umap_projections_list=[umap_projections_5_08_35,umap_projections_10_08_35,umap_projections_20_08_35,umap_projections_30_08_35, umap_projections_50_08_35, umap_projections_100_08_35],\n",
    "#     n_neighbors_values=n_neighbors_values,\n",
    "#     y_labels=y_train\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted Visualization of mean neighbor counts across runs\n",
    "def plot_mean_neighbor_counts_across_runs(umap_projections_list, n_neighbors_values, y_labels, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Plot mean number of neighbors across runs for different n_neighbors values with adjusted scaling for readability.\n",
    "    \"\"\"\n",
    "    neighbor_counts_avg_runs = []\n",
    "\n",
    "    for umap_projections in umap_projections_list:\n",
    "        # Calculate neighbor counts for each run\n",
    "        neighbor_counts_per_run = []\n",
    "        for run_idx, x_umap in enumerate(umap_projections):\n",
    "            cluster_centers = []\n",
    "            radii_per_cluster = []\n",
    "            for cluster_idx in range(n_clusters):\n",
    "                # Compute cluster center\n",
    "                cluster_points = x_umap[y_labels == cluster_idx]\n",
    "                if len(cluster_points) > 0:\n",
    "                    cluster_center = np.mean(cluster_points, axis=0)\n",
    "                    cluster_centers.append(cluster_center)\n",
    "\n",
    "                    # Compute dynamic radius for this cluster\n",
    "                    distances_to_center = np.linalg.norm(cluster_points - cluster_center, axis=1)\n",
    "                    dynamic_radius = np.mean(distances_to_center)\n",
    "                    radii_per_cluster.append(dynamic_radius)\n",
    "                else:\n",
    "                    radii_per_cluster.append(0)\n",
    "                    cluster_centers.append(np.array([0, 0]))\n",
    "\n",
    "            # Compute number of neighbors within radius for each cluster\n",
    "            neighbor_counts = []\n",
    "            for cluster_idx, cluster_center in enumerate(cluster_centers):\n",
    "                if radii_per_cluster[cluster_idx] > 0:  # Avoid empty clusters\n",
    "                    distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "                    neighbor_count = np.sum(distances_to_center <= radii_per_cluster[cluster_idx])\n",
    "                    neighbor_counts.append(neighbor_count)\n",
    "\n",
    "            # Store the mean neighbor count for this run\n",
    "            neighbor_counts_per_run.append(np.mean(neighbor_counts))\n",
    "        \n",
    "        neighbor_counts_avg_runs.append(neighbor_counts_per_run)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, counts in enumerate(neighbor_counts_avg_runs):\n",
    "        plt.plot(range(1, len(counts) + 1), counts, label=f'n_neighbors={n_neighbors_values[i]}', marker='o')\n",
    "\n",
    "    # Adjust y-axis scale and appearance\n",
    "    plt.ylim(3600, 4600)  # Set y-axis range for readability\n",
    "    plt.xlabel(\"Run Index\")\n",
    "    plt.ylabel(\"Mean Number of Points\")\n",
    "    plt.title(\"Mean Number of Points Across Runs for min_dist = 0.8\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))  # Adjust legend position\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_neighbor_counts_across_runs(\n",
    "    umap_projections_list=[\n",
    "        umap_projections_5_08_35,\n",
    "        umap_projections_10_08_35,\n",
    "        umap_projections_20_08_35,\n",
    "        umap_projections_30_08_35,\n",
    "        umap_projections_50_08_35,\n",
    "        umap_projections_100_08_35\n",
    "    ],\n",
    "    n_neighbors_values=n_neighbors_values,\n",
    "    y_labels=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading .csv \n",
    "df_results_08 = pd.read_csv('radius_neighbor_analysis_merged_MinDist_08.csv')\n",
    "\n",
    "# Add a density column to df_results\n",
    "df_results_08['Density'] = df_results_08['Number of Neighbors'] / df_results_08['Radius']\n",
    "\n",
    "# Find the row with the maximum density\n",
    "max_density_row = df_results_08.loc[df_results_08['Density'].idxmax()]\n",
    "\n",
    "# Extract the cluster, n_neighbors, and maximum density\n",
    "max_density = max_density_row['Density']\n",
    "max_cluster = max_density_row['Cluster']\n",
    "max_n_neighbors = max_density_row['N']\n",
    "\n",
    "# Print the results\n",
    "print(f\"Highest Density: {max_density:.2f}\")\n",
    "print(f\"Cluster: {int(max_cluster)}\")\n",
    "print(f\"n_neighbors: {int(max_n_neighbors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE THIS - DON'T OVERTHINK ##\n",
    "\n",
    "# # Function to calculate cluster centers, radii, and neighbor counts\n",
    "# def calculate_cluster_metrics(umap_projections, y_labels, n_clusters=10, n_runs=35):\n",
    "#     # Step 1: Calculate Cluster Centers for Each Run\n",
    "#     cluster_centers_full = []\n",
    "#     for run_idx, x_umap in enumerate(umap_projections):\n",
    "#         cluster_centers_run = []\n",
    "#         for label in np.unique(y_labels):  # Iterate over all labels\n",
    "#             cluster_points = x_umap[y_labels == label]\n",
    "#             if len(cluster_points) > 0:\n",
    "#                 cluster_center = np.mean(cluster_points, axis=0)\n",
    "#                 cluster_centers_run.append(cluster_center)\n",
    "#         cluster_centers_full.append(np.array(cluster_centers_run))\n",
    "\n",
    "#     cluster_centers_full = np.array(cluster_centers_full)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "#     # Step 2: Calculate the Average Radius Across Runs for Each Cluster\n",
    "#     radii_per_cluster = []\n",
    "#     for cluster_idx in range(n_clusters):\n",
    "#         radii_cluster = []\n",
    "#         for run_idx, x_umap in enumerate(umap_projections):\n",
    "#             cluster_center = cluster_centers_full[run_idx][cluster_idx]\n",
    "#             cluster_points = x_umap[y_labels == cluster_idx]\n",
    "#             if len(cluster_points) > 0:\n",
    "#                 distances_to_center = np.linalg.norm(cluster_points - cluster_center, axis=1)\n",
    "#                 dynamic_radius = np.mean(distances_to_center)  # Use mean distance as the radius\n",
    "#                 radii_cluster.append(dynamic_radius)\n",
    "#         radii_per_cluster.append(np.mean(radii_cluster))  # Average radius across runs\n",
    "\n",
    "#     # Step 3: Count Neighbors Using the Average Radius\n",
    "#     neighbor_counts_full = []\n",
    "#     for run_idx, x_umap in enumerate(umap_projections):\n",
    "#         counts_run = []\n",
    "#         for cluster_idx, cluster_center in enumerate(cluster_centers_full[run_idx]):\n",
    "#             radius = radii_per_cluster[cluster_idx]  # Use the average radius\n",
    "#             distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "#             count = np.sum(distances_to_center <= radius)  # Count points within the radius\n",
    "#             counts_run.append(count)\n",
    "#         neighbor_counts_full.append(counts_run)\n",
    "\n",
    "#     neighbor_counts_full = np.array(neighbor_counts_full)  # Shape: (n_runs, n_clusters)\n",
    "#     average_neighbor_counts = np.mean(neighbor_counts_full, axis=0)  # Average across runs\n",
    "\n",
    "#     return radii_per_cluster, average_neighbor_counts\n",
    "\n",
    "# # Define n_neighbors values\n",
    "# n_neighbors_values = [10, 50, 100]\n",
    "\n",
    "# # Placeholder for results\n",
    "# results = []\n",
    "\n",
    "# # Iterate over each n_neighbors value\n",
    "# for n_neighbors in n_neighbors_values:\n",
    "#     if n_neighbors == 10:\n",
    "#         umap_projections = umap_projections_10_01_35\n",
    "#     elif n_neighbors == 50:\n",
    "#         umap_projections = umap_projections_50_01_35\n",
    "#     elif n_neighbors == 100:\n",
    "#         umap_projections = umap_projections_100_01_35\n",
    "\n",
    "#     # Calculate metrics\n",
    "#     radii_per_cluster, average_neighbor_counts = calculate_cluster_metrics(umap_projections, y_train)\n",
    "\n",
    "#     # Store results\n",
    "#     for cluster_idx in range(len(radii_per_cluster)):\n",
    "#         results.append({\n",
    "#             \"N\": n_neighbors,\n",
    "#             \"Cluster\": cluster_idx,\n",
    "#             \"Radius\": np.round(radii_per_cluster[cluster_idx],3),\n",
    "#             \"Number of Neighbors\": np.round(average_neighbor_counts[cluster_idx],0)\n",
    "#         })\n",
    "\n",
    "# # Create a DataFrame for easier analysis\n",
    "# df_results = pd.DataFrame(results)\n",
    "\n",
    "# # Save results\n",
    "# df_results.to_csv(\"radius_neighbor_analysis_merged.csv\", index=False)\n",
    "\n",
    "# # Pivot table for easier comparison\n",
    "# pivot_table = df_results.pivot(index=\"Cluster\", columns=\"N\", values=[\"Radius\", \"Number of Neighbors\"])\n",
    "# print(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle\n",
    "\n",
    "# Visualization of UMAP clusters with dynamic radii for each n_neighbors value\n",
    "def plot_umap_clusters_with_radii(umap_projections, cluster_centers, radii_per_cluster, n_neighbors, y_labels):\n",
    "    \"\"\"\n",
    "    Plot UMAP clusters with dynamic radii for a given n_neighbors value.\n",
    "    \n",
    "    Parameters:\n",
    "        umap_projections: UMAP projections for a single n_neighbors value.\n",
    "        cluster_centers: Cluster centers across runs.\n",
    "        radii_per_cluster: Average radius for each cluster.\n",
    "        n_neighbors: Number of neighbors used in UMAP.\n",
    "        y_labels: Original labels of the data points.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(umap_projections[0][:, 0], umap_projections[0][:, 1], s=1, c=y_labels, cmap='Spectral', alpha=0.5, label='UMAP Points')\n",
    "    \n",
    "    for cluster_idx, (center, radius) in enumerate(zip(cluster_centers, radii_per_cluster)):\n",
    "        circle = Circle(center, radius, color='black', fill=False, linestyle='--', linewidth=1.5)\n",
    "        plt.gca().add_patch(circle)\n",
    "        plt.text(center[0], center[1], f'{cluster_idx}', color='black', fontsize=10, ha='center', va='center')\n",
    "    \n",
    "    plt.title(f\"UMAP Clusters with Dynamic Radii (n_neighbors={n_neighbors})\")\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.colorbar(label='Digit Labels')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "# Iterate over each n_neighbors value and visualize\n",
    "for n_neighbors, umap_projections in zip(\n",
    "    [10, 50, 100],\n",
    "    [umap_projections_10_01_35, umap_projections_50_01_35, umap_projections_100_01_35]\n",
    "):\n",
    "    # Calculate the mean cluster centers for the first run\n",
    "    cluster_centers_mean = np.mean(cluster_centers_full, axis=0)  # Average over runs\n",
    "    # Visualize the clusters\n",
    "    plot_umap_clusters_with_radii(\n",
    "        umap_projections=umap_projections,\n",
    "        cluster_centers=cluster_centers_mean,\n",
    "        radii_per_cluster=radii_per_cluster,\n",
    "        n_neighbors=n_neighbors,\n",
    "        y_labels=y_train\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=5 with matrices aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instead of using the mean_umap I used ARI to choose the best run and use it as a reference to align all the runs with Procrusted.\n",
    "Then this new aligned_projections will replace mean_projections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('aligned_projections_5_01_35.npy', aligned_projections)\n",
    "np.save('aligned_cluster_centroids_per_run_5_01_35.npy', aligned_cluster_centroids_per_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_projections = np.load('aligned_projections.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_5_01_35= np.load('umap_projections_5_01_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for ARI scores\n",
    "ari_scores = []\n",
    "\n",
    "# Iterate through each run's UMAP projections\n",
    "for i, run_projection in enumerate(umap_projections_5_01_35):\n",
    "    # Perform K-Means clustering on the UMAP projection for this run\n",
    "    kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "    cluster_labels = kmeans.fit_predict(run_projection)\n",
    "\n",
    "    # Compute ARI between true labels and cluster labels\n",
    "    ari_score = adjusted_rand_score(y_train, cluster_labels)\n",
    "    ari_scores.append(ari_score)\n",
    "    print(f\"Run {i}: Adjusted Rand Index (ARI): {ari_score}\")\n",
    "\n",
    "# Identify the best run based on ARI\n",
    "best_run_index = np.argmax(ari_scores)\n",
    "best_ari_score = ari_scores[best_run_index]\n",
    "\n",
    "print(f\"Best run based on ARI: Run {best_run_index} with ARI Score: {best_ari_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procrustes Alignment using the best run as the reference\n",
    "reference_projection = umap_projections_5_01_35[best_run_index] # Best run based on ARI: Run 18 with ARI Score: 0.915\n",
    "\n",
    "aligned_projections = []\n",
    "procrustes_distances = []\n",
    "\n",
    "for i, run_projection in enumerate(umap_projections_5_01_35):\n",
    "    # Apply Procrustes alignment\n",
    "    mtx1, mtx2, disparity = procrustes(reference_projection, run_projection)\n",
    "    aligned_projections.append(mtx2)\n",
    "    procrustes_distances.append(disparity)\n",
    "    print(f\"Run {i}: Procrustes disparity: {disparity}\")\n",
    "\n",
    "aligned_projections = np.array(aligned_projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a color map for 10 classes (digits 0-9)\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "# Create a scatter plot of the mean projections\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label in range(10):  # Assuming digits 0-9\n",
    "    # Select points corresponding to the current label\n",
    "    # points = aligned_projections[y_train == label]\n",
    "    plt.scatter(points[:, 0], points[:, 1], color=colors[label], label=f\"Digit {label}\")\n",
    "\n",
    "# Add labels, legend, and title\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.title(\"Mean UMAP Projections (n_neighbors=5, min_dist=0.1)\")\n",
    "plt.legend(loc='best', title=\"Digits\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the aligned projections for a few runs\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(3):  # Plot the first 3 aligned runs\n",
    "    plt.scatter(aligned_projections[i, :, 0], aligned_projections[i, :, 1], s=10, label=f\"Run {i}\")\n",
    "\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.title(\"Procrustes-Aligned UMAP Projections\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation of Procrustes disparities\n",
    "mean_disparity = np.mean(procrustes_distances)\n",
    "std_disparity = np.std(procrustes_distances)\n",
    "\n",
    "# Define a threshold for \"good\" runs (e.g., within one standard deviation)\n",
    "threshold = mean_disparity + std_disparity\n",
    "\n",
    "# Identify good runs\n",
    "good_runs = [i for i, disparity in enumerate(procrustes_distances) if disparity <= threshold]\n",
    "\n",
    "print(f\"Mean Procrustes disparity: {mean_disparity}\")\n",
    "print(f\"Standard deviation of disparities: {std_disparity}\")\n",
    "print(f\"Threshold for good runs: {threshold}\")\n",
    "print(f\"Good runs based on Procrustes disparity: {good_runs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO CORROBORATE\n",
    "# ## Recompute ARI scores after Procrustes alignment\n",
    "aligned_ari_scores = []\n",
    "\n",
    "for aligned_projection in aligned_projections:\n",
    "    kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(aligned_projection)\n",
    "    ari_score = adjusted_rand_score(y_train, cluster_labels)\n",
    "    aligned_ari_scores.append(ari_score)\n",
    "\n",
    "# Output ARI scores\n",
    "print(f\"Aligned ARI scores: {aligned_ari_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids for each aligned run\n",
    "aligned_cluster_centroids_per_run = []\n",
    "\n",
    "# Compute centroids for each aligned run\n",
    "for run in range(len(aligned_projections)):\n",
    "    aligned_projection = aligned_projections[run]  # Current aligned projection\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        # Select points belonging to the current cluster\n",
    "        cluster_points = aligned_projection[y_train == cluster_label]\n",
    "        # Compute the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    # Store centroids for this run\n",
    "    aligned_cluster_centroids_per_run.append(np.array(centroids))\n",
    "\n",
    "aligned_cluster_centroids_per_run = np.array(aligned_cluster_centroids_per_run)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "# Output centroids for the first run (for verification)\n",
    "# print(f\"Cluster centroids for first aligned run:\\n{aligned_cluster_centroids_per_run[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_n5 = np.zeros(10)\n",
    "std_dev_y_n5 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = aligned_cluster_centroids_per_run[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = aligned_cluster_centroids_per_run[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_n5[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_n5[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_n5)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_n5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_umap_n5 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(len(aligned_cluster_centroids_per_run)):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = aligned_cluster_centroids_per_run[trial, cluster]\n",
    "\n",
    "        # Calculate the bounds for the 3 standard deviations range for x and y\n",
    "        mean_x, mean_y = np.mean(aligned_cluster_centroids_per_run[:, cluster, :], axis=0)\n",
    "        lower_bound_x, upper_bound_x = mean_x - 3 * std_dev_x_n5[cluster], mean_x + 3 * std_dev_x_n5[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 3 * std_dev_y_n5[cluster], mean_y + 3 * std_dev_y_n5[cluster]\n",
    "\n",
    "        # Check if the centroid is inside the 3 std range\n",
    "        inside_3_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "\n",
    "        # Append the data as a new row in the list\n",
    "        data_umap_n5.append([trial + 1, cluster, centroid_coord, inside_3_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_umap_n5 = pd.DataFrame(data_umap_n5, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 3 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group the DataFrame by Trial\n",
    "grouped_by_trial = df_results_umap_n5.groupby('Trial')\n",
    "\n",
    "# Step 2: Check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_with_all_true = grouped_by_trial['Inside 3 std'].all()\n",
    "\n",
    "# Step 3: Filter the trials where all clusters were True\n",
    "valid_trials = trials_with_all_true[trials_with_all_true].index.tolist()\n",
    "\n",
    "# Step 4: Show the list of trials\n",
    "print(f\"Trials where all clusters are inside 3 std: {valid_trials}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter aligned projections to keep only valid runs\n",
    "# valid_runs = [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 29, 30, 31, 33, 34, 35]\n",
    "aligned_projections_5_01_35 = aligned_projections[[run - 1 for run in valid_trials]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_projections_5_01_35.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter aligned_cluster_centroids_per_run to match valid runs\n",
    "aligned_cluster_centroids_per_run_filtered = aligned_cluster_centroids_per_run[[run - 1 for run in valid_trials]]\n",
    "\n",
    "# Placeholder for pairwise distance matrices\n",
    "distance_matrices_5_01_35_al = []\n",
    "\n",
    "# Calculate pairwise distances between centroids for each valid run\n",
    "for centroids in aligned_cluster_centroids_per_run_filtered:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_5_01_35_al.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_5_01_35_al = np.array(distance_matrices_5_01_35_al)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_5_01_35_al = np.mean(distance_matrices_5_01_35_al, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_5_01_35_al = (mean_distance_matrix_5_01_35_al - np.min(mean_distance_matrix_5_01_35_al)) / (\n",
    "    np.max(mean_distance_matrix_5_01_35_al) - np.min(mean_distance_matrix_5_01_35_al)\n",
    ")\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_01_35_al, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=5)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_5_01_35_al.npy', distance_matrices_5_01_35_al)\n",
    "np.save('mean_distance_matrix_neighbors_5_01_35_al.npy', mean_distance_matrix_5_01_35_al)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_5_01_35_al}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (using true labels)\n",
    "aligned_cluster_centroids_true_labels_filtered = []\n",
    "\n",
    "# Calculate centroids for each valid run using true labels\n",
    "for run_projection in aligned_projections_100_01_35:  # Already filtered projections\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        # Select points belonging to the current cluster\n",
    "        cluster_points = run_projection[y_train == cluster_label]\n",
    "        # Compute the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        centroids.append(centroid)\n",
    "    # Store centroids for this run\n",
    "    aligned_cluster_centroids_true_labels_filtered.append(np.array(centroids))\n",
    "\n",
    "# Placeholder for pairwise distance matrices\n",
    "distance_matrices_5_01_35_labels = []\n",
    "\n",
    "# Calculate pairwise distances between centroids for each valid run\n",
    "for centroids in aligned_cluster_centroids_true_labels_filtered:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_5_01_35_labels.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_5_01_35_labels = np.array(distance_matrices_5_01_35_labels)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_5_01_35_labels = np.mean(distance_matrices_5_01_35_labels, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_5_01_35_labels = (mean_distance_matrix_5_01_35_labels - np.min(mean_distance_matrix_5_01_35_labels)) / (\n",
    "    np.max(mean_distance_matrix_5_01_35_labels) - np.min(mean_distance_matrix_5_01_35_labels)\n",
    ")\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_01_35_labels, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (True Labels, k=10, n_neighbors=5)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_5_01_35_labels.npy', distance_matrices_5_01_35_labels)\n",
    "np.save('mean_distance_matrix_neighbors_5_01_35_labels.npy', mean_distance_matrix_5_01_35_labels)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs (True Labels):\\n{mean_distance_matrix_5_01_35_labels}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, run in enumerate(valid_trials):\n",
    "    print(f\"Run {i}: Centroids difference (A vs B):\")\n",
    "    print(np.linalg.norm(cluster_centroids_per_run[i] - aligned_cluster_centroids_per_run_filtered[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_diff = np.abs(mean_distance_matrix_5_01_35 - mean_distance_matrix_5_01_35_al)\n",
    "print(f\"Difference between mean distance matrices:\\n{distance_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_diff = np.abs(normalized_mean_distance_matrix_5_01_35 - normalized_mean_distance_matrix_5_01_35_al)\n",
    "print(f\"Difference between normalized mean distance matrices:\\n{normalized_diff}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_10_01_35= np.load(f'umap_projections_10_01_35.npy')\n",
    "mean_projection_10_01_35= np.load(f'mean_projection_10_01_35.npy')\n",
    "std_projection_10_01_35= np.load(f'std_projection_10_01_35.npy')\n",
    "lower_limit_intconf_matrix_10_01_35= np.load(f'lower_limit_intconf_matrix_10_01_35.npy')\n",
    "upper_limit_intconf_matrix_10_01_35= np.load(f'upper_limit_intconf_matrix_10_01_35.npy')\n",
    "distance_matrices_10_01_35=np.load(f'distance_matrices_neighbors_10_01_35.npy')\n",
    "mean_distance_matrix_10_01_35=np.load(f'mean_distance_matrix_neighbors_10_01_35.npy')\n",
    "norm_lower_limit_intconf_matrix_10_01_35=np.load(f'norm_lower_limit_intconf_matrix_10_01_35.npy')\n",
    "norm_upper_limit_intconf_matrix_10_01_35=np.load(f'norm_upper_limit_intconf_matrix_10_01_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_10_01_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_10_01_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_10_01_35 = np.array(umap_projections_10_01_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_10_01_35 = np.mean(umap_projections_10_01_35, axis=0)\n",
    "std_projection_10_01_35 = np.std(umap_projections_10_01_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_10_01_35.npy', umap_projections_10_01_35)\n",
    "np.save('mean_projection_10_01_35.npy', mean_projection_10_01_35)\n",
    "np.save('std_projection_10_01_35.npy', std_projection_10_01_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_10_01_35'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_10_01_35 = np.sqrt(np.sum((umap_projections_10_01_35 - mean_projection_10_01_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_10_01_35, axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_mean_distance_matrix_10_01_35 = (mean_distance_matrix_10_01_35 - np.min(mean_distance_matrix_10_01_35)) / (np.max(mean_distance_matrix_10_01_35) - np.min(mean_distance_matrix_10_01_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_10_01_35[run]  # Shape: (n_samples, 2)\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_10_01_35 = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_10_01_35.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_10_01_35 = np.array(distance_matrices_10_01_35)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_10_01_35 = np.mean(distance_matrices_10_01_35, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_10_01_35 = (mean_distance_matrix_10_01_35 - np.min(mean_distance_matrix_10_01_35)) / (np.max(mean_distance_matrix_10_01_35) - np.min(mean_distance_matrix_10_01_35))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=10)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_10_01_35.npy', distance_matrices_10_01_35)\n",
    "np.save('mean_distance_matrix_neighbors_10_01_35.npy', mean_distance_matrix_10_01_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_10_01_35}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_10_01_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_10_01_35,3))\n",
    "np.save('G_10_01_35.npy',G_10_01_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_10_01_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_10_01_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_10_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_10_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total weight of the MST\n",
    "total_weight = sum(nx.get_edge_attributes(mst_10_01_35, 'weight').values())\n",
    "\n",
    "# Print the total weight\n",
    "print(f\"Total weight of the MST: {total_weight}\")\n",
    "\n",
    "# Compute the minimum spanning tree of the graph\n",
    "mst_10_01_35 = nx.minimum_spanning_tree(G_10_01_35)\n",
    "np.save('mst_10_01_35.npy', mst_10_01_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_10_01_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_10_01_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_10_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_10_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=10, min_dist=0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree (MST) of the graph\n",
    "mst_10_01_35 = nx.minimum_spanning_tree(G_10_01_35)\n",
    "\n",
    "# Save the MST for later use\n",
    "np.save('mst_10_01_35.npy', mst_10_01_35)\n",
    "\n",
    "# Define positions for all nodes in the MST using a spring layout\n",
    "pos = nx.spring_layout(mst_10_01_35, seed=42)\n",
    "\n",
    "# Increase figure size for better visibility\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Draw the MST with larger nodes, thicker edges, and a larger font for labels\n",
    "nx.draw(\n",
    "    mst_10_01_35,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    node_color='lightblue',\n",
    "    edge_color='red',\n",
    "    node_size=1000,  # Larger node size for better visibility\n",
    "    font_size=12,    # Larger font size for node labels\n",
    "    width=3          # Thicker edge lines\n",
    ")\n",
    "\n",
    "# Get edge weights and format them to 2 decimal places for clarity\n",
    "edge_labels = nx.get_edge_attributes(mst_10_01_35, 'weight')\n",
    "formatted_edge_labels = {k: f\"{v:.2f}\" for k, v in edge_labels.items()}\n",
    "\n",
    "# Draw edge labels with formatted weights\n",
    "nx.draw_networkx_edge_labels(\n",
    "    mst_10_01_35,\n",
    "    pos,\n",
    "    edge_labels=formatted_edge_labels,\n",
    "    font_size=20,    # Font size for edge labels\n",
    "    label_pos=0.5    # Position edge labels at the center of edges\n",
    ")\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title(\"MST UMAP - n_neighbors=10, min_dist=0.1\", fontsize=16)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_10_01_35 = np.std(distance_matrices_10_01_35, axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_10_01_35.npy\", distance_matrix_std_10_01_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (10_01_35):\\n\", distance_matrix_std_10_01_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_10_01_35 = distance_matrix_std_10_01_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_10_01_35 = z_score * sem_matrix_10_01_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_10_01_35 = mean_distance_matrix_10_01_35 - margin_of_error_matrix_10_01_35\n",
    "upper_limit_intconf_matrix_10_01_35 = mean_distance_matrix_10_01_35 + margin_of_error_matrix_10_01_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_10_01_35 = np.maximum(lower_limit_intconf_matrix_10_01_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_10_01_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_10_01_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_10_01_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_10_01_35.npy', lower_limit_intconf_matrix_10_01_35)\n",
    "np.save('upper_limit_intconf_matrix_10_01_35.npy', upper_limit_intconf_matrix_10_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_10_01_35 = normalize_matrix(lower_limit_intconf_matrix_10_01_35)\n",
    "norm_upper_limit_intconf_matrix_10_01_35 = normalize_matrix(upper_limit_intconf_matrix_10_01_35)\n",
    "np.save('norm_lower_limit_intconf_matrix_10_01_35.npy', norm_lower_limit_intconf_matrix_10_01_35)\n",
    "np.save('norm_upper_limit_intconf_matrix_10_01_35.npy', norm_upper_limit_intconf_matrix_10_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_10_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=10, min_dist=0.1)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=10, min_dist=0.1)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_10_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=10, min_dist=0.1)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_10_01_35, \"UMAP MST - Mean Distances - n_neighbors=10, min_dist=0.1\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_10_01_35, \"UMAP MST - Lower Limit- n_neighbors=10, min_dist=0.1\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_10_01_35, \"UMAP MST - Upper Limit- n_neighbors=10, min_dist=0.1\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_20_01_35= np.load(f'umap_projections_20_01_35.npy')\n",
    "mean_projection_20_01_35= np.load(f'mean_projection_20_01_35.npy')\n",
    "std_projection_20_01_35= np.load(f'std_projection_20_01_35.npy')\n",
    "lower_limit_intconf_matrix_20_01_35= np.load(f'lower_limit_intconf_matrix_20_01_35.npy')\n",
    "upper_limit_intconf_matrix_20_01_35= np.load(f'upper_limit_intconf_matrix_20_01_35.npy')\n",
    "distance_matrices_20_01_35=np.load(f'distance_matrices_neighbors_20_01_35.npy')\n",
    "mean_distance_matrix_20_01_35=np.load(f'mean_distance_matrix_neighbors_20_01_35.npy')\n",
    "norm_lower_limit_intconf_matrix_20_01_35=np.load(f'norm_lower_limit_intconf_matrix_20_01_35.npy')\n",
    "norm_upper_limit_intconf_matrix_20_01_35=np.load(f'norm_upper_limit_intconf_matrix_20_01_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 20\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_20_01_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_20_01_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_20_01_35 = np.array(umap_projections_20_01_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_20_01_35 = np.mean(umap_projections_20_01_35, axis=0)\n",
    "std_projection_20_01_35 = np.std(umap_projections_20_01_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_20_01_35.npy', umap_projections_20_01_35)\n",
    "np.save('mean_projection_20_01_35.npy', mean_projection_20_01_35)\n",
    "np.save('std_projection_20_01_35.npy', std_projection_20_01_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_20_01_35'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_20_01_35 = np.sqrt(np.sum((umap_projections_20_01_35 - mean_projection_20_01_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_20_01_35, axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_20_01_35[run]  # Shape: (n_samples, 2)\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_20_01_35 = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_20_01_35.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_20_01_35 = np.array(distance_matrices_20_01_35)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_20_01_35 = np.mean(distance_matrices_20_01_35, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_20_01_35 = (mean_distance_matrix_20_01_35 - np.min(mean_distance_matrix_20_01_35)) / (np.max(mean_distance_matrix_20_01_35) - np.min(mean_distance_matrix_20_01_35))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_20_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=20, min_dist=0.1)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_20_01_35.npy', distance_matrices_20_01_35)\n",
    "np.save('mean_distance_matrix_neighbors_20_01_35.npy', mean_distance_matrix_20_01_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_20_01_35}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_20_01_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_20_01_35,3))\n",
    "np.save('G_20_01_35.npy',G_20_01_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_20_01_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_20_01_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_20_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_20_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_20_01_35 = nx.minimum_spanning_tree(G_20_01_35)\n",
    "np.save('mst_20_01_35.npy', mst_20_01_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_20_01_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_20_01_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_20_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_20_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=20, min_dist=0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_20_01_35 = np.std(distance_matrices_20_01_35, axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_20_01_35.npy\", distance_matrix_std_20_01_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (20_01_35):\\n\", distance_matrix_std_20_01_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_20_01_35 = distance_matrix_std_20_01_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_20_01_35 = z_score * sem_matrix_20_01_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_20_01_35 = mean_distance_matrix_20_01_35 - margin_of_error_matrix_20_01_35\n",
    "upper_limit_intconf_matrix_20_01_35 = mean_distance_matrix_20_01_35 + margin_of_error_matrix_20_01_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_20_01_35 = np.maximum(lower_limit_intconf_matrix_20_01_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_20_01_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_20_01_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_20_01_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_20_01_35.npy', lower_limit_intconf_matrix_20_01_35)\n",
    "np.save('upper_limit_intconf_matrix_20_01_35.npy', upper_limit_intconf_matrix_20_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_20_01_35 = normalize_matrix(lower_limit_intconf_matrix_20_01_35)\n",
    "norm_upper_limit_intconf_matrix_20_01_35 = normalize_matrix(upper_limit_intconf_matrix_20_01_35)\n",
    "np.save('norm_lower_limit_intconf_matrix_20_01_35.npy', norm_lower_limit_intconf_matrix_20_01_35)\n",
    "np.save('norm_upper_limit_intconf_matrix_20_01_35.npy', norm_upper_limit_intconf_matrix_20_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_20_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=20, min_dist=0.1)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_20_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=20, min_dist=0.1)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_20_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=20, min_dist=0.1)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_20_01_35, \"UMAP MST - Mean Distances - n_neighbors=20, min_dist=0.1\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_20_01_35, \"UMAP MST - Lower Limit - n_neighbors=20, min_dist=0.1\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_20_01_35, \"UMAP MST - Upper Limit - n_neighbors=20, min_dist=0.1\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_30_01_35= np.load(f'umap_projections_30_01_35.npy')\n",
    "mean_projection_30_01_35= np.load(f'mean_projection_30_01_35.npy')\n",
    "std_projection_30_01_35= np.load(f'std_projection_30_01_35.npy')\n",
    "lower_limit_intconf_matrix_30_01_35= np.load(f'lower_limit_intconf_matrix_30_01_35.npy')\n",
    "upper_limit_intconf_matrix_30_01_35= np.load(f'upper_limit_intconf_matrix_30_01_35.npy')\n",
    "distance_matrices_30_01_35=np.load(f'distance_matrices_neighbors_30_01_35.npy')\n",
    "mean_distance_matrix_30_01_35=np.load(f'mean_distance_matrix_neighbors_30_01_35.npy')\n",
    "norm_lower_limit_intconf_matrix_30_01_35=np.load(f'norm_lower_limit_intconf_matrix_30_01_35.npy')\n",
    "norm_upper_limit_intconf_matrix_30_01_35=np.load(f'norm_upper_limit_intconf_matrix_30_01_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 30\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_30_01_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_30_01_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_30_01_35 = np.array(umap_projections_30_01_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_30_01_35 = np.mean(umap_projections_30_01_35, axis=0)\n",
    "std_projection_30_01_35 = np.std(umap_projections_30_01_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_30_01_35.npy', umap_projections_30_01_35)\n",
    "np.save('mean_projection_30_01_35.npy', mean_projection_30_01_35)\n",
    "np.save('std_projection_30_01_35.npy', std_projection_30_01_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_30_01_35'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_30_01_35 = np.sqrt(np.sum((umap_projections_30_01_35 - mean_projection_30_01_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_30_01_35, axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_30_01_35[run]  # Shape: (n_samples, 2)\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_30_01_35 = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_30_01_35.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_30_01_35 = np.array(distance_matrices_30_01_35)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_30_01_35 = np.mean(distance_matrices_30_01_35, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_30_01_35 = (mean_distance_matrix_30_01_35 - np.min(mean_distance_matrix_30_01_35)) / (np.max(mean_distance_matrix_30_01_35) - np.min(mean_distance_matrix_30_01_35))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_30_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10,  n_neighbors=30, min_dist=0.1)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_30_01_35.npy', distance_matrices_30_01_35)\n",
    "np.save('mean_distance_matrix_neighbors_30_01_35.npy', mean_distance_matrix_30_01_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_30_01_35}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_30_01_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_30_01_35,3))\n",
    "np.save('G_30_01_35.npy',G_30_01_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_30_01_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_30_01_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_30_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_30_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_30_01_35 = nx.minimum_spanning_tree(G_30_01_35)\n",
    "np.save('mst_30_01_35.npy', mst_30_01_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_30_01_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_30_01_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_30_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_30_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=30, min_dist=0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_30_01_35 = np.std(distance_matrices_30_01_35, axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_30_01_35.npy\", distance_matrix_std_30_01_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (30_01_35):\\n\", distance_matrix_std_30_01_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_30_01_35 = distance_matrix_std_30_01_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_30_01_35 = z_score * sem_matrix_30_01_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_30_01_35 = mean_distance_matrix_30_01_35 - margin_of_error_matrix_30_01_35\n",
    "upper_limit_intconf_matrix_30_01_35 = mean_distance_matrix_30_01_35 + margin_of_error_matrix_30_01_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_30_01_35 = np.maximum(lower_limit_intconf_matrix_30_01_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_30_01_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_30_01_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_30_01_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_30_01_35.npy', lower_limit_intconf_matrix_30_01_35)\n",
    "np.save('upper_limit_intconf_matrix_30_01_35.npy', upper_limit_intconf_matrix_30_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_30_01_35 = normalize_matrix(lower_limit_intconf_matrix_30_01_35)\n",
    "norm_upper_limit_intconf_matrix_30_01_35 = normalize_matrix(upper_limit_intconf_matrix_30_01_35)\n",
    "np.save('norm_lower_limit_intconf_matrix_30_01_35.npy', norm_lower_limit_intconf_matrix_30_01_35)\n",
    "np.save('norm_upper_limit_intconf_matrix_30_01_35.npy', norm_upper_limit_intconf_matrix_30_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_30_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=30, min_dist=0.1)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_30_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=30, min_dist=0.1)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_30_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=30, min_dist=0.1)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_30_01_35, \"UMAP MST - Mean Distances - n_neighbors=30, min_dist=0.1\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_30_01_35, \"UMAP MST - Lower Limit - n_neighbors=30, min_dist=0.1\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_30_01_35, \"UMAP MST - Upper Limit - n_neighbors=30, min_dist=0.1\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_50_01_35= np.load(f'umap_projections_50_01_35.npy')\n",
    "mean_projection_50_01_35= np.load(f'mean_projection_50_01_35.npy')\n",
    "std_projection_50_01_35= np.load(f'std_projection_50_01_35.npy')\n",
    "lower_limit_intconf_matrix_50_01_35= np.load(f'lower_limit_intconf_matrix_50_01_35.npy')\n",
    "upper_limit_intconf_matrix_50_01_35= np.load(f'upper_limit_intconf_matrix_50_01_35.npy')\n",
    "distance_matrices_50_01_35=np.load(f'distance_matrices_neighbors_50_01_35.npy')\n",
    "mean_distance_matrix_50_01_35=np.load(f'mean_distance_matrix_neighbors_50_01_35.npy')\n",
    "norm_lower_limit_intconf_matrix_50_01_35=np.load(f'norm_lower_limit_intconf_matrix_50_01_35.npy')\n",
    "norm_upper_limit_intconf_matrix_50_01_35=np.load(f'norm_upper_limit_intconf_matrix_50_01_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 50\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_50_01_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_50_01_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_50_01_35 = np.array(umap_projections_50_01_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_50_01_35 = np.mean(umap_projections_50_01_35, axis=0)\n",
    "std_projection_50_01_35 = np.std(umap_projections_50_01_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_50_01_35.npy', umap_projections_50_01_35)\n",
    "np.save('mean_projection_50_01_35.npy', mean_projection_50_01_35)\n",
    "np.save('std_projection_50_01_35.npy', std_projection_50_01_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_50_01_35'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_50_01_35 = np.sqrt(np.sum((umap_projections_50_01_35 - mean_projection_50_01_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_50_01_35, axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_50_01_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_50_01 = np.zeros((n_runs, n_clusters, umap_projections_50_01_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_50_01_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_50_01[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_50_01 = np.zeros(10)\n",
    "std_dev_y_50_01 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_50_01[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_50_01[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_50_01[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_50_01[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_50_01)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_50_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_50_01_35[run]  # Shape: (n_samples, 2)\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_50_01_35 = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_50_01_35.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_50_01_35 = np.array(distance_matrices_50_01_35)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_50_01_35 = np.mean(distance_matrices_50_01_35, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_50_01_35 = (mean_distance_matrix_50_01_35 - np.min(mean_distance_matrix_50_01_35)) / (np.max(mean_distance_matrix_50_01_35) - np.min(mean_distance_matrix_50_01_35))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_50_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=50, min_dist=0.1)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_50_01_35.npy', distance_matrices_50_01_35)\n",
    "np.save('mean_distance_matrix_neighbors_50_01_35.npy', mean_distance_matrix_50_01_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_50_01_35}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_50_01_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_50_01_35,3))\n",
    "np.save('G_50_01_35.npy',G_50_01_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_50_01_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_50_01_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_50_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_50_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_50_01_35 = (mean_distance_matrix_50_01_35 - np.min(mean_distance_matrix_50_01_35)) / (np.max(mean_distance_matrix_50_01_35) - np.min(mean_distance_matrix_50_01_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total weight of the MST\n",
    "total_weight_50 = sum(nx.get_edge_attributes(mst_50_01_35, 'weight').values())\n",
    "\n",
    "# Print the total weight\n",
    "print(f\"Total weight of the MST: {total_weight_50}\")\n",
    "\n",
    "# Compute the minimum spanning tree of the graph\n",
    "mst_50_01_35 = nx.minimum_spanning_tree(G_50_01_35)\n",
    "np.save('mst_50_01_35.npy', mst_50_01_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_50_01_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_50_01_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_50_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_50_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=50, min_dist=0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_50_01_35 = np.std(distance_matrices_50_01_35, axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_50_01_35.npy\", distance_matrix_std_50_01_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (50_01_35):\\n\", distance_matrix_std_50_01_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_50_01_35 = distance_matrix_std_50_01_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_50_01_35 = z_score * sem_matrix_50_01_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_50_01_35 = mean_distance_matrix_50_01_35 - margin_of_error_matrix_50_01_35\n",
    "upper_limit_intconf_matrix_50_01_35 = mean_distance_matrix_50_01_35 + margin_of_error_matrix_50_01_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_50_01_35 = np.maximum(lower_limit_intconf_matrix_50_01_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_50_01_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_50_01_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_50_01_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_50_01_35.npy', lower_limit_intconf_matrix_50_01_35)\n",
    "np.save('upper_limit_intconf_matrix_50_01_35.npy', upper_limit_intconf_matrix_50_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_50_01_35 = normalize_matrix(lower_limit_intconf_matrix_50_01_35)\n",
    "norm_upper_limit_intconf_matrix_50_01_35 = normalize_matrix(upper_limit_intconf_matrix_50_01_35)\n",
    "np.save('norm_lower_limit_intconf_matrix_50_01_35.npy', norm_lower_limit_intconf_matrix_50_01_35)\n",
    "np.save('norm_upper_limit_intconf_matrix_50_01_35.npy', norm_upper_limit_intconf_matrix_50_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_50_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=50, min_dist=0.1)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_50_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=50, min_dist=0.1)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_50_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=50, min_dist=0.1)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_50_01_35, \"UMAP MST - Mean Distances - n_neighbors=50, min_dist=0.1\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_50_01_35, \"UMAP MST - Lower Limit - n_neighbors=50, min_dist=0.1\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_50_01_35, \"UMAP MST - Upper Limit - n_neighbors=50, min_dist=0.1\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_100_01_35= np.load(f'umap_projections_100_01_35.npy')\n",
    "mean_projection_100_01_35= np.load(f'mean_projection_100_01_35.npy')\n",
    "std_projection_100_01_35= np.load(f'std_projection_100_01_35.npy')\n",
    "lower_limit_intconf_matrix_100_01_35= np.load(f'lower_limit_intconf_matrix_100_01_35.npy')\n",
    "upper_limit_intconf_matrix_100_01_35= np.load(f'upper_limit_intconf_matrix_100_01_35.npy')\n",
    "distance_matrices_100_01_35=np.load(f'distance_matrices_neighbors_100_01_35.npy')\n",
    "mean_distance_matrix_100_01_35=np.load(f'mean_distance_matrix_neighbors_100_01_35.npy')\n",
    "norm_lower_limit_intconf_matrix_100_01_35=np.load(f'norm_lower_limit_intconf_matrix_100_01_35.npy')\n",
    "norm_upper_limit_intconf_matrix_100_01_35=np.load(f'norm_upper_limit_intconf_matrix_100_01_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 100\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_100_01_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_100_01_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_100_01_35 = np.array(umap_projections_100_01_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_100_01_35 = np.mean(umap_projections_100_01_35, axis=0)\n",
    "std_projection_100_01_35 = np.std(umap_projections_100_01_35, axis=0)\n",
    "                                                                        \n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_100_01_35.npy', umap_projections_100_01_35)\n",
    "np.save('mean_projection_100_01_35.npy', mean_projection_100_01_35)\n",
    "np.save('std_projection_100_01_35.npy', std_projection_100_01_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_100_01_35'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_100_01_35 = (mean_distance_matrix_100_01_35 - np.min(mean_distance_matrix_100_01_35)) / (np.max(mean_distance_matrix_100_01_35) - np.min(mean_distance_matrix_100_01_35))\n",
    "normalized_mean_distance_matrix_50_01_35 = (mean_distance_matrix_50_01_35 - np.min(mean_distance_matrix_50_01_35)) / (np.max(mean_distance_matrix_50_01_35) - np.min(mean_distance_matrix_50_01_35))\n",
    "normalized_mean_distance_matrix_30_01_35 = (mean_distance_matrix_30_01_35 - np.min(mean_distance_matrix_30_01_35)) / (np.max(mean_distance_matrix_30_01_35) - np.min(mean_distance_matrix_30_01_35))\n",
    "normalized_mean_distance_matrix_20_01_35 = (mean_distance_matrix_20_01_35 - np.min(mean_distance_matrix_20_01_35)) / (np.max(mean_distance_matrix_20_01_35) - np.min(mean_distance_matrix_20_01_35))\n",
    "normalized_mean_distance_matrix_10_01_35 = (mean_distance_matrix_10_01_35 - np.min(mean_distance_matrix_10_01_35)) / (np.max(mean_distance_matrix_10_01_35) - np.min(mean_distance_matrix_10_01_35))\n",
    "normalized_mean_distance_matrix_5_01_35 = (mean_distance_matrix_5_01_35 - np.min(mean_distance_matrix_5_01_35)) / (np.max(mean_distance_matrix_5_01_35) - np.min(mean_distance_matrix_5_01_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_100_01_35 = np.sqrt(np.sum((umap_projections_100_01_35 - mean_projection_100_01_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_100_01_35, axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_100_01_35[run]  # Shape: (n_samples, 2)\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_100_01_35 = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_100_01_35.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_100_01_35 = np.array(distance_matrices_100_01_35)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_100_01_35 = np.mean(distance_matrices_100_01_35, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_100_01_35 = (mean_distance_matrix_100_01_35 - np.min(mean_distance_matrix_100_01_35)) / (np.max(mean_distance_matrix_100_01_35) - np.min(mean_distance_matrix_100_01_35))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=100, min_dist=0.1)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_100_01_35.npy', distance_matrices_100_01_35)\n",
    "np.save('mean_distance_matrix_neighbors_100_01_35.npy', mean_distance_matrix_100_01_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_100_01_35}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_100_01_35 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_100_01_35,3))\n",
    "np.save('G_100_01_35.npy',G_100_01_35)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_100_01_35, seed=42)  # positions for all nodes\n",
    "nx.draw(G_100_01_35, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_100_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_100_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_100_01_35 = (mean_distance_matrix_100_01_35 - np.min(mean_distance_matrix_100_01_35)) / (np.max(mean_distance_matrix_100_01_35) - np.min(mean_distance_matrix_100_01_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total weight of the MST\n",
    "total_weight_100 = sum(nx.get_edge_attributes(mst_100_01_35, 'weight').values())\n",
    "\n",
    "# Print the total weight\n",
    "print(f\"Total weight of the MST: {total_weight_100}\")\n",
    "\n",
    "# Compute the minimum spanning tree of the graph\n",
    "mst_100_01_35 = nx.minimum_spanning_tree(G_100_01_35)\n",
    "np.save('mst_100_01_35.npy', mst_100_01_35)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_100_01_35, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_100_01_35, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_100_01_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_100_01_35, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=100, min_dist=0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_100_01_35 = np.std(distance_matrices_100_01_35, axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_100_01_35.npy\", distance_matrix_std_100_01_35)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (100_01_35):\\n\", distance_matrix_std_100_01_35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_100_01_35 = distance_matrix_std_100_01_35 / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_100_01_35 = z_score * sem_matrix_100_01_35\n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_100_01_35 = mean_distance_matrix_100_01_35 - margin_of_error_matrix_100_01_35\n",
    "upper_limit_intconf_matrix_100_01_35 = mean_distance_matrix_100_01_35 + margin_of_error_matrix_100_01_35\n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_100_01_35 = np.maximum(lower_limit_intconf_matrix_100_01_35, 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_100_01_35)\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_100_01_35)\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_100_01_35)\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_100_01_35.npy', lower_limit_intconf_matrix_100_01_35)\n",
    "np.save('upper_limit_intconf_matrix_100_01_35.npy', upper_limit_intconf_matrix_100_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_100_01_35 = normalize_matrix(lower_limit_intconf_matrix_100_01_35)\n",
    "norm_upper_limit_intconf_matrix_100_01_35 = normalize_matrix(upper_limit_intconf_matrix_100_01_35)\n",
    "np.save('norm_lower_limit_intconf_matrix_100_01_35.npy', norm_lower_limit_intconf_matrix_100_01_35)\n",
    "np.save('norm_upper_limit_intconf_matrix_100_01_35.npy', norm_upper_limit_intconf_matrix_100_01_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_100_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix (k=10, n_neighbors=100, min_dist=0.1)\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=100, min_dist=0.1)\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_100_01_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix (k=10, n_neighbors=100, min_dist=0.1)\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_100_01_35, \"UMAP MST - Mean Distances - n_neighbors=100, min_dist=0.1\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_100_01_35, \"UMAP MST - Lower Limit - n_neighbors=100, min_dist=0.1\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_100_01_35, \"UMAP MST - Upper Limit - n_neighbors=100, min_dist=0.1\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALIGNED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('aligned_projections_100_01_35.npy', aligned_projections)\n",
    "np.save('aligned_cluster_centroids_per_run_100_01_35.npy', aligned_cluster_centroids_per_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for ARI scores\n",
    "ari_scores = []\n",
    "\n",
    "# Iterate through each run's UMAP projections\n",
    "for i, run_projection in enumerate(umap_projections_100_01_35):\n",
    "    # Perform K-Means clustering on the UMAP projection for this run\n",
    "    kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "    cluster_labels = kmeans.fit_predict(run_projection)\n",
    "\n",
    "    # Compute ARI between true labels and cluster labels\n",
    "    ari_score = adjusted_rand_score(y_train, cluster_labels)\n",
    "    ari_scores.append(ari_score)\n",
    "    print(f\"Run {i}: Adjusted Rand Index (ARI): {ari_score}\")\n",
    "\n",
    "# Identify the best run based on ARI\n",
    "best_run_index = np.argmax(ari_scores)\n",
    "best_ari_score = ari_scores[best_run_index]\n",
    "\n",
    "print(f\"Best run based on ARI: Run {best_run_index} with ARI Score: {best_ari_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procrustes Alignment using the best run as the reference\n",
    "reference_projection_100 = umap_projections_100_01_35[best_run_index] # Best run based on ARI: Run 18 with ARI Score: 0.915\n",
    "\n",
    "aligned_projections_100 = []\n",
    "procrustes_distances_100 = []\n",
    "\n",
    "for i, run_projection in enumerate(umap_projections_100_01_35):\n",
    "    # Apply Procrustes alignment\n",
    "    mtx1, mtx2, disparity = procrustes(reference_projection_100, run_projection)\n",
    "    aligned_projections_100.append(mtx2)\n",
    "    procrustes_distances_100.append(disparity)\n",
    "    print(f\"Run {i}: Procrustes disparity: {disparity}\")\n",
    "\n",
    "aligned_projections_100 = np.array(aligned_projections_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the aligned projections for a few runs\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(3):  # Plot the first 3 aligned runs\n",
    "    plt.scatter(aligned_projections_100[i, :, 0], aligned_projections_100[i, :, 1], s=10, label=f\"Run {i}\")\n",
    "\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.title(\"Procrustes-Aligned UMAP Projections\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation of Procrustes disparities\n",
    "mean_disparity = np.mean(procrustes_distances_100)\n",
    "std_disparity = np.std(procrustes_distances_100)\n",
    "\n",
    "# Define a threshold for \"good\" runs (e.g., within one standard deviation)\n",
    "threshold = mean_disparity + std_disparity\n",
    "\n",
    "# Identify good runs\n",
    "good_runs = [i for i, disparity in enumerate(procrustes_distances_100) if disparity <= threshold]\n",
    "\n",
    "print(f\"Mean Procrustes disparity: {mean_disparity}\")\n",
    "print(f\"Standard deviation of disparities: {std_disparity}\")\n",
    "print(f\"Threshold for good runs: {threshold}\")\n",
    "print(f\"Good runs based on Procrustes disparity: {good_runs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO CORROBORATE\n",
    "# ## Recompute ARI scores after Procrustes alignment\n",
    "aligned_ari_scores_100 = []\n",
    "\n",
    "for aligned_projection in aligned_projections_100:\n",
    "    kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(aligned_projection)\n",
    "    ari_score = adjusted_rand_score(y_train, cluster_labels)\n",
    "    aligned_ari_scores_100.append(ari_score)\n",
    "\n",
    "# Output ARI scores\n",
    "print(f\"Aligned ARI scores: {aligned_ari_scores_100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids for each aligned run\n",
    "aligned_cluster_centroids_per_run_100 = []\n",
    "\n",
    "# Compute centroids for each aligned run\n",
    "for run in range(len(aligned_projections)):\n",
    "    aligned_projection = aligned_projections[run]  # Current aligned projection\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        # Select points belonging to the current cluster\n",
    "        cluster_points = aligned_projection[y_train == cluster_label]\n",
    "        # Compute the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    # Store centroids for this run\n",
    "    aligned_cluster_centroids_per_run_100.append(np.array(centroids))\n",
    "\n",
    "aligned_cluster_centroids_per_run_100 = np.array(aligned_cluster_centroids_per_run_100)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "# Output centroids for the first run (for verification)\n",
    "# print(f\"Cluster centroids for first aligned run:\\n{aligned_cluster_centroids_per_run_100[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_100 = np.zeros(10)\n",
    "std_dev_y_100 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = aligned_cluster_centroids_per_run_100[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = aligned_cluster_centroids_per_run_100[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_n5[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_n5[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_100)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_umap_100 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(len(aligned_cluster_centroids_per_run_100)):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = aligned_cluster_centroids_per_run_100[trial, cluster]\n",
    "\n",
    "        # Calculate the bounds for the 3 standard deviations range for x and y\n",
    "        mean_x, mean_y = np.mean(aligned_cluster_centroids_per_run_100[:, cluster, :], axis=0)\n",
    "        lower_bound_x, upper_bound_x = mean_x - 3 * std_dev_x_n5[cluster], mean_x + 3 * std_dev_x_n5[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 3 * std_dev_y_n5[cluster], mean_y + 3 * std_dev_y_n5[cluster]\n",
    "\n",
    "        # Check if the centroid is inside the 3 std range\n",
    "        inside_3_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "\n",
    "        # Append the data as a new row in the list\n",
    "        data_umap_100.append([trial + 1, cluster, centroid_coord, inside_3_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_umap_100 = pd.DataFrame(data_umap_100, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 3 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group the DataFrame by Trial\n",
    "grouped_by_trial = df_results_umap_100.groupby('Trial')\n",
    "\n",
    "# Step 2: Check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_with_all_true = grouped_by_trial['Inside 3 std'].all()\n",
    "\n",
    "# Step 3: Filter the trials where all clusters were True\n",
    "valid_trials_100 = trials_with_all_true[trials_with_all_true].index.tolist()\n",
    "\n",
    "# Step 4: Show the list of trials\n",
    "print(f\"Trials where all clusters are inside 3 std: {valid_trials}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter aligned projections to keep only valid runs\n",
    "# valid_runs = [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 29, 30, 31, 33, 34, 35]\n",
    "aligned_projections_100 = aligned_projections_100[[run - 1 for run in valid_trials_100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Placeholder for pairwise distance matrices\n",
    "distance_matrices_kmeans_100 = []\n",
    "\n",
    "# Compute pairwise distances between centroids for each run\n",
    "for centroids in aligned_cluster_centroids_per_run_100:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_kmeans_100.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_kmeans_100 = np.array(distance_matrices_kmeans_100)  # Shape: (n_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all runs\n",
    "mean_distance_matrix_kmeans_100 = np.mean(distance_matrices_kmeans_100, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_kmeans_100 = (\n",
    "    mean_distance_matrix_kmeans_100 - np.min(mean_distance_matrix_kmeans_100)\n",
    ") / (np.max(mean_distance_matrix_kmeans_100) - np.min(mean_distance_matrix_kmeans_100))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_kmeans_100, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (Precomputed KMeans Centroids, n_neighbors=100, min_dist=0.1)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "np.save('distance_matrices_kmeans_100.npy', distance_matrices_kmeans_100)\n",
    "np.save('mean_distance_matrix_kmeans_100.npy', mean_distance_matrix_kmeans_100)\n",
    "np.save('normalized_mean_distance_matrix_kmeans_100.npy', normalized_mean_distance_matrix_kmeans_100)\n",
    "\n",
    "# Output results\n",
    "print(\"Mean Distance Matrix (KMeans, Precomputed Centroids):\")\n",
    "print(mean_distance_matrix_kmeans_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter aligned_cluster_centroids_per_run to match valid runs\n",
    "aligned_cluster_centroids_per_run_filtered_100 = aligned_cluster_centroids_per_run[[run - 1 for run in valid_trials_100]]\n",
    "\n",
    "# Placeholder for pairwise distance matrices\n",
    "distance_matrices_100_01_35_al = []\n",
    "\n",
    "# Calculate pairwise distances between centroids for each valid run\n",
    "for centroids in aligned_cluster_centroids_per_run_filtered_100:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_100_01_35_al.append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_100_01_35_al = np.array(distance_matrices_100_01_35_al)  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_100_01_35_al = np.mean(distance_matrices_100_01_35_al, axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_100_01_35_al = (mean_distance_matrix_100_01_35_al - np.min(mean_distance_matrix_100_01_35_al)) / (\n",
    "    np.max(mean_distance_matrix_100_01_35_al) - np.min(mean_distance_matrix_100_01_35_al)\n",
    ")\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_01_35_al, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=100)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_100_01_35_al.npy', distance_matrices_100_01_35_al)\n",
    "np.save('mean_distance_matrix_neighbors_100_01_35_al.npy', mean_distance_matrix_100_01_35_al)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_100_01_35_al}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General UMAP Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define clusters\n",
    "clusters = np.arange(10)  # Clusters from 0 to 9\n",
    "\n",
    "# Define colors for each n_neighbors\n",
    "colors = {5: \"orange\", 10: \"blue\", 20: \"yellow\", 30: \"grey\", 50: \"green\", 100: \"red\"}\n",
    "\n",
    "# Create a PDF to save all the plots\n",
    "with PdfPages(\"Cluster_Confidence_Intervals.pdf\") as pdf:\n",
    "    # Iterate over each cluster as the base cluster\n",
    "    for base_cluster in clusters:\n",
    "        \n",
    "        # Define the data for each n_neighbors, adjusted for the base cluster\n",
    "        data = {\n",
    "            5: {\n",
    "                \"mean\": np.delete(mean_distance_matrix_5_01_35[base_cluster], base_cluster),  # Distances from base cluster\n",
    "                \"lower\": np.delete(lower_limit_intconf_matrix_5_01_35[base_cluster], base_cluster),  # Lower bounds\n",
    "                \"upper\": np.delete(upper_limit_intconf_matrix_5_01_35[base_cluster], base_cluster)   # Upper bounds\n",
    "            },\n",
    "            10: {\n",
    "                \"mean\": np.delete(mean_distance_matrix_10_01_35[base_cluster], base_cluster),\n",
    "                \"lower\": np.delete(lower_limit_intconf_matrix_10_01_35[base_cluster], base_cluster),\n",
    "                \"upper\": np.delete(upper_limit_intconf_matrix_10_01_35[base_cluster], base_cluster)\n",
    "            },\n",
    "            20: {\n",
    "                \"mean\": np.delete(mean_distance_matrix_20_01_35[base_cluster], base_cluster),\n",
    "                \"lower\": np.delete(lower_limit_intconf_matrix_20_01_35[base_cluster], base_cluster),\n",
    "                \"upper\": np.delete(upper_limit_intconf_matrix_20_01_35[base_cluster], base_cluster)\n",
    "            },\n",
    "            30: {\n",
    "                \"mean\": np.delete(mean_distance_matrix_30_01_35[base_cluster], base_cluster),\n",
    "                \"lower\": np.delete(lower_limit_intconf_matrix_30_01_35[base_cluster], base_cluster),\n",
    "                \"upper\": np.delete(upper_limit_intconf_matrix_30_01_35[base_cluster], base_cluster)\n",
    "            },\n",
    "            50: {\n",
    "                \"mean\": np.delete(mean_distance_matrix_50_01_35[base_cluster], base_cluster),\n",
    "                \"lower\": np.delete(lower_limit_intconf_matrix_50_01_35[base_cluster], base_cluster),\n",
    "                \"upper\": np.delete(upper_limit_intconf_matrix_50_01_35[base_cluster], base_cluster)\n",
    "            },\n",
    "            100: {\n",
    "                \"mean\": np.delete(mean_distance_matrix_100_01_35[base_cluster], base_cluster),\n",
    "                \"lower\": np.delete(lower_limit_intconf_matrix_100_01_35[base_cluster], base_cluster),\n",
    "                \"upper\": np.delete(upper_limit_intconf_matrix_100_01_35[base_cluster], base_cluster)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Define clusters to be compared against (excluding the base cluster)\n",
    "        compare_clusters = np.delete(clusters, base_cluster)\n",
    "\n",
    "        # Plotting\n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "        width = 0.15  # Bar width\n",
    "        x = np.arange(len(compare_clusters))  # X positions for clusters\n",
    "\n",
    "        for idx, (n_neighbors, values) in enumerate(data.items()):\n",
    "            # Calculate positions for the current set of bars\n",
    "            x_positions = x + (idx - len(data) / 2) * width\n",
    "\n",
    "            # Plot bars for the mean distances\n",
    "            ax.bar(\n",
    "                x_positions,\n",
    "                values[\"mean\"],  # Mean distances\n",
    "                yerr=[\n",
    "                    values[\"mean\"] - values[\"lower\"],  # Lower error\n",
    "                    values[\"upper\"] - values[\"mean\"]   # Upper error\n",
    "                ],\n",
    "                width=width,\n",
    "                color=colors[n_neighbors],\n",
    "                alpha=0.7,\n",
    "                label=f\"n={n_neighbors}\",\n",
    "                capsize=5\n",
    "            )\n",
    "\n",
    "        # Add labels, title, and legend\n",
    "        ax.set_xlabel(\"Clusters\", fontsize=14)\n",
    "        ax.set_ylabel(\"Distance\", fontsize=14)\n",
    "        ax.set_title(f\"Confidence Intervals of Distances from Cluster {base_cluster} to Other Clusters\", fontsize=16)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([f\"{i}\" for i in compare_clusters], fontsize=12)\n",
    "        ax.legend(title=\"n_neighbors\", fontsize=10)\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the figure to the PDF\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"PDF with cluster confidence intervals has been successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define clusters\n",
    "clusters = np.arange(10)  # Clusters from 0 to 9\n",
    "\n",
    "# Define colors for each n_neighbors\n",
    "colors = {5: \"orange\", 10: \"blue\", 20: \"yellow\", 30: \"grey\", 50: \"green\", 100: \"red\"}\n",
    "\n",
    "# Iterate over each cluster as the base cluster\n",
    "for base_cluster in clusters:\n",
    "    \n",
    "    # Define the data for each n_neighbors, adjusted for the base cluster\n",
    "    data = {\n",
    "        5: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_5_01_35[base_cluster], base_cluster),  # Distances from base cluster\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_5_01_35[base_cluster], base_cluster),  # Lower bounds\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_5_01_35[base_cluster], base_cluster)   # Upper bounds\n",
    "        },\n",
    "        10: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_10_01_35[base_cluster], base_cluster),\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_10_01_35[base_cluster], base_cluster),\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_10_01_35[base_cluster], base_cluster)\n",
    "        },\n",
    "        20: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_20_01_35[base_cluster], base_cluster),\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_20_01_35[base_cluster], base_cluster),\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_20_01_35[base_cluster], base_cluster)\n",
    "        },\n",
    "        30: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_30_01_35[base_cluster], base_cluster),\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_30_01_35[base_cluster], base_cluster),\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_30_01_35[base_cluster], base_cluster)\n",
    "        },\n",
    "        50: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_50_01_35[base_cluster], base_cluster),\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_50_01_35[base_cluster], base_cluster),\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_50_01_35[base_cluster], base_cluster)\n",
    "        },\n",
    "        100: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_100_01_35[base_cluster], base_cluster),\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_100_01_35[base_cluster], base_cluster),\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_100_01_35[base_cluster], base_cluster)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define clusters to be compared against (excluding the base cluster)\n",
    "    compare_clusters = np.delete(clusters, base_cluster)\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "    width = 0.15  # Bar width\n",
    "    x = np.arange(len(compare_clusters))  # X positions for clusters\n",
    "\n",
    "    for idx, (n_neighbors, values) in enumerate(data.items()):\n",
    "        # Calculate positions for the current set of bars\n",
    "        x_positions = x + (idx - len(data) / 2) * width\n",
    "\n",
    "        # Plot bars for the mean distances\n",
    "        ax.bar(\n",
    "            x_positions,\n",
    "            values[\"mean\"],  # Mean distances\n",
    "            yerr=[\n",
    "                values[\"mean\"] - values[\"lower\"],  # Lower error\n",
    "                values[\"upper\"] - values[\"mean\"]   # Upper error\n",
    "            ],\n",
    "            width=width,\n",
    "            color=colors[n_neighbors],\n",
    "            alpha=0.7,\n",
    "            label=f\"n={n_neighbors}\",\n",
    "            capsize=5\n",
    "        )\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    ax.set_xlabel(\"Clusters\", fontsize=14)\n",
    "    ax.set_ylabel(\"Distance\", fontsize=14)\n",
    "    ax.set_title(f\"Confidence Intervals of Distances from Cluster {base_cluster} to Other Clusters\", fontsize=16)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f\"{i}\" for i in compare_clusters], fontsize=12)\n",
    "    ax.legend(title=\"n_neighbors\", fontsize=10)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Define your matrices for each n_neighbors value\n",
    "matrices = {\n",
    "    5: {\n",
    "        \"mean\": normalized_mean_distance_matrix_5_08_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_5_08_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_5_08_35\n",
    "    },\n",
    "    10: {\n",
    "        \"mean\": normalized_mean_distance_matrix_10_08_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_10_08_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_10_08_35\n",
    "    },\n",
    "    20: {\n",
    "        \"mean\": normalized_mean_distance_matrix_20_08_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_20_08_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_20_08_35\n",
    "    },\n",
    "    30: {\n",
    "        \"mean\": normalized_mean_distance_matrix_30_08_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_30_08_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_30_08_35\n",
    "    },\n",
    "    50: {\n",
    "        \"mean\": normalized_mean_distance_matrix_50_08_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_50_08_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_50_08_35\n",
    "    },\n",
    "    100: {\n",
    "        \"mean\": normalized_mean_distance_matrix_100_08_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_100_08_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_100_08_35\n",
    "    }\n",
    "}\n",
    "\n",
    "# Open a PDF to save the plots\n",
    "with PdfPages('MST_UMAP_Comparisons min_dis=0.8.pdf') as pdf:\n",
    "    for n_neighbors, matrix_set in matrices.items():\n",
    "        # Set up the figure with three subplots\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # Plot MSTs for mean, lower, and upper matrices\n",
    "        plot_mst(matrix_set[\"mean\"], f\"MST UMAP - Mean Distances (n_neighbors={n_neighbors}, min_dis=0.8)\", axes[1], color='red')\n",
    "        plot_mst(matrix_set[\"lower\"], f\"MST UMAP - Lower Limit (n_neighbors={n_neighbors}, min_dis=0.8)\", axes[0], color='blue')\n",
    "        plot_mst(matrix_set[\"upper\"], f\"MST UMAP - Upper Limit (n_neighbors={n_neighbors}, min_dis=0.8)\", axes[2], color='green')\n",
    "        \n",
    "        # Adjust layout for better spacing\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the current figure to the PDF\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"PDF with MST UMAP Comparisons has been successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS vs UMAP Sammon's stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Downsample the Dataset Consistently\n",
    "def downsample_mnist_consistent(x_data, y_labels, sample_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Downsample the dataset consistently, returning indices to ensure\n",
    "    the same points are selected in both spaces.\n",
    "    \"\"\"\n",
    "    sampled_indices = []\n",
    "    unique_labels = np.unique(y_labels)\n",
    "    for label in unique_labels:\n",
    "        # Select indices for the current label\n",
    "        label_indices = np.where(y_labels == label)[0]\n",
    "        # Sample a fraction of points for this label\n",
    "        sampled_indices_label = resample(\n",
    "            label_indices, n_samples=int(len(label_indices) * sample_fraction), replace=False\n",
    "        )\n",
    "        sampled_indices.extend(sampled_indices_label)\n",
    "    return np.array(sampled_indices)\n",
    "\n",
    "# Get consistent indices for sampling\n",
    "sampled_indices = downsample_mnist_consistent(x_train_flattened, y_train, sample_fraction=0.1)\n",
    "\n",
    "# Step 2: Use the Sampled Indices to Extract Points from Both Spaces\n",
    "# Downsample the high-dimensional original space\n",
    "x_sampled = x_train_flattened[sampled_indices]  # Original high-dimensional space\n",
    "y_sampled = y_train[sampled_indices]            # Corresponding labels\n",
    "\n",
    "# Load the mean projections and downsample\n",
    "umap_projections_downsampled = {\n",
    "    10: np.load(\"mean_projection_10_01_35.npy\")[sampled_indices],  # Mean projection for n_neighbors=10\n",
    "    50: np.load(\"mean_projection_50_01_35.npy\")[sampled_indices],  # Mean projection for n_neighbors=50\n",
    "    100: np.load(\"mean_projection_100_01_35.npy\")[sampled_indices],  # Mean projection for n_neighbors=100\n",
    "}\n",
    "\n",
    "# Output shapes for verification\n",
    "print(f\"x_sampled shape: {x_sampled.shape}\")\n",
    "print(f\"y_sampled shape: {y_sampled.shape}\")\n",
    "for n_neighbors, projection in umap_projections_downsampled.items():\n",
    "    print(f\"UMAP (n_neighbors={n_neighbors}) downsampled shape: {projection.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn's pairwise_distances for better handling of large arrays\n",
    "# \"True\" distances between points in the original/high dimensional space\n",
    "pairwise_distances = sklearn_pairwise_distances(x_sampled, metric='euclidean')\n",
    "\n",
    "# Initialize and fit MDS\n",
    "# Uses MDS to create a reference (ideal or baseline) embedding in 2D while preserving the global structure of pairwise distances\n",
    "mds_model = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "mds_embedding = mds_model.fit_transform(pairwise_distances)\n",
    "\n",
    "def sammons_stress(original_distances, embedding_distances):\n",
    "    \"\"\"\n",
    "    Calculate Sammon's stress/error with normalization.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-9  # Avoid division by zero\n",
    "    original_distances = np.maximum(original_distances, epsilon)\n",
    "    normalization = np.sum(original_distances)  # Sum of all original distances\n",
    "    stress = np.sum(((original_distances - embedding_distances) ** 2) / original_distances)\n",
    "    return stress / normalization  # Normalize by the total sum of original distances\n",
    "\n",
    "# Above eq. quantifies the degree to which the low-dimensional embedding preserves the pairwise distances from the original space. Lower stress indicates better preservation.\n",
    "\n",
    "# Calculate Sammon's stress for UMAP embeddings\n",
    "# Evaluates how well each UMAP embedding preserves global structures compared to the original distances\n",
    "stress_results = {}\n",
    "original_distances = pairwise_distances\n",
    "for n_neighbors, umap_embedding in umap_projections_downsampled.items():\n",
    "    # Compute pairwise distances for the UMAP embedding\n",
    "    umap_distances = sklearn_pairwise_distances(umap_embedding, metric='euclidean')\n",
    "    # Calculate Sammon's stress\n",
    "    stress = sammons_stress(original_distances, umap_distances)\n",
    "    stress_results[n_neighbors] = stress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything between these and botton line works and should be consider final version - READY FOR OVERLEAF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the MDS indeces and results from MNIST silhouette Score + ARI + Acc... section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last version, is the one to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sampled indices\n",
    "sampled_indices_train_mds= np.load(\"sampled_indices_train_mds.npy\")\n",
    "sampled_indices_test_mds= np.load(\"sampled_indices_test_mds.npy\")\n",
    "\n",
    "# Load downsampled dataset\n",
    "x_train_sampled_mds= np.load(\"x_train_sampled_mds.npy\")\n",
    "y_train_sampled_mds= np.load(\"y_train_sampled_mds.npy\")\n",
    "x_test_sampled_mds= np.load(\"x_test_sampled_mds.npy\")\n",
    "y_test_sampled_mds= np.load(\"y_test_sampled_mds.npy\")\n",
    "\n",
    "# Load MDS embeddings\n",
    "x_train_mds_c2= np.load(\"x_train_mds_c2.npy\")\n",
    "x_test_mds_c2= np.load(\"x_test_mds_c2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mean projections and downsample\n",
    "umap_projections_downsampled = {\n",
    "    10: np.load(\"mean_projection_10_01_35.npy\")[sampled_indices_train_mds],  # Mean projection for n_neighbors=10\n",
    "    50: np.load(\"mean_projection_50_01_35.npy\")[sampled_indices_train_mds],  # Mean projection for n_neighbors=50\n",
    "    100: np.load(\"mean_projection_100_01_35.npy\")[sampled_indices_train_mds],  # Mean projection for n_neighbors=100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neighbors, projection in umap_projections_downsampled.items():\n",
    "    print(f\"UMAP (n_neighbors={n_neighbors}) downsampled shape: {projection.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Sammon's stress\n",
    "def sammons_stress(original_distances, embedding_distances):\n",
    "    \"\"\"\n",
    "    Calculate Sammon's stress/error with normalization.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-9  # Avoid division by zero\n",
    "    original_distances = np.maximum(original_distances, epsilon)  # Prevent zero distances\n",
    "    normalization = np.sum(original_distances)  # Sum of all original distances\n",
    "    stress = np.sum(((original_distances - embedding_distances) ** 2) / original_distances)\n",
    "    return stress / normalization  # Normalize by the total sum of original distances\n",
    "\n",
    "# Compute pairwise distances for the original MDS embedding\n",
    "original_distances = sklearn_pairwise_distances(x_train_mds_c2, metric='euclidean')\n",
    "print(f\"Original distances shape: {original_distances.shape}\")  # Should be (5996, 5996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Sammon's stress for UMAP embeddings\n",
    "stress_results = {}\n",
    "for n_neighbors, umap_embedding in umap_projections_downsampled.items():\n",
    "    # Compute pairwise distances for the UMAP embedding\n",
    "    umap_distances = sklearn_pairwise_distances(umap_embedding, metric='euclidean')\n",
    "    print(f\"UMAP (n_neighbors={n_neighbors}) distances shape: {umap_distances.shape}\")  # Should be (5996, 5996)\n",
    "\n",
    "    # Calculate Sammon's stress\n",
    "    stress = sammons_stress(original_distances, umap_distances)\n",
    "    stress_results[n_neighbors] = stress\n",
    "\n",
    "# Print the Sammon's stress results\n",
    "for n_neighbors, stress in stress_results.items():\n",
    "    print(f\"Sammon's stress for UMAP (n_neighbors={n_neighbors}): {stress}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Visualize Results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot MDS Embedding\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(x_train_mds_c2[:, 0], x_train_mds_c2[:, 1], c=y_train_sampled_mds, cmap='Spectral', s=5)\n",
    "plt.title(\"MDS Embedding\")\n",
    "plt.colorbar(label=\"Digit Label\")\n",
    "\n",
    "# Plot UMAP Embeddings for different n_neighbors\n",
    "for idx, n_neighbors in enumerate([10, 50, 100], start=2):\n",
    "    plt.subplot(2, 2, idx)\n",
    "    plt.scatter(\n",
    "        umap_projections_downsampled[n_neighbors][:, 0], \n",
    "        umap_projections_downsampled[n_neighbors][:, 1], \n",
    "        c=y_train_sampled_mds, cmap='Spectral', s=5\n",
    "    )\n",
    "    plt.title(f\"UMAP Embedding (n_neighbors={n_neighbors})\")\n",
    "    plt.colorbar(label=\"Digit Label\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procrustes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import procrustes\n",
    "\n",
    "# Calculate Procrustes distance between MDS and UMAP embeddings\n",
    "procrustes_results = {}\n",
    "for n_neighbors, umap_embedding in umap_projections_downsampled.items():\n",
    "    # Perform Procrustes analysis\n",
    "    mds_embedding = x_train_mds_c2  # Reference embedding (MDS)\n",
    "    _, umap_aligned, disparity = procrustes(mds_embedding, umap_embedding)\n",
    "    # Store the Procrustes distance (disparity)\n",
    "    procrustes_results[n_neighbors] = disparity\n",
    "\n",
    "# Print results\n",
    "for n_neighbors, distance in procrustes_results.items():\n",
    "    print(f\"Procrustes Distance for UMAP (n_neighbors={n_neighbors}): {np.round(distance,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammon's stress and Variability with t-student (due to runs= 10<30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over stress results\n",
    "for n_neighbors, run_stress_values in stress_results.items():\n",
    "    # Ensure run_stress_values is an array\n",
    "    run_stress_values = np.array(run_stress_values)\n",
    "\n",
    "    if run_stress_values.ndim == 0:  # Handle scalar case (not iterable)\n",
    "        run_stress_values = np.array([run_stress_values])  # Convert scalar to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.stats import t\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the UMAP Projections for the First 10 Runs\n",
    "umap_projections_dict = {\n",
    "    10: umap_projections_10_01_35[:10, sampled_indices_train_mds, :],  # First 10 runs for n_neighbors=10\n",
    "    50: umap_projections_50_01_35[:10, sampled_indices_train_mds, :],  # First 10 runs for n_neighbors=50\n",
    "    100: umap_projections_100_01_35[:10, sampled_indices_train_mds, :]  # First 10 runs for n_neighbors=100\n",
    "}\n",
    "\n",
    "# Step 2: Calculate Sammon's Stress for Each Run\n",
    "sammon_results = {}  # Dictionary to store stress results for each n_neighbors\n",
    "for n_neighbors, projections in umap_projections_dict.items():\n",
    "    stress_values = []\n",
    "    for run_number, run_projection in enumerate(projections, start=1):\n",
    "        # Compute pairwise distances for the UMAP embedding\n",
    "        try:\n",
    "            embedding_distances = pairwise_distances(run_projection, metric='euclidean')\n",
    "            # Calculate Sammon's stress\n",
    "            stress = sammons_stress(original_distances, embedding_distances)\n",
    "            stress_values.append((run_number, stress))\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating stress for n_neighbors={n_neighbors}, run={run_number}: {e}\")\n",
    "            continue\n",
    "    sammon_results[n_neighbors] = stress_values\n",
    "\n",
    "# Step 3: Print the Results for the First 10 Runs\n",
    "print(\"Sammon's Stress Results for the First 10 Runs:\")\n",
    "for n_neighbors, stress_values in sammon_results.items():\n",
    "    print(f\"\\nUMAP (n_neighbors={n_neighbors}):\")\n",
    "    for run_number, stress in stress_values:\n",
    "        print(f\"  Run {run_number}: Sammon's Stress = {stress:.6f}\")\n",
    "\n",
    "# Step 4: Update Variability Computation with Multiple Runs\n",
    "final_results = {}\n",
    "run_variability = {}\n",
    "\n",
    "for n_neighbors, stress_values in sammon_results.items():\n",
    "    run_stress_values = [stress for _, stress in stress_values]\n",
    "    mean_stress = np.mean(run_stress_values)\n",
    "    std_stress = np.std(run_stress_values, ddof=1)  # Use ddof=1 for sample standard deviation\n",
    "\n",
    "    # Calculate confidence interval using Student's t-distribution\n",
    "    if len(run_stress_values) > 1:  # Ensure enough data points for CI calculation\n",
    "        t_score = t.ppf(0.975, df=len(run_stress_values) - 1)\n",
    "        margin_of_error = t_score * (std_stress / np.sqrt(len(run_stress_values)))\n",
    "        confidence_interval = (mean_stress - margin_of_error, mean_stress + margin_of_error)\n",
    "    else:\n",
    "        confidence_interval = (mean_stress, mean_stress)\n",
    "\n",
    "    # Store final results\n",
    "    final_results[n_neighbors] = {\n",
    "        \"mean\": mean_stress,\n",
    "        \"std\": std_stress,\n",
    "        \"95% CI\": confidence_interval,\n",
    "        \"run_values\": run_stress_values\n",
    "    }\n",
    "    # Store standard deviation for run-to-run variability\n",
    "    run_variability[n_neighbors] = std_stress\n",
    "\n",
    "# Step 5: Print Final Results with Variability\n",
    "print(\"\\nSammon's Stress Results with Variability (10 Runs):\")\n",
    "for n_neighbors, stats in final_results.items():\n",
    "    print(f\"n_neighbors={n_neighbors}:\")\n",
    "    print(f\"  Mean Stress: {stats['mean']:.4f}\")\n",
    "    print(f\"  Standard Deviation Across Runs: {stats['std']:.4f}\")\n",
    "    print(f\"  95% Confidence Interval: {stats['95% CI']}\")\n",
    "    for run_idx, stress in enumerate(stats['run_values'], start=1):\n",
    "        print(f\"    Run {run_idx}: Stress={stress:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code is for adding std. deviation per run, but it is given extremely high values due to not normalized or errors in the formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to calculate Sammon's stress and its standard deviation per run\n",
    "# def calculate_stress_with_variability_fixed(umap_projections_dict, original_distances):\n",
    "#     stress_results_with_variability = {}\n",
    "    \n",
    "#     for n_neighbors, projections in umap_projections_dict.items():\n",
    "#         run_results = []\n",
    "#         for run_number, run_projection in enumerate(projections, start=1):\n",
    "#             try:\n",
    "#                 # Calculate pairwise distances for the embedding\n",
    "#                 embedding_distances = pairwise_distances(run_projection, metric='euclidean')\n",
    "                \n",
    "#                 # Compute Sammon's stress\n",
    "#                 stress = sammons_stress(original_distances, embedding_distances)\n",
    "                \n",
    "#                 # Compute standard deviation of pointwise stress values\n",
    "#                 pointwise_stress = ((original_distances - embedding_distances) ** 2) / np.maximum(original_distances, 1e-9)\n",
    "#                 std_dev = np.std(pointwise_stress.flatten())  # Flatten to compute correct standard deviation\n",
    "                \n",
    "#                 # Normalize the standard deviation relative to the stress\n",
    "#                 normalized_std_dev = std_dev / stress\n",
    "                \n",
    "#                 run_results.append((run_number, stress, std_dev, normalized_std_dev))\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error calculating stress for n_neighbors={n_neighbors}, run={run_number}: {e}\")\n",
    "        \n",
    "#         stress_results_with_variability[n_neighbors] = run_results\n",
    "    \n",
    "#     return stress_results_with_variability\n",
    "\n",
    "# # Call the fixed function\n",
    "# sammon_results_with_variability_fixed = calculate_stress_with_variability_fixed(umap_projections_dict, original_distances)\n",
    "\n",
    "# # Print results\n",
    "# print(\"Sammon's Stress Results with Per-Run Variability (Fixed):\")\n",
    "# for n_neighbors, results in sammon_results_with_variability_fixed.items():\n",
    "#     print(f\"\\nn_neighbors={n_neighbors}:\")\n",
    "#     for run_number, stress, std_dev, normalized_std_dev in results:\n",
    "#         print(f\"  Run {run_number}: Stress={stress:.4f}, Std Dev={std_dev:.4f}, Normalized Std Dev={normalized_std_dev:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Visualize Results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot MDS Embedding\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(mds_embedding[:, 0], mds_embedding[:, 1], c=y_sampled, cmap='Spectral', s=5)\n",
    "plt.title(\"MDS Embedding\")\n",
    "plt.colorbar(label=\"Digit Label\")\n",
    "\n",
    "# Plot UMAP Embeddings for different n_neighbors\n",
    "for idx, n_neighbors in enumerate([10, 50, 100], start=2):\n",
    "    plt.subplot(2, 2, idx)\n",
    "    plt.scatter(\n",
    "        umap_projections_downsampled[n_neighbors][:, 0], \n",
    "        umap_projections_downsampled[n_neighbors][:, 1], \n",
    "        c=y_sampled, cmap='Spectral', s=5\n",
    "    )\n",
    "    plt.title(f\"UMAP Embedding (n_neighbors={n_neighbors})\")\n",
    "    plt.colorbar(label=\"Digit Label\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Print Sammon's Stress Results\n",
    "print(\"Sammon's Stress/Error for UMAP:\")\n",
    "for n_neighbors, stress in stress_results.items():\n",
    "    print(f\"n_neighbors={n_neighbors}: {stress:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import procrustes\n",
    "\n",
    "# Calculate Procrustes distance for UMAP embeddings compared to MDS\n",
    "procrustes_distances = {}\n",
    "for n_neighbors, umap_embedding in umap_projections_downsampled.items():\n",
    "    # Apply Procrustes analysis\n",
    "    mtx1, mtx2, disparity = procrustes(mds_embedding, umap_embedding)\n",
    "    procrustes_distances[n_neighbors] = disparity\n",
    "\n",
    "# Print Procrustes distances\n",
    "print(\"Procrustes Distances (MDS vs UMAP):\")\n",
    "for n_neighbors, distance in procrustes_distances.items():\n",
    "    print(f\"n_neighbors={n_neighbors}: {distance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_coordinates(data):\n",
    "    \"\"\"Normalize data to the range [0, 1].\"\"\"\n",
    "    min_val = np.min(data, axis=0)\n",
    "    max_val = np.max(data, axis=0)\n",
    "    return (data - min_val) / (max_val - min_val)\n",
    "\n",
    "# Normalize MDS embedding\n",
    "mds_embedding_normalized = normalize_coordinates(mds_embedding)\n",
    "\n",
    "# Normalize UMAP embeddings\n",
    "umap_embeddings_normalized = {\n",
    "    n_neighbors: normalize_coordinates(embedding)\n",
    "    for n_neighbors, embedding in umap_projections_downsampled.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "for idx, n_neighbors in enumerate([10, 50, 100], start=1):\n",
    "    plt.subplot(1, 3, idx)\n",
    "    # Plot MDS in blue\n",
    "    plt.scatter(\n",
    "        mds_embedding_normalized[:, 0],\n",
    "        mds_embedding_normalized[:, 1],\n",
    "        c=\"blue\",\n",
    "        s=10,\n",
    "        alpha=0.5,\n",
    "        label=\"MDS\"\n",
    "    )\n",
    "    # Plot UMAP in red\n",
    "    plt.scatter(\n",
    "        umap_embeddings_normalized[n_neighbors][:, 0],\n",
    "        umap_embeddings_normalized[n_neighbors][:, 1],\n",
    "        c=\"red\",\n",
    "        s=10,\n",
    "        alpha=0.5,\n",
    "        marker=\"x\",\n",
    "        label=f\"UMAP (n_neighbors={n_neighbors})\"\n",
    "    )\n",
    "    plt.title(f\"MDS vs UMAP Overlay (n_neighbors={n_neighbors})\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_neighborhood_consistency(mds_embedding, umap_embedding, k=10):\n",
    "    \"\"\"\n",
    "    Calculate neighborhood consistency between MDS and UMAP embeddings.\n",
    "    Parameters:\n",
    "        mds_embedding (np.ndarray): The MDS embedding of shape (n_samples, 2).\n",
    "        umap_embedding (np.ndarray): The UMAP embedding of shape (n_samples, 2).\n",
    "        k (int): Number of neighbors to consider.\n",
    "    Returns:\n",
    "        avg_consistency (float): Average neighborhood consistency.\n",
    "    \"\"\"\n",
    "    # Compute pairwise distances\n",
    "    mds_distances = pairwise_distances(mds_embedding, metric=\"euclidean\")\n",
    "    umap_distances = pairwise_distances(umap_embedding, metric=\"euclidean\")\n",
    "\n",
    "    # Get k-nearest neighbors for each point\n",
    "    mds_neighbors = np.argsort(mds_distances, axis=1)[:, 1 : k + 1]  # Exclude self (index 0)\n",
    "    umap_neighbors = np.argsort(umap_distances, axis=1)[:, 1 : k + 1]\n",
    "\n",
    "    # Calculate overlap ratios for each point\n",
    "    overlap_ratios = []\n",
    "    for i in range(mds_neighbors.shape[0]):\n",
    "        overlap = len(set(mds_neighbors[i]).intersection(set(umap_neighbors[i])))\n",
    "        overlap_ratios.append(overlap / k)\n",
    "\n",
    "    # Compute average consistency\n",
    "    avg_consistency = np.mean(overlap_ratios)\n",
    "    return avg_consistency\n",
    "\n",
    "# Calculate neighborhood consistency for different n_neighbors in UMAP\n",
    "neighborhood_consistency_results = {}\n",
    "k_neighbors = 10  # Number of neighbors to consider\n",
    "for n_neighbors, umap_embedding in umap_projections_downsampled.items():\n",
    "    consistency = calculate_neighborhood_consistency(mds_embedding, umap_embedding, k=k_neighbors)\n",
    "    neighborhood_consistency_results[n_neighbors] = consistency\n",
    "\n",
    "# Print results\n",
    "print(\"Neighborhood Consistency between MDS and UMAP:\")\n",
    "for n_neighbors, consistency in neighborhood_consistency_results.items():\n",
    "    print(f\"n_neighbors={n_neighbors}: {consistency:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sammon's stress and Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import t\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Same analysis but with downsampling to 25% and 10 runs instead of all 35\n",
    "\n",
    "# Step 1: Downsample the Dataset Consistently\n",
    "def downsample_mnist_consistent(x_data, y_labels, sample_fraction=0.25):\n",
    "    \"\"\"\n",
    "    Downsample the dataset consistently, returning indices to ensure\n",
    "    the same points are selected in both spaces.\n",
    "    \"\"\"\n",
    "    sampled_indices = []\n",
    "    unique_labels = np.unique(y_labels)\n",
    "    for label in unique_labels:\n",
    "        # Select indices for the current label\n",
    "        label_indices = np.where(y_labels == label)[0]\n",
    "        # Sample a fraction of points for this label\n",
    "        sampled_indices_label = resample(\n",
    "            label_indices, n_samples=int(len(label_indices) * sample_fraction), replace=False\n",
    "        )\n",
    "        sampled_indices.extend(sampled_indices_label)\n",
    "    return np.array(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the dataset\n",
    "sampled_indices = downsample_mnist_consistent(x_train_flattened, y_train, sample_fraction=0.25)\n",
    "x_sampled = x_train_flattened[sampled_indices]  # Original high-dimensional space\n",
    "y_sampled = y_train[sampled_indices]            # Corresponding labels\n",
    "\n",
    "# Precompute pairwise distances for the original high-dimensional data\n",
    "original_distances = pairwise_distances(x_sampled, metric='euclidean')\n",
    "\n",
    "# Load the UMAP projections for 10 runs\n",
    "umap_projections_dict = {\n",
    "    10: umap_projections_10_01_35[:10, sampled_indices, :],  # First 10 runs\n",
    "    50: umap_projections_50_01_35[:10, sampled_indices, :],\n",
    "    100: umap_projections_100_01_35[:10, sampled_indices, :]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate Sammon's Stress for Each Run\n",
    "sammon_results = {}\n",
    "for n_neighbors, projections in umap_projections_dict.items():\n",
    "    stress_values = []\n",
    "    for run_number, run_projection in enumerate(projections, start=1):\n",
    "        embedding_distances = pairwise_distances(run_projection, metric='euclidean')\n",
    "        stress = sammons_stress(original_distances, embedding_distances)\n",
    "        stress_values.append((run_number, stress))\n",
    "    sammon_results[n_neighbors] = stress_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "final_results = {}\n",
    "run_variability = {}  # To store standard deviation across runs\n",
    "\n",
    "for n_neighbors, stress_values in sammon_results.items():\n",
    "    run_stress_values = [stress for _, stress in stress_values]\n",
    "    mean_stress = np.mean(run_stress_values)\n",
    "    std_stress = np.std(run_stress_values)  # Standard deviation across runs\n",
    "\n",
    "    # Calculate confidence interval using Gaussian assumptions\n",
    "    z_score = norm.ppf(0.975)  # z-score for 95% confidence interval\n",
    "    margin_of_error = z_score * (std_stress / np.sqrt(len(run_stress_values)))\n",
    "    confidence_interval = (\n",
    "        mean_stress - margin_of_error,\n",
    "        mean_stress + margin_of_error\n",
    "    )\n",
    "\n",
    "    # Store final results\n",
    "    final_results[n_neighbors] = {\n",
    "        \"mean\": mean_stress,\n",
    "        \"std\": std_stress,\n",
    "        \"95% CI\": confidence_interval,\n",
    "        \"run_values\": run_stress_values  # Per-run stress values for visualization\n",
    "    }\n",
    "    # Store standard deviation for run-to-run variability\n",
    "    run_variability[n_neighbors] = std_stress\n",
    "\n",
    "    # Print Results with Run Variability\n",
    "print(\"Sammon's Stress Results (10 Runs, Gaussian):\")\n",
    "for n_neighbors, stats in final_results.items():\n",
    "    print(f\"n_neighbors={n_neighbors}:\")\n",
    "    print(f\"  Mean Stress: {stats['mean']:.4f}\")\n",
    "    print(f\"  Standard Deviation Across Runs: {stats['std']:.4f}\")\n",
    "    print(f\"  95% Confidence Interval: {stats['95% CI']}\")\n",
    "    print(f\"  Standard Deviation Across Runs: {run_variability[n_neighbors]:.4f}\")\n",
    "    for run_idx, stress in enumerate(stats['run_values'], start=1):\n",
    "        print(f\"    Run {run_idx}: Stress={stress:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Downsample the Dataset Consistently\n",
    "# def downsample_mnist_consistent(x_data, y_labels, sample_fraction=0.15):\n",
    "#     \"\"\"\n",
    "#     Downsample the dataset consistently, returning indices to ensure\n",
    "#     the same points are selected in both spaces.\n",
    "#     \"\"\"\n",
    "#     sampled_indices = []\n",
    "#     unique_labels = np.unique(y_labels)\n",
    "#     for label in unique_labels:\n",
    "#         # Select indices for the current label\n",
    "#         label_indices = np.where(y_labels == label)[0]\n",
    "#         # Sample a fraction of points for this label\n",
    "#         sampled_indices_label = resample(\n",
    "#             label_indices, n_samples=int(len(label_indices) * sample_fraction), replace=False\n",
    "#         )\n",
    "#         sampled_indices.extend(sampled_indices_label)\n",
    "#     return np.array(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Calculate Sammon's Stress\n",
    "# def sammons_stress(original_distances, embedding_distances):\n",
    "#     \"\"\"\n",
    "#     Calculate Sammon's stress/error with normalization.\n",
    "#     \"\"\"\n",
    "#     epsilon = 1e-9  # Avoid division by zero\n",
    "#     original_distances = np.maximum(original_distances, epsilon)\n",
    "#     normalization = np.sum(original_distances)  # Sum of all original distances\n",
    "#     stress = np.sum(((original_distances - embedding_distances) ** 2) / original_distances)\n",
    "#     return stress / normalization  # Normalize by the total sum of original distances\n",
    "\n",
    "# # Sample the dataset\n",
    "# sampled_indices = downsample_mnist_consistent(x_train_flattened, y_train, sample_fraction=0.15)\n",
    "# x_sampled = x_train_flattened[sampled_indices]  # Original high-dimensional space\n",
    "# y_sampled = y_train[sampled_indices]            # Corresponding labels\n",
    "\n",
    "# # Precompute pairwise distances for the original high-dimensional data\n",
    "# original_distances = pairwise_distances(x_sampled, metric='euclidean')\n",
    "\n",
    "# # Load the UMAP projections for multiple runs\n",
    "# umap_projections_dict = {\n",
    "#     10: umap_projections_10_01_35[:, sampled_indices, :],\n",
    "#     50: umap_projections_50_01_35[:, sampled_indices, :],\n",
    "#     100: umap_projections_100_01_35[:, sampled_indices, :]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3: Calculate Sammon's Stress for Each Run and print the run number\n",
    "# sammon_results = {}\n",
    "# for n_neighbors, projections in umap_projections_dict.items():\n",
    "#     stress_values = []\n",
    "#     for run_number, run_projection in enumerate(projections, start=1):\n",
    "#         embedding_distances = pairwise_distances(run_projection, metric='euclidean')\n",
    "#         stress = sammons_stress(original_distances, embedding_distances)\n",
    "#         stress_values.append((run_number, stress))\n",
    "#     sammon_results[n_neighbors] = stress_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 4: Compute Mean, Standard Deviation, and Confidence Intervals\n",
    "# final_results = {}\n",
    "# for n_neighbors, stress_values in sammon_results.items():\n",
    "#     mean_stress = np.mean([stress for _, stress in stress_values])\n",
    "#     std_stress = np.std([stress for _, stress in stress_values])\n",
    "#     confidence_interval = t.interval(\n",
    "#         0.95, len(stress_values) - 1,\n",
    "#         loc=mean_stress, scale=std_stress / np.sqrt(len(stress_values))\n",
    "#     )\n",
    "#     final_results[n_neighbors] = {\n",
    "#         \"mean\": mean_stress,\n",
    "#         \"std\": std_stress,\n",
    "#         \"95% CI\": confidence_interval\n",
    "#     }\n",
    "\n",
    "# # Print Results with run number\n",
    "# print(\"Sammon's Stress Results:\")\n",
    "# for n_neighbors, stats in final_results.items():\n",
    "#     print(f\"n_neighbors={n_neighbors}: Mean={stats['mean']:.4f}, Std={stats['std']:.4f}, 95% CI={stats['95% CI']}\")\n",
    "#     for run_number, stress in sammon_results[n_neighbors]:\n",
    "#         print(f\"Run {run_number}: Stress={stress:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print sampled indices for each run\n",
    "# for run_number in range(35):\n",
    "#     sampled_indices = downsample_mnist_consistent(x_train_flattened, y_train, sample_fraction=0.1)\n",
    "#     print(f\"Run {run_number + 1}: Sampled Indices = {sampled_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize embeddings for different runs\n",
    "# for n_neighbors, projections in umap_projections_dict.items():\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     for run_number, run_projection in enumerate(projections, start=1):\n",
    "#         plt.scatter(run_projection[:, 0], run_projection[:, 1], label=f'Run {run_number}', alpha=0.5)\n",
    "#     plt.title(f'UMAP Projections for n_neighbors={n_neighbors}')\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(list(run_variability.keys()), list(run_variability.values()), color='skyblue')\n",
    "plt.xticks(list(run_variability.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Plot Stress Values for Each Run\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for n_neighbors, stats in final_results.items():\n",
    "    run_stress_values = stats.get('run_values', [])\n",
    "    plt.plot(\n",
    "        range(1, len(run_stress_values) + 1), run_stress_values,\n",
    "        label=f'n_neighbors={n_neighbors}', marker='o'\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Run Index\")\n",
    "plt.ylabel(\"Sammon's Stress\")\n",
    "plt.title(\"Sammon's Stress Across Runs for Different n_neighbors\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualization: Bar Chart for Standard Deviation Across Runs\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(list(run_variability.keys()), list(run_variability.values()), color='skyblue')\n",
    "plt.xticks(list(run_variability.keys()))\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylabel(\"Standard Deviation of Sammon's Stress\")\n",
    "plt.title(\"Run-to-Run Variability (Std. Dev.) of Sammon's Stress\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Using Gaussian distribution ####\n",
    "# from scipy.stats import norm\n",
    "\n",
    "# # Step 4: Compute Mean, Standard Deviation, and Confidence Intervals using Gaussian metric\n",
    "# final_results = {}\n",
    "# for n_neighbors, stress_values in sammon_results.items():\n",
    "#     mean_stress = np.mean([stress for _, stress in stress_values])  # Mean of stress values\n",
    "#     std_stress = np.std([stress for _, stress in stress_values])   # Standard deviation of stress values\n",
    "\n",
    "#     # Calculate 95% confidence interval using Gaussian assumptions\n",
    "#     z_score = norm.ppf(0.975)  # z-score for 95% confidence interval (two-tailed)\n",
    "#     margin_of_error = z_score * (std_stress / np.sqrt(len(stress_values)))\n",
    "#     confidence_interval = (\n",
    "#         mean_stress - margin_of_error,\n",
    "#         mean_stress + margin_of_error\n",
    "#     )\n",
    "\n",
    "#     final_results[n_neighbors] = {\n",
    "#         \"mean\": mean_stress,\n",
    "#         \"std\": std_stress,\n",
    "#         \"95% CI\": confidence_interval\n",
    "#     }\n",
    "\n",
    "# # Print Results with run number\n",
    "# print(\"Sammon's Stress Results (Gaussian):\")\n",
    "# for n_neighbors, stats in final_results.items():\n",
    "#     print(f\"n_neighbors={n_neighbors}: Mean={stats['mean']:.4f}, Std={stats['std']:.4f}, 95% CI={stats['95% CI']}\")\n",
    "#     for run_number, stress in sammon_results[n_neighbors]:\n",
    "#         print(f\"Run {run_number}: Stress={stress:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3: Compute Mean, Standard Deviation, and Confidence Intervals Using Gaussian Metric\n",
    "# from scipy.stats import norm\n",
    "\n",
    "# final_results = {}\n",
    "# for n_neighbors, stress_values in sammon_results.items():\n",
    "#     mean_stress = np.mean([stress for _, stress in stress_values])  # Mean of stress values\n",
    "#     std_stress = np.std([stress for _, stress in stress_values])   # Standard deviation of stress values\n",
    "\n",
    "#     # Calculate 95% confidence interval using Gaussian assumptions\n",
    "#     z_score = norm.ppf(0.975)  # z-score for 95% confidence interval (two-tailed)\n",
    "#     margin_of_error = z_score * (std_stress / np.sqrt(len(stress_values)))\n",
    "#     confidence_interval = (\n",
    "#         mean_stress - margin_of_error,\n",
    "#         mean_stress + margin_of_error\n",
    "#     )\n",
    "\n",
    "#     final_results[n_neighbors] = {\n",
    "#         \"mean\": mean_stress,\n",
    "#         \"std\": std_stress,\n",
    "#         \"95% CI\": confidence_interval\n",
    "#     }\n",
    "\n",
    "# # Print Results\n",
    "# print(\"Sammon's Stress Results (10 Runs, Gaussian):\")\n",
    "# for n_neighbors, stats in final_results.items():\n",
    "#     print(f\"n_neighbors={n_neighbors}: Mean={stats['mean']:.4f}, Std={stats['std']:.4f}, 95% CI={stats['95% CI']}\")\n",
    "#     for run_number, stress in sammon_results[n_neighbors]:\n",
    "#         print(f\"Run {run_number}: Stress={stress:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sammon's stress and Gaussian for T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate Sammon's Stress for t-SNE\n",
    "# Downsample t-SNE embedding\n",
    "tsne_embedding_downsampled = x_train_tsne_c2[sampled_indices]  # Downsampled t-SNE embedding\n",
    "\n",
    "# Compute pairwise distances for t-SNE embedding\n",
    "tsne_distances = pairwise_distances(tsne_embedding_downsampled, metric='euclidean')\n",
    "\n",
    "# Calculate Sammon's Stress\n",
    "from scipy.stats import norm\n",
    "\n",
    "tsne_stress = sammons_stress(original_distances, tsne_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Report Results\n",
    "# Since t-SNE is run only once, we use the stress value directly\n",
    "print(\"Sammon's Stress for t-SNE:\")\n",
    "print(f\"Stress: {tsne_stress:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize t-SNE Embedding (Optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(\n",
    "    tsne_embedding_downsampled[:, 0], tsne_embedding_downsampled[:, 1],\n",
    "    c=y_sampled, cmap='Spectral', s=10, alpha=0.7\n",
    ")\n",
    "plt.colorbar(label='Digit Labels')\n",
    "plt.title(\"t-SNE Embedding with Downsampling (25% of MNIST)\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate Sammon's Stress for t-SNE\n",
    "# Downsample t-SNE embedding\n",
    "pca_embedding_downsampled = x_train_pca_c2[sampled_indices]  # Downsampled pca embedding\n",
    "\n",
    "# Compute pairwise distances for t-SNE embedding\n",
    "pca_distances = pairwise_distances(pca_embedding_downsampled, metric='euclidean')\n",
    "\n",
    "# Calculate Sammon's Stress\n",
    "from scipy.stats import norm\n",
    "\n",
    "pca_stress = debug_sammons_stress (original_distances, pca_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Report Results\n",
    "# Since t-SNE is run only once, we use the stress value directly\n",
    "print(\"Sammon's Stress for PCA:\")\n",
    "print(f\"Stress: {pca_stress:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_sammons_stress(original_distances, embedding_distances):\n",
    "    epsilon = 1e-9\n",
    "    original_distances = np.maximum(original_distances, epsilon)\n",
    "    normalization = np.sum(original_distances)\n",
    "    stress_numerator = np.sum(((original_distances - embedding_distances) ** 2) / original_distances)\n",
    "    print(f\"Numerator: {stress_numerator}, Normalization: {normalization}\")\n",
    "    return stress_numerator / normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_distances_sample = embedding_distances[:5, :5]  # Example for t-SNE\n",
    "print(\"Embedding Distances Matrix (Sample):\")\n",
    "print(embedding_distances_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_factor = np.mean(original_distances) / np.mean(embedding_distances)\n",
    "embedding_distances_rescaled = embedding_distances * scaling_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptable radius with min_dis between cluster/2. Mix of local and global details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not fully adaptable per cluster. \n",
    "# It uses a fixed value for all clusters within a run, derived from half of the min. intercluster dist. across all clusters in that run. \n",
    "# This fixed radius is then used to count the neighbors for each cluster.\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate combined cluster metrics\n",
    "def calculate_combined_cluster_metrics(umap_projections, y_labels, n_clusters=10, n_runs=35):\n",
    "    # Step 1: Calculate Cluster Centers for Each Run\n",
    "    cluster_centers_full = []\n",
    "    for run_idx, x_umap in enumerate(umap_projections):\n",
    "        cluster_centers_run = []\n",
    "        for label in np.unique(y_labels):  # Iterate over all labels\n",
    "            cluster_points = x_umap[y_labels == label]\n",
    "            if len(cluster_points) > 0:\n",
    "                cluster_center = np.mean(cluster_points, axis=0)\n",
    "                cluster_centers_run.append(cluster_center)\n",
    "        cluster_centers_full.append(np.array(cluster_centers_run))\n",
    "    \n",
    "    cluster_centers_full = np.array(cluster_centers_full)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "    # Step 2: Calculate the Adaptable Radius (Combining Metrics)\n",
    "    radii_per_cluster = []\n",
    "    for run_idx, cluster_centers in enumerate(cluster_centers_full):\n",
    "        # Calculate intercluster distances for this run\n",
    "        intercluster_distances = cdist(cluster_centers, cluster_centers, metric='euclidean')\n",
    "        np.fill_diagonal(intercluster_distances, np.inf)  # Ignore self-distances\n",
    "        min_intercluster_distance = np.min(intercluster_distances) / 2  # Half of the minimum intercluster distance\n",
    "        \n",
    "        # Calculate intracluster radius for each cluster\n",
    "        radii_cluster = []\n",
    "        for cluster_idx, cluster_center in enumerate(cluster_centers):\n",
    "            cluster_points = umap_projections[run_idx][y_labels == cluster_idx]\n",
    "            if len(cluster_points) > 0:\n",
    "                intracluster_radius = np.mean(np.linalg.norm(cluster_points - cluster_center, axis=1))\n",
    "                # Combine intercluster and intracluster metrics\n",
    "                combined_radius = min(min_intercluster_distance, intracluster_radius)\n",
    "                radii_cluster.append(combined_radius)\n",
    "        radii_per_cluster.append(radii_cluster)\n",
    "    \n",
    "    # Average radius across runs for each cluster\n",
    "    radii_per_cluster_mean = np.mean(radii_per_cluster, axis=0)\n",
    "\n",
    "    # Step 3: Count Neighbors Using the Combined Radius\n",
    "    neighbor_counts_full = []\n",
    "    for run_idx, x_umap in enumerate(umap_projections):\n",
    "        counts_run = []\n",
    "        for cluster_idx, cluster_center in enumerate(cluster_centers_full[run_idx]):\n",
    "            radius = radii_per_cluster_mean[cluster_idx]  # Use the combined radius\n",
    "            distances_to_center = np.linalg.norm(x_umap - cluster_center, axis=1)\n",
    "            count = np.sum(distances_to_center <= radius)  # Count points within the radius\n",
    "            counts_run.append(count)\n",
    "        neighbor_counts_full.append(counts_run)\n",
    "    \n",
    "    neighbor_counts_full = np.array(neighbor_counts_full)  # Shape: (n_runs, n_clusters)\n",
    "    average_neighbor_counts = np.mean(neighbor_counts_full, axis=0)  # Average across runs\n",
    "\n",
    "    # Step 4: Calculate Intercluster Separation Metrics\n",
    "    separation_metrics = []\n",
    "    for run_idx, cluster_centers in enumerate(cluster_centers_full):\n",
    "        intercluster_distances = cdist(cluster_centers, cluster_centers, metric='euclidean')\n",
    "        np.fill_diagonal(intercluster_distances, np.inf)  # Ignore self-distances\n",
    "        separation_metrics.append(np.min(intercluster_distances))  # Minimum intercluster distance\n",
    "    \n",
    "    average_intercluster_separation = np.mean(separation_metrics)\n",
    "\n",
    "    return radii_per_cluster_mean, average_neighbor_counts, average_intercluster_separation\n",
    "\n",
    "# Define n_neighbors values\n",
    "n_neighbors_values = [10, 50, 100]\n",
    "\n",
    "# Placeholder for results\n",
    "results = []\n",
    "\n",
    "# Iterate over each n_neighbors value\n",
    "for n_neighbors in n_neighbors_values:\n",
    "    if n_neighbors == 10:\n",
    "        umap_projections = umap_projections_10_01_35\n",
    "    elif n_neighbors == 50:\n",
    "        umap_projections = umap_projections_50_01_35\n",
    "    elif n_neighbors == 100:\n",
    "        umap_projections = umap_projections_100_01_35\n",
    "\n",
    "    # Calculate combined metrics\n",
    "    radii_per_cluster, average_neighbor_counts, avg_intercluster_sep = calculate_combined_cluster_metrics(\n",
    "        umap_projections, y_train\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    for cluster_idx in range(len(radii_per_cluster)):\n",
    "        results.append({\n",
    "            \"N\": n_neighbors,\n",
    "            \"Cluster\": cluster_idx,\n",
    "            \"Radius\": np.round(radii_per_cluster[cluster_idx], 3),\n",
    "            \"Number of Neighbors\": np.round(average_neighbor_counts[cluster_idx], 0),\n",
    "            \"Intercluster Separation\": np.round(avg_intercluster_sep, 3)\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for easier analysis\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save results\n",
    "df_results.to_csv(\"combined_radius_neighbor_analysis.csv\", index=False)\n",
    "\n",
    "# Pivot table for easy comparison\n",
    "pivot_table = df_results.pivot(index=\"Cluster\", columns=\"N\", values=[\"Radius\", \"Number of Neighbors\", \"Intercluster Separation\"])\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=5, min_dist=0.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_5_00125_35= np.load('umap_projections_5_00125_35.npy')\n",
    "mean_umap_projection_5_00125_35= np.load('mean_projection_5_00125_35.npy')\n",
    "std_projection_umap_5_00125_35= np.load('std_projection_5_00125_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 5\n",
    "min_dist = 0.0125\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_5_00125_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_5_00125_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_5_00125_35 = np.array(umap_projections_5_00125_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_5_00125_35 = np.mean(umap_projections_5_00125_35, axis=0)\n",
    "std_projection_5_00125_35 = np.std(umap_projections_5_00125_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_5_00125_35.npy', umap_projections_5_00125_35)\n",
    "np.save('mean_projection_5_00125_35.npy', mean_projection_5_00125_35)\n",
    "np.save('std_projection_5_00125_35.npy', std_projection_5_00125_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_5_00125_35'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_5_00125_35  = np.sqrt(np.sum((umap_projections_5_00125_35 - mean_umap_projection_5_00125_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_5_00125_35 , axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_5_00125_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_5_00125_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_5_00125_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_5_00125_35  = np.array(distance_matrices_5_00125_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_5_00125_35  = np.mean(distance_matrices_5_00125_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_5_00125_35  = (mean_distance_matrix_5_00125_35  - np.min(mean_distance_matrix_5_00125_35 )) / (np.max(mean_distance_matrix_5_00125_35 ) - np.min(mean_distance_matrix_5_00125_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=5, , min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_5_00125_35 .npy', distance_matrices_5_00125_35)\n",
    "np.save('mean_distance_matrix_neighbors_5_00125_35 .npy', mean_distance_matrix_5_00125_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_5_00125_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_5_00125_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_5_00125_35 ,3))\n",
    "np.save('G_5_00125_35 .npy',G_5_00125_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_5_00125_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_5_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_5_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_5_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_5_00125_35  = nx.minimum_spanning_tree(G_5_00125_35 )\n",
    "np.save('mst_5_00125_35 .npy', mst_5_00125_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_5_00125_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_5_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_5_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_5_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=5, min_dist=0.0125\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_5_00125_35  = np.std(distance_matrices_5_00125_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_5_00125_35 .npy\", distance_matrix_std_5_00125_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_00125_35):\\n\", distance_matrix_std_5_00125_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_5_00125_35  = distance_matrix_std_5_00125_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_5_00125_35  = z_score * sem_matrix_5_00125_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_5_00125_35  = mean_distance_matrix_5_00125_35  - margin_of_error_matrix_5_00125_35 \n",
    "upper_limit_intconf_matrix_5_00125_35  = mean_distance_matrix_5_00125_35  + margin_of_error_matrix_5_00125_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_5_00125_35  = np.maximum(lower_limit_intconf_matrix_5_00125_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_5_00125_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_5_00125_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_5_00125_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_5_00125_35 .npy', lower_limit_intconf_matrix_5_00125_35 )\n",
    "np.save('upper_limit_intconf_matrix_5_00125_35 .npy', upper_limit_intconf_matrix_5_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_5_00125_35  = normalize_matrix(lower_limit_intconf_matrix_5_00125_35 )\n",
    "norm_upper_limit_intconf_matrix_5_00125_35  = normalize_matrix(upper_limit_intconf_matrix_5_00125_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_5_00125_35.npy', norm_lower_limit_intconf_matrix_5_00125_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_5_00125_35.npy', norm_upper_limit_intconf_matrix_5_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_5_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=5, min_dist=0.0125\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=5, min_dist=0.0125\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_5_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=5, min_dist=0.0125\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_5_00125_35 , \"UMAP MST - Mean Distances - n_neighbors=5, min_dist=0.0125\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_5_00125_35 , \"UMAP MST - Lower Limit - n_neighbors=5, min_dist=0.0125\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_5_00125_35 , \"UMAP MST - Upper Limit - n_neighbors=5, min_dist=0.0125\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=10, min_dist=0.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_10_00125_35= np.load('umap_projections_10_00125_35.npy')\n",
    "mean_umap_projection_10_00125_35= np.load('mean_projection_10_00125_35.npy')\n",
    "std_projection_umap_10_00125_35= np.load('std_projection_10_00125_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.0125\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_10_00125_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_10_00125_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_10_00125_35 = np.array(umap_projections_10_00125_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_10_00125_35 = np.mean(umap_projections_10_00125_35, axis=0)\n",
    "std_projection_10_00125_35 = np.std(umap_projections_10_00125_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_10_00125_35.npy', umap_projections_10_00125_35)\n",
    "np.save('mean_projection_10_00125_35.npy', mean_projection_10_00125_35)\n",
    "np.save('std_projection_10_00125_35.npy', std_projection_10_00125_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_10_00125_35'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_10_00125_35  = np.sqrt(np.sum((umap_projections_10_00125_35 - mean_umap_projection_10_00125_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_10_00125_35 , axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_10_00125_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_10_00125_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_10_00125_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_10_00125_35  = np.array(distance_matrices_10_00125_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_10_00125_35  = np.mean(distance_matrices_10_00125_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_10_00125_35  = (mean_distance_matrix_10_00125_35  - np.min(mean_distance_matrix_10_00125_35 )) / (np.max(mean_distance_matrix_10_00125_35 ) - np.min(mean_distance_matrix_10_00125_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=10, , min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_10_00125_35 .npy', distance_matrices_10_00125_35)\n",
    "np.save('mean_distance_matrix_neighbors_10_00125_35 .npy', mean_distance_matrix_10_00125_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_10_00125_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_10_00125_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_10_00125_35 ,3))\n",
    "np.save('G_10_00125_35 .npy',G_10_00125_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_10_00125_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_10_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_10_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_10_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_10_00125_35  = nx.minimum_spanning_tree(G_10_00125_35 )\n",
    "np.save('mst_10_00125_35 .npy', mst_10_00125_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_10_00125_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_10_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_10_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_10_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=10, min_dist=0.0125\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_10_00125_35  = np.std(distance_matrices_10_00125_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_10_00125_35 .npy\", distance_matrix_std_10_00125_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_00125_35):\\n\", distance_matrix_std_10_00125_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_10_00125_35  = distance_matrix_std_10_00125_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_10_00125_35  = z_score * sem_matrix_10_00125_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_10_00125_35  = mean_distance_matrix_10_00125_35  - margin_of_error_matrix_10_00125_35 \n",
    "upper_limit_intconf_matrix_10_00125_35  = mean_distance_matrix_10_00125_35  + margin_of_error_matrix_10_00125_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_10_00125_35  = np.maximum(lower_limit_intconf_matrix_10_00125_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_10_00125_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_10_00125_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_10_00125_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_10_00125_35 .npy', lower_limit_intconf_matrix_10_00125_35 )\n",
    "np.save('upper_limit_intconf_matrix_10_00125_35 .npy', upper_limit_intconf_matrix_10_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_10_00125_35  = normalize_matrix(lower_limit_intconf_matrix_10_00125_35 )\n",
    "norm_upper_limit_intconf_matrix_10_00125_35  = normalize_matrix(upper_limit_intconf_matrix_10_00125_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_10_00125_35.npy', norm_lower_limit_intconf_matrix_10_00125_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_10_00125_35.npy', norm_upper_limit_intconf_matrix_10_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_10_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=10, min_dist=0.0125\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=10, min_dist=0.0125\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_10_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=10, min_dist=0.0125\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_10_00125_35 , \"UMAP MST - Mean Distances - n_neighbors=10, min_dist=0.0125\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_10_00125_35 , \"UMAP MST - Lower Limit - n_neighbors=10, min_dist=0.0125\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_10_00125_35 , \"UMAP MST - Upper Limit - n_neighbors=10, min_dist=0.0125\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=20, min_dist=0.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_20_00125_35= np.load('umap_projections_20_00125_35.npy')\n",
    "mean_umap_projection_20_00125_35= np.load('mean_projection_20_00125_35.npy')\n",
    "std_projection_umap_20_00125_35= np.load('std_projection_20_00125_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 20\n",
    "min_dist = 0.0125\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_20_00125_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_20_00125_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_20_00125_35 = np.array(umap_projections_20_00125_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_20_00125_35 = np.mean(umap_projections_20_00125_35, axis=0)\n",
    "std_projection_20_00125_35 = np.std(umap_projections_20_00125_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_20_00125_35.npy', umap_projections_20_00125_35)\n",
    "np.save('mean_projection_20_00125_35.npy', mean_projection_20_00125_35)\n",
    "np.save('std_projection_20_00125_35.npy', std_projection_20_00125_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_20_00125_35'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_20_00125_35  = np.sqrt(np.sum((umap_projections_20_00125_35 - mean_umap_projection_20_00125_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_20_00125_35 , axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_20_00125_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_20_00125_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_20_00125_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_20_00125_35  = np.array(distance_matrices_20_00125_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_20_00125_35  = np.mean(distance_matrices_20_00125_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_20_00125_35  = (mean_distance_matrix_20_00125_35  - np.min(mean_distance_matrix_20_00125_35 )) / (np.max(mean_distance_matrix_20_00125_35 ) - np.min(mean_distance_matrix_20_00125_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_20_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=10, , min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_20_00125_35 .npy', distance_matrices_20_00125_35)\n",
    "np.save('mean_distance_matrix_neighbors_20_00125_35 .npy', mean_distance_matrix_20_00125_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_20_00125_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_20_00125_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_20_00125_35 ,3))\n",
    "np.save('G_20_00125_35 .npy',G_20_00125_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_20_00125_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_20_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_20_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_20_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_20_00125_35  = nx.minimum_spanning_tree(G_20_00125_35 )\n",
    "np.save('mst_20_00125_35 .npy', mst_20_00125_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_20_00125_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_20_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_20_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_20_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=20, min_dist=0.0125\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_20_00125_35  = np.std(distance_matrices_20_00125_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_20_00125_35 .npy\", distance_matrix_std_20_00125_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_00125_35):\\n\", distance_matrix_std_20_00125_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_20_00125_35  = distance_matrix_std_20_00125_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_20_00125_35  = z_score * sem_matrix_20_00125_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_20_00125_35  = mean_distance_matrix_20_00125_35  - margin_of_error_matrix_20_00125_35 \n",
    "upper_limit_intconf_matrix_20_00125_35  = mean_distance_matrix_20_00125_35  + margin_of_error_matrix_20_00125_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_20_00125_35  = np.maximum(lower_limit_intconf_matrix_20_00125_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_20_00125_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_20_00125_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_20_00125_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_20_00125_35 .npy', lower_limit_intconf_matrix_20_00125_35 )\n",
    "np.save('upper_limit_intconf_matrix_20_00125_35 .npy', upper_limit_intconf_matrix_20_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_20_00125_35  = normalize_matrix(lower_limit_intconf_matrix_20_00125_35 )\n",
    "norm_upper_limit_intconf_matrix_20_00125_35  = normalize_matrix(upper_limit_intconf_matrix_20_00125_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_20_00125_35.npy', norm_lower_limit_intconf_matrix_20_00125_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_20_00125_35.npy', norm_upper_limit_intconf_matrix_20_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_20_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=20, min_dist=0.0125\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_20_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=20, min_dist=0.0125\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_20_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=20, min_dist=0.0125\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_20_00125_35 , \"UMAP MST - Mean Distances - n_neighbors=20, min_dist=0.0125\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_20_00125_35 , \"UMAP MST - Lower Limit - n_neighbors=20, min_dist=0.0125\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_20_00125_35 , \"UMAP MST - Upper Limit - n_neighbors=20, min_dist=0.0125\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=30, min_dist=0.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_30_00125_35= np.load('umap_projections_30_00125_35.npy')\n",
    "mean_umap_projection_30_00125_35= np.load('mean_projection_30_00125_35.npy')\n",
    "std_projection_umap_30_00125_35= np.load('std_projection_30_00125_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 30\n",
    "min_dist = 0.0125\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_30_00125_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_30_00125_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_30_00125_35 = np.array(umap_projections_30_00125_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_30_00125_35 = np.mean(umap_projections_30_00125_35, axis=0)\n",
    "std_projection_30_00125_35 = np.std(umap_projections_30_00125_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_30_00125_35.npy', umap_projections_30_00125_35)\n",
    "np.save('mean_projection_30_00125_35.npy', mean_projection_30_00125_35)\n",
    "np.save('std_projection_30_00125_35.npy', std_projection_30_00125_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_30_00125_35'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_30_00125_35  = np.sqrt(np.sum((umap_projections_30_00125_35 - mean_umap_projection_30_00125_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_30_00125_35 , axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_30_00125_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_30_00125_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_30_00125_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_30_00125_35  = np.array(distance_matrices_30_00125_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_30_00125_35  = np.mean(distance_matrices_30_00125_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_30_00125_35  = (mean_distance_matrix_30_00125_35  - np.min(mean_distance_matrix_30_00125_35 )) / (np.max(mean_distance_matrix_30_00125_35 ) - np.min(mean_distance_matrix_30_00125_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_30_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=10, , min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_30_00125_35 .npy', distance_matrices_30_00125_35)\n",
    "np.save('mean_distance_matrix_neighbors_30_00125_35 .npy', mean_distance_matrix_30_00125_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_30_00125_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_30_00125_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_30_00125_35 ,3))\n",
    "np.save('G_30_00125_35 .npy',G_30_00125_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_30_00125_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_30_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_30_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_30_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_30_00125_35  = nx.minimum_spanning_tree(G_30_00125_35 )\n",
    "np.save('mst_30_00125_35 .npy', mst_30_00125_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_30_00125_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_30_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_30_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_30_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=30, min_dist=0.0125\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_30_00125_35  = np.std(distance_matrices_30_00125_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_30_00125_35 .npy\", distance_matrix_std_30_00125_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_00125_35):\\n\", distance_matrix_std_30_00125_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_30_00125_35  = distance_matrix_std_30_00125_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_30_00125_35  = z_score * sem_matrix_30_00125_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_30_00125_35  = mean_distance_matrix_30_00125_35  - margin_of_error_matrix_30_00125_35 \n",
    "upper_limit_intconf_matrix_30_00125_35  = mean_distance_matrix_30_00125_35  + margin_of_error_matrix_30_00125_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_30_00125_35  = np.maximum(lower_limit_intconf_matrix_30_00125_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_30_00125_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_30_00125_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_30_00125_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_30_00125_35 .npy', lower_limit_intconf_matrix_30_00125_35 )\n",
    "np.save('upper_limit_intconf_matrix_30_00125_35 .npy', upper_limit_intconf_matrix_30_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_30_00125_35  = normalize_matrix(lower_limit_intconf_matrix_30_00125_35 )\n",
    "norm_upper_limit_intconf_matrix_30_00125_35  = normalize_matrix(upper_limit_intconf_matrix_30_00125_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_30_00125_35.npy', norm_lower_limit_intconf_matrix_30_00125_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_30_00125_35.npy', norm_upper_limit_intconf_matrix_30_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_30_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=30, min_dist=0.0125\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_30_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=30, min_dist=0.0125\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_30_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=30, min_dist=0.0125\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_30_00125_35 , \"UMAP MST - Mean Distances - n_neighbors=30, min_dist=0.0125\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_30_00125_35 , \"UMAP MST - Lower Limit - n_neighbors=30, min_dist=0.0125\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_30_00125_35 , \"UMAP MST - Upper Limit - n_neighbors=30, min_dist=0.0125\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=50, min_dist=0.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_50_00125_35= np.load('umap_projections_50_00125_35.npy')\n",
    "mean_umap_projection_50_00125_35= np.load('mean_projection_50_00125_35.npy')\n",
    "std_projection_umap_50_00125_35= np.load('std_projection_50_00125_35.npy')\n",
    "distance_matrices_50_00125_35= np.load('distance_matrices_neighbors_50_00125_35 .npy')\n",
    "mean_distance_matrix_50_00125_35= np.load('mean_distance_matrix_neighbors_50_00125_35 .npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 50\n",
    "min_dist = 0.0125\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "    \n",
    "# Store UMAP projections for each run\n",
    "umap_projections_50_00125_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_50_00125_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_50_00125_35 = np.array(umap_projections_50_00125_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_50_00125_35 = np.mean(umap_projections_50_00125_35, axis=0)\n",
    "std_projection_50_00125_35 = np.std(umap_projections_50_00125_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_50_00125_35.npy', umap_projections_50_00125_35)\n",
    "np.save('mean_projection_50_00125_35.npy', mean_projection_50_00125_35)\n",
    "np.save('std_projection_50_00125_35.npy', std_projection_50_00125_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_50_00125_35'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (e.g., 10)\n",
    "n_clusters = 10\n",
    "\n",
    "# Number of runs (e.g., 35)\n",
    "n_runs = umap_projections_50_00125_35.shape[0]\n",
    "\n",
    "# Array to store KMeans centroids for all runs\n",
    "kmeans_centroids_50_00125 = np.zeros((n_runs, n_clusters, umap_projections_50_00125_35.shape[2]))\n",
    "\n",
    "# Apply KMeans for each run and store centroids\n",
    "for run in range(n_runs):\n",
    "    umap_projection = umap_projections_50_00125_35[run]  # Shape (n_samples, n_dimensions)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(umap_projection)\n",
    "    kmeans_centroids_50_00125[run] = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_50_00125 = np.zeros(10)\n",
    "std_dev_y_50_00125 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_50_00125[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_50_00125[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_50_00125[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_50_00125[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_50_00125)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_50_00125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_50_00125_35  = np.sqrt(np.sum((umap_projections_50_00125_35 - mean_umap_projection_50_00125_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_50_00125_35 , axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_50_00125_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_50_00125_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_50_00125_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_50_00125_35  = np.array(distance_matrices_50_00125_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_50_00125_35  = np.mean(distance_matrices_50_00125_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_50_00125_35  = (mean_distance_matrix_50_00125_35  - np.min(mean_distance_matrix_50_00125_35 )) / (np.max(mean_distance_matrix_50_00125_35 ) - np.min(mean_distance_matrix_50_00125_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_50_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=50, , min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_50_00125_35 .npy', distance_matrices_50_00125_35)\n",
    "np.save('mean_distance_matrix_neighbors_50_00125_35 .npy', mean_distance_matrix_50_00125_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_50_00125_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_50_00125_35  = (mean_distance_matrix_50_00125_35  - np.min(mean_distance_matrix_50_00125_35 )) / (np.max(mean_distance_matrix_50_00125_35 ) - np.min(mean_distance_matrix_50_00125_35 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_50_00125_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_50_00125_35 ,3))\n",
    "np.save('G_50_00125_35 .npy',G_50_00125_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_50_00125_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_50_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_50_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_50_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total weight of the MST\n",
    "total_weight_50_00125 = sum(nx.get_edge_attributes(mst_50_00125_35, 'weight').values())\n",
    "\n",
    "# Print the total weight\n",
    "print(f\"Total weight of the MST: {total_weight_50_00125}\")\n",
    "\n",
    "# Compute the minimum spanning tree of the graph\n",
    "mst_50_00125_35  = nx.minimum_spanning_tree(G_50_00125_35 )\n",
    "np.save('mst_50_00125_35 .npy', mst_50_00125_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_50_00125_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_50_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_50_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_50_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=50, min_dist=0.0125\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_50_00125_35  = np.std(distance_matrices_50_00125_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_50_00125_35 .npy\", distance_matrix_std_50_00125_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_00125_35):\\n\", distance_matrix_std_50_00125_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_50_00125_35  = distance_matrix_std_50_00125_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_50_00125_35  = z_score * sem_matrix_50_00125_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_50_00125_35  = mean_distance_matrix_50_00125_35  - margin_of_error_matrix_50_00125_35 \n",
    "upper_limit_intconf_matrix_50_00125_35  = mean_distance_matrix_50_00125_35  + margin_of_error_matrix_50_00125_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_50_00125_35  = np.maximum(lower_limit_intconf_matrix_50_00125_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_50_00125_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_50_00125_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_50_00125_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_50_00125_35 .npy', lower_limit_intconf_matrix_50_00125_35 )\n",
    "np.save('upper_limit_intconf_matrix_50_00125_35 .npy', upper_limit_intconf_matrix_50_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_50_00125_35  = normalize_matrix(lower_limit_intconf_matrix_50_00125_35 )\n",
    "norm_upper_limit_intconf_matrix_50_00125_35  = normalize_matrix(upper_limit_intconf_matrix_50_00125_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_50_00125_35.npy', norm_lower_limit_intconf_matrix_50_00125_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_50_00125_35.npy', norm_upper_limit_intconf_matrix_50_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_50_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=50, min_dist=0.0125\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_50_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=50, min_dist=0.0125\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_50_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=50, min_dist=0.0125\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_50_00125_35 , \"UMAP MST - Mean Distances - n_neighbors=50, min_dist=0.0125\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_50_00125_35 , \"UMAP MST - Lower Limit - n_neighbors=50, min_dist=0.0125\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_50_00125_35 , \"UMAP MST - Upper Limit - n_neighbors=50, min_dist=0.0125\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=100, min_dist=0.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_100_00125_35= np.load('umap_projections_100_00125_35.npy')\n",
    "mean_umap_projection_100_00125_35= np.load('mean_projection_100_00125_35.npy')\n",
    "std_projection_umap_100_00125_35= np.load('std_projection_100_00125_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 100\n",
    "min_dist = 0.0125\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_100_00125_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_100_00125_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_100_00125_35 = np.array(umap_projections_100_00125_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_100_00125_35 = np.mean(umap_projections_100_00125_35, axis=0)\n",
    "std_projection_100_00125_35 = np.std(umap_projections_100_00125_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_100_00125_35.npy', umap_projections_100_00125_35)\n",
    "np.save('mean_projection_100_00125_35.npy', mean_projection_100_00125_35)\n",
    "np.save('std_projection_100_00125_35.npy', std_projection_100_00125_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_100_00125_35'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_100_00125_35  = np.sqrt(np.sum((umap_projections_100_00125_35 - mean_umap_projection_100_00125_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_100_00125_35 , axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_100_00125_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_100_00125_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_100_00125_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_100_00125_35  = np.array(distance_matrices_100_00125_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_100_00125_35  = np.mean(distance_matrices_100_00125_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_100_00125_35  = (mean_distance_matrix_100_00125_35  - np.min(mean_distance_matrix_100_00125_35 )) / (np.max(mean_distance_matrix_100_00125_35 ) - np.min(mean_distance_matrix_100_00125_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=100, min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_100_00125_35 .npy', distance_matrices_100_00125_35)\n",
    "np.save('mean_distance_matrix_neighbors_100_00125_35 .npy', mean_distance_matrix_100_00125_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_100_00125_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_100_00125_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_100_00125_35 ,3))\n",
    "np.save('G_100_00125_35 .npy',G_100_00125_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_100_00125_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_100_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_100_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_100_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_100_00125_35  = nx.minimum_spanning_tree(G_100_00125_35 )\n",
    "np.save('mst_100_00125_35 .npy', mst_100_00125_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_100_00125_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_100_00125_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_100_00125_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_100_00125_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=100, min_dist=0.0125\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_100_00125_35  = np.std(distance_matrices_100_00125_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_100_00125_35 .npy\", distance_matrix_std_100_00125_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_00125_35):\\n\", distance_matrix_std_100_00125_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_100_00125_35  = distance_matrix_std_100_00125_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_100_00125_35  = z_score * sem_matrix_100_00125_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_100_00125_35  = mean_distance_matrix_100_00125_35  - margin_of_error_matrix_100_00125_35 \n",
    "upper_limit_intconf_matrix_100_00125_35  = mean_distance_matrix_100_00125_35  + margin_of_error_matrix_100_00125_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_100_00125_35  = np.maximum(lower_limit_intconf_matrix_100_00125_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_100_00125_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_100_00125_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_100_00125_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_100_00125_35 .npy', lower_limit_intconf_matrix_100_00125_35 )\n",
    "np.save('upper_limit_intconf_matrix_100_00125_35 .npy', upper_limit_intconf_matrix_100_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_100_00125_35  = normalize_matrix(lower_limit_intconf_matrix_100_00125_35 )\n",
    "norm_upper_limit_intconf_matrix_100_00125_35  = normalize_matrix(upper_limit_intconf_matrix_100_00125_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_100_00125_35.npy', norm_lower_limit_intconf_matrix_100_00125_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_100_00125_35.npy', norm_upper_limit_intconf_matrix_100_00125_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_100_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=100, min_dist=0.0125\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=100, min_dist=0.0125\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_100_00125_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=100, min_dist=0.0125\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_100_00125_35 , \"UMAP MST - Mean Distances - n_neighbors=100, min_dist=0.0125\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_100_00125_35 , \"UMAP MST - Lower Limit - n_neighbors=100, min_dist=0.0125\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_100_00125_35 , \"UMAP MST - Upper Limit - n_neighbors=100, min_dist=0.0125\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=5, min_dist=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_5_08_35= np.load('umap_projections_5_08_35.npy')\n",
    "mean_umap_projection_5_08_35= np.load('mean_projection_5_08_35.npy')\n",
    "std_projection_umap_5_08_35= np.load('std_projection_5_08_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 5\n",
    "min_dist = 0.8\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_5_08_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_5_08_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_5_08_35 = np.array(umap_projections_5_08_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_5_08_35 = np.mean(umap_projections_5_08_35, axis=0)\n",
    "std_projection_5_08_35 = np.std(umap_projections_5_08_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_5_08_35.npy', umap_projections_5_08_35)\n",
    "np.save('mean_projection_5_08_35.npy', mean_projection_5_08_35)\n",
    "np.save('std_projection_5_08_35.npy', std_projection_5_08_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_5_08_35'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_5_08_35  = np.sqrt(np.sum((umap_projections_5_08_35 - mean_umap_projection_5_08_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_5_08_35 , axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_5_08_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_5_08_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_5_08_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_5_08_35  = np.array(distance_matrices_5_08_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_5_08_35  = np.mean(distance_matrices_5_08_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_5_08_35  = (mean_distance_matrix_5_08_35  - np.min(mean_distance_matrix_5_08_35 )) / (np.max(mean_distance_matrix_5_08_35 ) - np.min(mean_distance_matrix_5_08_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=100, min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_5_08_35 .npy', distance_matrices_5_08_35)\n",
    "np.save('mean_distance_matrix_neighbors_5_08_35 .npy', mean_distance_matrix_5_08_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_5_08_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_5_08_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_5_08_35 ,3))\n",
    "np.save('G_5_08_35 .npy',G_5_08_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_5_08_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_5_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_5_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_5_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_5_08_35  = nx.minimum_spanning_tree(G_5_08_35 )\n",
    "np.save('mst_5_08_35 .npy', mst_5_08_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_5_08_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_5_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_5_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_5_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=5, min_dist=0.8\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_5_08_35  = np.std(distance_matrices_5_08_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_5_08_35 .npy\", distance_matrix_std_5_08_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (5_00125_35):\\n\", distance_matrix_std_5_08_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_5_08_35  = distance_matrix_std_5_08_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_5_08_35  = z_score * sem_matrix_5_08_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_5_08_35  = mean_distance_matrix_5_08_35  - margin_of_error_matrix_5_08_35 \n",
    "upper_limit_intconf_matrix_5_08_35  = mean_distance_matrix_5_08_35  + margin_of_error_matrix_5_08_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_5_08_35  = np.maximum(lower_limit_intconf_matrix_5_08_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_5_08_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_5_08_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_5_08_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_5_08_35 .npy', lower_limit_intconf_matrix_5_08_35 )\n",
    "np.save('upper_limit_intconf_matrix_5_08_35 .npy', upper_limit_intconf_matrix_5_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_5_08_35  = normalize_matrix(lower_limit_intconf_matrix_5_08_35 )\n",
    "norm_upper_limit_intconf_matrix_5_08_35  = normalize_matrix(upper_limit_intconf_matrix_5_08_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_5_08_35.npy', norm_lower_limit_intconf_matrix_5_08_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_5_08_35.npy', norm_upper_limit_intconf_matrix_5_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_5_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=5, min_dist=0.8\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=5, min_dist=0.8\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_5_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=5, min_dist=0.8\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_5_08_35 , \"UMAP MST - Mean Distances - n_neighbors=5, min_dist=0.8\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_5_08_35 , \"UMAP MST - Lower Limit - n_neighbors=5, min_dist=0.8\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_5_08_35 , \"UMAP MST - Upper Limit - n_neighbors=5, min_dist=0.8\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=10, min_dist=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_10_08_35= np.load('umap_projections_10_08_35.npy')\n",
    "mean_umap_projection_10_08_35= np.load('mean_projection_10_08_35.npy')\n",
    "std_projection_umap_10_08_35= np.load('std_projection_10_08_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.8\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_10_08_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_10_08_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_10_08_35 = np.array(umap_projections_10_08_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_10_08_35 = np.mean(umap_projections_10_08_35, axis=0)\n",
    "std_projection_10_08_35 = np.std(umap_projections_10_08_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_10_08_35.npy', umap_projections_10_08_35)\n",
    "np.save('mean_projection_10_08_35.npy', mean_projection_10_08_35)\n",
    "np.save('std_projection_10_08_35.npy', std_projection_10_08_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_10_08_35'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_10_08_35  = np.sqrt(np.sum((umap_projections_10_08_35 - mean_umap_projection_10_08_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_10_08_35 , axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_10_08_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_10_08_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_10_08_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_10_08_35  = np.array(distance_matrices_10_08_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_10_08_35  = np.mean(distance_matrices_10_08_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_10_08_35  = (mean_distance_matrix_10_08_35  - np.min(mean_distance_matrix_10_08_35 )) / (np.max(mean_distance_matrix_10_08_35 ) - np.min(mean_distance_matrix_10_08_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=10, min_dists=0.8)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_10_08_35 .npy', distance_matrices_10_08_35)\n",
    "np.save('mean_distance_matrix_neighbors_10_08_35 .npy', mean_distance_matrix_10_08_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_10_08_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_10_08_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_10_08_35 ,3))\n",
    "np.save('G_10_08_35 .npy',G_10_08_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_10_08_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_10_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_10_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_10_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_10_08_35  = nx.minimum_spanning_tree(G_10_08_35 )\n",
    "np.save('mst_10_08_35 .npy', mst_10_08_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_10_08_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_10_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_10_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_10_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=10, min_dist=0.8\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_10_08_35  = np.std(distance_matrices_10_08_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_10_08_35 .npy\", distance_matrix_std_10_08_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (10_08_35):\\n\", distance_matrix_std_10_08_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_10_08_35  = distance_matrix_std_10_08_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_10_08_35  = z_score * sem_matrix_10_08_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_10_08_35  = mean_distance_matrix_10_08_35  - margin_of_error_matrix_10_08_35 \n",
    "upper_limit_intconf_matrix_10_08_35  = mean_distance_matrix_10_08_35  + margin_of_error_matrix_10_08_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_10_08_35  = np.maximum(lower_limit_intconf_matrix_10_08_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_10_08_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_10_08_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_10_08_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_10_08_35 .npy', lower_limit_intconf_matrix_10_08_35 )\n",
    "np.save('upper_limit_intconf_matrix_10_08_35 .npy', upper_limit_intconf_matrix_10_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_10_08_35  = normalize_matrix(lower_limit_intconf_matrix_10_08_35 )\n",
    "norm_upper_limit_intconf_matrix_10_08_35  = normalize_matrix(upper_limit_intconf_matrix_10_08_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_10_08_35.npy', norm_lower_limit_intconf_matrix_10_08_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_10_08_35.npy', norm_upper_limit_intconf_matrix_10_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_10_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=10, min_dist=0.8\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=10, min_dist=0.8\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_10_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=10, min_dist=0.8\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_10_08_35 , \"UMAP MST - Mean Distances - n_neighbors=10, min_dist=0.8\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_10_08_35 , \"UMAP MST - Lower Limit - n_neighbors=10, min_dist=0.8\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_10_08_35 , \"UMAP MST - Upper Limit - n_neighbors=10, min_dist=0.8\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=20, min_dist=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_20_08_35= np.load('umap_projections_20_08_35.npy')\n",
    "mean_umap_projection_20_08_35= np.load('mean_projection_20_08_35.npy')\n",
    "std_projection_umap_20_08_35= np.load('std_projection_20_08_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 20\n",
    "min_dist = 0.8\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_20_08_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_20_08_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_20_08_35 = np.array(umap_projections_20_08_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_20_08_35 = np.mean(umap_projections_20_08_35, axis=0)\n",
    "std_projection_20_08_35 = np.std(umap_projections_20_08_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_20_08_35.npy', umap_projections_20_08_35)\n",
    "np.save('mean_projection_20_08_35.npy', mean_projection_20_08_35)\n",
    "np.save('std_projection_20_08_35.npy', std_projection_20_08_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_20_08_35'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_20_08_35  = np.sqrt(np.sum((umap_projections_20_08_35 - mean_umap_projection_20_08_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_20_08_35 , axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_20_08_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_20_08_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_20_08_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_20_08_35  = np.array(distance_matrices_20_08_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_20_08_35  = np.mean(distance_matrices_20_08_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_20_08_35  = (mean_distance_matrix_20_08_35  - np.min(mean_distance_matrix_20_08_35 )) / (np.max(mean_distance_matrix_20_08_35 ) - np.min(mean_distance_matrix_20_08_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_20_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=100, min_dists=0.0125)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_20_08_35 .npy', distance_matrices_20_08_35)\n",
    "np.save('mean_distance_matrix_neighbors_20_08_35 .npy', mean_distance_matrix_20_08_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_20_08_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_20_08_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_20_08_35 ,3))\n",
    "np.save('G_20_08_35 .npy',G_20_08_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_20_08_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_20_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_20_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_20_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_20_08_35  = nx.minimum_spanning_tree(G_20_08_35 )\n",
    "np.save('mst_20_08_35 .npy', mst_20_08_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_20_08_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_20_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_20_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_20_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=20, min_dist=0.8\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_20_08_35  = np.std(distance_matrices_20_08_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_20_08_35 .npy\", distance_matrix_std_20_08_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (20_08_35):\\n\", distance_matrix_std_20_08_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_20_08_35  = distance_matrix_std_20_08_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_20_08_35  = z_score * sem_matrix_20_08_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_20_08_35  = mean_distance_matrix_20_08_35  - margin_of_error_matrix_20_08_35 \n",
    "upper_limit_intconf_matrix_20_08_35  = mean_distance_matrix_20_08_35  + margin_of_error_matrix_20_08_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_20_08_35  = np.maximum(lower_limit_intconf_matrix_20_08_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_20_08_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_20_08_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_20_08_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_20_08_35 .npy', lower_limit_intconf_matrix_20_08_35 )\n",
    "np.save('upper_limit_intconf_matrix_20_08_35 .npy', upper_limit_intconf_matrix_20_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_20_08_35  = normalize_matrix(lower_limit_intconf_matrix_20_08_35 )\n",
    "norm_upper_limit_intconf_matrix_20_08_35  = normalize_matrix(upper_limit_intconf_matrix_20_08_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_20_08_35.npy', norm_lower_limit_intconf_matrix_20_08_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_20_08_35.npy', norm_upper_limit_intconf_matrix_20_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_20_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=20, min_dist=0.8\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_20_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=20, min_dist=0.8\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_20_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=20, min_dist=0.8\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_20_08_35 , \"UMAP MST - Mean Distances - n_neighbors=20, min_dist=0.8\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_20_08_35 , \"UMAP MST - Lower Limit - n_neighbors=20, min_dist=0.8\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_20_08_35 , \"UMAP MST - Upper Limit - n_neighbors=20, min_dist=0.8\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=30, min_dist=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_30_08_35= np.load('umap_projections_30_08_35.npy')\n",
    "mean_umap_projection_30_08_35= np.load('mean_projection_30_08_35.npy')\n",
    "std_projection_umap_30_08_35= np.load('std_projection_30_08_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 30\n",
    "min_dist = 0.8\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_30_08_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_30_08_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_30_08_35 = np.array(umap_projections_30_08_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_30_08_35 = np.mean(umap_projections_30_08_35, axis=0)\n",
    "std_projection_30_08_35 = np.std(umap_projections_30_08_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_30_08_35.npy', umap_projections_30_08_35)\n",
    "np.save('mean_projection_30_08_35.npy', mean_projection_30_08_35)\n",
    "np.save('std_projection_30_08_35.npy', std_projection_30_08_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_30_08_35'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_30_08_35  = np.sqrt(np.sum((umap_projections_30_08_35 - mean_umap_projection_30_08_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_30_08_35 , axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_30_08_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_30_08_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_30_08_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_30_08_35  = np.array(distance_matrices_30_08_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_30_08_35  = np.mean(distance_matrices_30_08_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_30_08_35  = (mean_distance_matrix_30_08_35  - np.min(mean_distance_matrix_30_08_35 )) / (np.max(mean_distance_matrix_30_08_35 ) - np.min(mean_distance_matrix_30_08_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_30_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=30, min_dists=0.8)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_30_08_35 .npy', distance_matrices_30_08_35)\n",
    "np.save('mean_distance_matrix_neighbors_30_08_35 .npy', mean_distance_matrix_30_08_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_30_08_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_30_08_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_30_08_35 ,3))\n",
    "np.save('G_30_08_35 .npy',G_30_08_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_30_08_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_30_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_30_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_30_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_30_08_35  = nx.minimum_spanning_tree(G_30_08_35 )\n",
    "np.save('mst_30_08_35 .npy', mst_30_08_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_30_08_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_30_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_30_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_30_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=30, min_dist=0.8\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_30_08_35  = np.std(distance_matrices_30_08_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_30_08_35 .npy\", distance_matrix_std_30_08_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (30_08_35):\\n\", distance_matrix_std_30_08_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_30_08_35  = distance_matrix_std_30_08_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_30_08_35  = z_score * sem_matrix_30_08_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_30_08_35  = mean_distance_matrix_30_08_35  - margin_of_error_matrix_30_08_35 \n",
    "upper_limit_intconf_matrix_30_08_35  = mean_distance_matrix_30_08_35  + margin_of_error_matrix_30_08_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_30_08_35  = np.maximum(lower_limit_intconf_matrix_30_08_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_30_08_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_30_08_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_30_08_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_30_08_35 .npy', lower_limit_intconf_matrix_30_08_35 )\n",
    "np.save('upper_limit_intconf_matrix_30_08_35 .npy', upper_limit_intconf_matrix_30_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_30_08_35  = normalize_matrix(lower_limit_intconf_matrix_30_08_35 )\n",
    "norm_upper_limit_intconf_matrix_30_08_35  = normalize_matrix(upper_limit_intconf_matrix_30_08_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_30_08_35.npy', norm_lower_limit_intconf_matrix_30_08_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_30_08_35.npy', norm_upper_limit_intconf_matrix_30_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_30_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=30, min_dist=0.8\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_30_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=30, min_dist=0.8\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_30_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=30, min_dist=0.8\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_30_08_35 , \"UMAP MST - Mean Distances - n_neighbors=30, min_dist=0.8\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_30_08_35 , \"UMAP MST - Lower Limit - n_neighbors=30, min_dist=0.8\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_30_08_35 , \"UMAP MST - Upper Limit - n_neighbors=30, min_dist=0.8\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=50, min_dist=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_50_08_35= np.load('umap_projections_50_08_35.npy')\n",
    "mean_umap_projection_50_08_35= np.load('mean_projection_50_08_35.npy')\n",
    "std_projection_umap_50_08_35= np.load('std_projection_50_08_35.npy')\n",
    "distance_matrices_50_08_35= np.load('distance_matrices_neighbors_50_08_35 .npy')\n",
    "mean_distance_matrix_50_08_35= np.load('mean_distance_matrix_neighbors_50_08_35 .npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 50\n",
    "min_dist = 0.8\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_50_08_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_50_08_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_50_08_35 = np.array(umap_projections_50_08_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_50_08_35 = np.mean(umap_projections_50_08_35, axis=0)\n",
    "std_projection_50_08_35 = np.std(umap_projections_50_08_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_50_08_35.npy', umap_projections_50_08_35)\n",
    "np.save('mean_projection_50_08_35.npy', mean_projection_50_08_35)\n",
    "np.save('std_projection_50_08_35.npy', std_projection_50_08_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_50_08_35'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_50_08_35  = np.sqrt(np.sum((umap_projections_50_08_35 - mean_umap_projection_50_08_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_50_08_35 , axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_50_08_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_50_08_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_50_08_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_50_08_35  = np.array(distance_matrices_50_08_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_50_08_35  = np.mean(distance_matrices_50_08_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_50_08_35  = (mean_distance_matrix_50_08_35  - np.min(mean_distance_matrix_50_08_35 )) / (np.max(mean_distance_matrix_50_08_35 ) - np.min(mean_distance_matrix_50_08_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_50_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=50, min_dists=0.8)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_50_08_35 .npy', distance_matrices_50_08_35)\n",
    "np.save('mean_distance_matrix_neighbors_50_08_35 .npy', mean_distance_matrix_50_08_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_50_08_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_50_08_35  = (mean_distance_matrix_50_08_35  - np.min(mean_distance_matrix_50_08_35 )) / (np.max(mean_distance_matrix_50_08_35 ) - np.min(mean_distance_matrix_50_08_35 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_50_08_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_50_08_35 ,3))\n",
    "np.save('G_50_08_35 .npy',G_50_08_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_50_08_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_50_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_50_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_50_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total weight of the MST\n",
    "total_weight_50_08_35 = sum(nx.get_edge_attributes(mst_50_08_35, 'weight').values())\n",
    "\n",
    "# Print the total weight\n",
    "print(f\"Total weight of the MST: {total_weight_50_08_35}\")\n",
    "\n",
    "# Compute the minimum spanning tree of the graph\n",
    "mst_50_08_35  = nx.minimum_spanning_tree(G_50_08_35 )\n",
    "np.save('mst_50_08_35 .npy', mst_50_08_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_50_08_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_50_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_50_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_50_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=50, min_dist=0.8\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_50_08_35  = np.std(distance_matrices_50_08_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_50_08_35 .npy\", distance_matrix_std_50_08_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (50_08_35):\\n\", distance_matrix_std_50_08_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_50_08_35  = distance_matrix_std_50_08_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_50_08_35  = z_score * sem_matrix_50_08_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_50_08_35  = mean_distance_matrix_50_08_35  - margin_of_error_matrix_50_08_35 \n",
    "upper_limit_intconf_matrix_50_08_35  = mean_distance_matrix_50_08_35  + margin_of_error_matrix_50_08_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_50_08_35  = np.maximum(lower_limit_intconf_matrix_50_08_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_50_08_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_50_08_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_50_08_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_50_08_35 .npy', lower_limit_intconf_matrix_50_08_35 )\n",
    "np.save('upper_limit_intconf_matrix_50_08_35 .npy', upper_limit_intconf_matrix_50_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_50_08_35  = normalize_matrix(lower_limit_intconf_matrix_50_08_35 )\n",
    "norm_upper_limit_intconf_matrix_50_08_35  = normalize_matrix(upper_limit_intconf_matrix_50_08_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_50_08_35.npy', norm_lower_limit_intconf_matrix_50_08_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_50_08_35.npy', norm_upper_limit_intconf_matrix_50_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_50_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=50, min_dist=0.8\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_50_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=50, min_dist=0.8\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_50_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=50, min_dist=0.8\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_lower_limit_intconf_matrix_50_08_35= np.load('norm_lower_limit_intconf_matrix_50_08_35.npy')\n",
    "norm_upper_limit_intconf_matrix_50_08_35= np.load('norm_upper_limit_intconf_matrix_50_08_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_50_08_35 , \"UMAP MST - Mean Distances - n_neighbors=50, min_dist=0.8\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_50_08_35 , \"UMAP MST - Lower Limit - n_neighbors=50, min_dist=0.8\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_50_08_35 , \"UMAP MST - Upper Limit - n_neighbors=50, min_dist=0.8\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n=100, min_dist=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_100_08_35= np.load('umap_projections_100_08_35.npy')\n",
    "mean_umap_projection_100_08_35= np.load('mean_projection_100_08_35.npy')\n",
    "std_projection_umap_100_08_35= np.load('std_projection_100_08_35.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 100\n",
    "min_dist = 0.8\n",
    "n_components = 2\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run\n",
    "umap_projections_100_08_35 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components,random_state=None)  # Allow randomness\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    projection = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Store the projection\n",
    "    umap_projections_100_08_35.append(projection)\n",
    "\n",
    "# Convert the list of projections to a numpy array\n",
    "umap_projections_100_08_35 = np.array(umap_projections_100_08_35)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs\n",
    "mean_projection_100_08_35 = np.mean(umap_projections_100_08_35, axis=0)\n",
    "std_projection_100_08_35 = np.std(umap_projections_100_08_35, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation\n",
    "np.save('umap_projections_100_08_35.npy', umap_projections_100_08_35)\n",
    "np.save('mean_projection_100_08_35.npy', mean_projection_100_08_35)\n",
    "np.save('std_projection_100_08_35.npy', std_projection_100_08_35)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_100_08_35'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE THIS ###\n",
    "\n",
    "# Instead of applying thresholds on a per-point basis.\n",
    "# Calculate the average distance for all points in a run and compare it against an aggregated threshold (e.g., mean or percentile of all average distances across runs).\n",
    "\n",
    "# Calculate distances from each point to the mean projection\n",
    "distances_to_mean_100_08_35  = np.sqrt(np.sum((umap_projections_100_08_35 - mean_umap_projection_100_08_35[None, :, :])**2, axis=2))  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Calculate average distance per run\n",
    "average_distances_per_run = np.mean(distances_to_mean_100_08_35 , axis=1)  # Shape: (n_runs,)\n",
    "\n",
    "# Define a threshold based on the 90th percentile of the average distances\n",
    "average_distance_threshold = np.percentile(average_distances_per_run, 90)\n",
    "\n",
    "# Identify runs that pass the threshold\n",
    "valid_runs = [run for run, avg_dist in enumerate(average_distances_per_run)\n",
    "              if avg_dist <= average_distance_threshold]\n",
    "\n",
    "print(f\"Valid runs: {valid_runs}\")\n",
    "print(f\"Number of valid runs: {len(valid_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cluster centroids (center of cluster i for each valid run)\n",
    "cluster_centroids_per_run = []\n",
    "\n",
    "# Iterate over valid runs to calculate centroids for each cluster\n",
    "for run in valid_runs:\n",
    "    # Extract the UMAP projections for this run\n",
    "    projections = umap_projections_100_08_35 [run]\n",
    "\n",
    "    # Calculate centroids for each cluster (digits 0-9)\n",
    "    centroids = []\n",
    "    for cluster_label in range(10):  # Assuming 10 clusters (digits 0-9)\n",
    "        cluster_points = projections[y_train == cluster_label]  # Points in this cluster\n",
    "        centroid = np.mean(cluster_points, axis=0)  # Calculate the centroid\n",
    "        centroids.append(centroid)\n",
    "    \n",
    "    cluster_centroids_per_run.append(np.array(centroids))  # Store centroids for this run\n",
    "\n",
    "# Calculate pairwise distances between centroids for each run\n",
    "distance_matrices_100_08_35  = []\n",
    "for centroids in cluster_centroids_per_run:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix = cdist(centroids, centroids, metric='euclidean')  # Shape: (10, 10)\n",
    "    distance_matrices_100_08_35 .append(distance_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a NumPy array\n",
    "distance_matrices_100_08_35  = np.array(distance_matrices_100_08_35 )  # Shape: (n_valid_runs, 10, 10)\n",
    "\n",
    "# Calculate the mean distance matrix across all valid runs\n",
    "mean_distance_matrix_100_08_35  = np.mean(distance_matrices_100_08_35 , axis=0)  # Shape: (10, 10)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_100_08_35  = (mean_distance_matrix_100_08_35  - np.min(mean_distance_matrix_100_08_35 )) / (np.max(mean_distance_matrix_100_08_35 ) - np.min(mean_distance_matrix_100_08_35 ))\n",
    "\n",
    "# Plot the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=100, min_dists=0.8)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_100_08_35 .npy', distance_matrices_100_08_35)\n",
    "np.save('mean_distance_matrix_neighbors_100_08_35 .npy', mean_distance_matrix_100_08_35)\n",
    "\n",
    "# Output the mean distance matrix\n",
    "print(f\"Mean distance matrix across all valid runs:\\n{mean_distance_matrix_100_08_35 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_100_08_35  = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_100_08_35 ,3))\n",
    "np.save('G_100_08_35 .npy',G_100_08_35 )\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_100_08_35 , seed=42)  # positions for all nodes\n",
    "nx.draw(G_100_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_100_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(G_100_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_100_08_35  = nx.minimum_spanning_tree(G_100_08_35 )\n",
    "np.save('mst_100_08_35 .npy', mst_100_08_35 )\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_100_08_35 , seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_100_08_35 , pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_100_08_35 , 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_100_08_35 , pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST UMAP - n_neighbors=100, min_dist=0.8\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation for each pair of clusters across all runs\n",
    "distance_matrix_std_100_08_35  = np.std(distance_matrices_100_08_35 , axis=0)  # Shape: (n_clusters, n_clusters)\n",
    "\n",
    "# Step 2: Save the standard deviation matrix for future use\n",
    "np.save(\"distance_matrix_std_100_08_35 .npy\", distance_matrix_std_100_08_35 )\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard Deviation Distance Matrix (100_08_35):\\n\", distance_matrix_std_100_08_35 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "confidence_level = 0.95\n",
    "z_score = norm.ppf((1 + confidence_level) / 2)  # Critical value for the normal distribution\n",
    "n_runs = 35  # Number of runs\n",
    "\n",
    "# Step 1: Calculate the Standard Error of the Mean (SEM)\n",
    "sem_matrix_100_08_35  = distance_matrix_std_100_08_35  / np.sqrt(n_runs)\n",
    "\n",
    "# Step 2: Calculate the margin of error\n",
    "margin_of_error_matrix_100_08_35  = z_score * sem_matrix_100_08_35 \n",
    "\n",
    "# Step 3: Compute the lower and upper confidence interval matrices\n",
    "lower_limit_intconf_matrix_100_08_35  = mean_distance_matrix_100_08_35  - margin_of_error_matrix_100_08_35 \n",
    "upper_limit_intconf_matrix_100_08_35  = mean_distance_matrix_100_08_35  + margin_of_error_matrix_100_08_35 \n",
    "\n",
    "# Ensure no negative values in the lower limit matrix (optional)\n",
    "lower_limit_intconf_matrix_100_08_35  = np.maximum(lower_limit_intconf_matrix_100_08_35 , 0)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean Distance Matrix:\\n\", mean_distance_matrix_100_08_35 )\n",
    "print(\"\\nLower Limit Matrix:\\n\", lower_limit_intconf_matrix_100_08_35 )\n",
    "print(\"\\nUpper Limit Matrix:\\n\", upper_limit_intconf_matrix_100_08_35 )\n",
    "\n",
    "# Save the matrices for future use\n",
    "np.save('lower_limit_intconf_matrix_100_08_35 .npy', lower_limit_intconf_matrix_100_08_35 )\n",
    "np.save('upper_limit_intconf_matrix_100_08_35 .npy', upper_limit_intconf_matrix_100_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "\n",
    "norm_lower_limit_intconf_matrix_100_08_35  = normalize_matrix(lower_limit_intconf_matrix_100_08_35 )\n",
    "norm_upper_limit_intconf_matrix_100_08_35  = normalize_matrix(upper_limit_intconf_matrix_100_08_35 )\n",
    "np.save('norm_lower_limit_intconf_matrix_100_08_35.npy', norm_lower_limit_intconf_matrix_100_08_35 )\n",
    "np.save('norm_upper_limit_intconf_matrix_100_08_35.npy', norm_upper_limit_intconf_matrix_100_08_35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 9))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(norm_lower_limit_intconf_matrix_100_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Normalized Lower bound Dist. Matrix - n_neighbors=100, min_dist=0.8\")\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Normalized Mean Dist. Matrix - n_neighbors=100, min_dist=0.8\")\n",
    "axes[1].set_xlabel(\"Cluster\")\n",
    "axes[1].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(norm_upper_limit_intconf_matrix_100_08_35 , annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[2])\n",
    "axes[2].set_title(\"Normalized Upper bound Dist. Matrix - n_neighbors=100, min_dist=0.8\")\n",
    "axes[2].set_xlabel(\"Cluster\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot MSTs for mean, lower, and upper matrices\n",
    "plot_mst(normalized_mean_distance_matrix_100_08_35 , \"UMAP MST - Mean Distances - n_neighbors=100, min_dist=0.8\", axes[1], color='red')\n",
    "plot_mst(norm_lower_limit_intconf_matrix_100_08_35 , \"UMAP MST - Lower Limit - n_neighbors=100, min_dist=0.8\", axes[0], color='blue')\n",
    "plot_mst(norm_upper_limit_intconf_matrix_100_08_35 , \"UMAP MST - Upper Limit - n_neighbors=100, min_dist=0.8\", axes[2], color='green')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0,0])\n",
    "axes[0,0].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=5)\")\n",
    "axes[0,0].set_xlabel(\"Cluster\")\n",
    "axes[0,0].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0,1])\n",
    "axes[0,1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=10)\")\n",
    "axes[0,1].set_xlabel(\"Cluster\")\n",
    "axes[0,1].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_20_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0,2])\n",
    "axes[0,2].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=20)\")\n",
    "axes[0,2].set_xlabel(\"Cluster\")\n",
    "axes[0,2].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_30_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1,0])\n",
    "axes[1,0].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=30)\")\n",
    "axes[1,0].set_xlabel(\"Cluster\")\n",
    "axes[1,0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_50_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1,1])\n",
    "axes[1,1].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=50)\")\n",
    "axes[1,1].set_xlabel(\"Cluster\")\n",
    "axes[1,1].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_mean_distance_matrix_100_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1,2])\n",
    "axes[1,2].set_title(\"Normalized Mean Dist. Matrix (k=10, n_neighbors=100)\")\n",
    "axes[1,2].set_xlabel(\"Cluster\")\n",
    "axes[1,2].set_ylabel(\"\")\n",
    "\n",
    "# Adjusted layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Smaller n_neighbors (5, 10):**\n",
    "\n",
    "- Clusters exhibit closer relationships, with smaller normalized distances between certain cluster pairs (e.g., clusters 2 and 3, 2 and 5).\n",
    "- Cluster overlap is visible in the UMAP embeddings, as smaller n_neighbors focus on local structures, resulting in tighter but more interconnected clusters.\n",
    "- Variations in distances between cluster pairs suggest some **instability** in global separation.\n",
    "\n",
    "**Larger n_neighbors ( 50, 100):**\n",
    "\n",
    "- Clusters become more distinct, with higher mean distances across most cluster pairs.\n",
    "- The normalized values tend to converge across different pairs, indicating that clusters are uniformly separated.\n",
    "- There is a loss of local relationships, but the global structure is more stable.\n",
    "\n",
    "**Insights:**\n",
    "- Smaller n_neighbors: Ideal for capturing fine-grained local relationships, but may lead to cluster overlap or ambiguity.\n",
    "- Larger n_neighbors: Better for creating distinct, globally separated clusters, though at the expense of local details.\n",
    "\n",
    "The *lack of a consistent* trend for certain cluster pairs (like Cluster 2 and Cluster 3) as n_neighbors increasesspecifically the fact that normalized distances at n=50 and n=100 differcan be explained by the *inherent trade-offs in UMAP's parameterization and how it balances local vs. global structure at different scales.*\n",
    "\n",
    "**- UMAP is nonlinear:**\n",
    "\n",
    "Algorithm uses local distances between neighbors to construct the high-dimensional graph, which it then embeds into a lower-dimensional space. Nonlinearities in how UMAP maps points into the embedding space can create irregularities in the behavior of normalized mean distances.\n",
    "\n",
    "**- Global vs. Local Trade-offs:**\n",
    "\n",
    "When n_neighbors is small (e.g., 5 or 10), the embedding is heavily focused on local connections. For cluster pairs that are naturally distinct (like Cluster 2 and Cluster 3), the local focus ensures strong separation.\n",
    "As n_neighbors increases, UMAP prioritizes global consistency, which can reduce the relative separation between certain cluster pairs. However, for n_neighbors=50 vs. n_neighbors=100, small shifts in the graph structure (how clusters are connected globally) can lead to inconsistent distances.\n",
    "\n",
    "**- Stochasticity in UMAP:**\n",
    "\n",
    "Even with fixed parameters, UMAP embeddings can exhibit small variations due to its stochastic nature. These variations may amplify when focusing on normalized metrics like distances between clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with three subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot each normalized matrix as a heatmap\n",
    "sns.heatmap(normalized_distance_matrix_std_5_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0,0])\n",
    "axes[0,0].set_title(\"Normalized Std. Dev. Dist. Matrix (k=10, n_neighbors=5)\")\n",
    "axes[0,0].set_xlabel(\"Cluster\")\n",
    "axes[0,0].set_ylabel(\"Cluster\")\n",
    "\n",
    "sns.heatmap(normalized_distance_matrix_std_10_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0,1])\n",
    "axes[0,1].set_title(\"Normalized Std. Dev. Dist. Matrix (k=10, n_neighbors=10)\")\n",
    "axes[0,1].set_xlabel(\"Cluster\")\n",
    "axes[0,1].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_distance_matrix_std_20_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[0,2])\n",
    "axes[0,2].set_title(\"Normalized Std. Dev. Dist. Matrix (k=10, n_neighbors=20)\")\n",
    "axes[0,2].set_xlabel(\"Cluster\")\n",
    "axes[0,2].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_distance_matrix_std_30_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1,0])\n",
    "axes[1,0].set_title(\"Normalized Std. Dev. Dist. Matrix (k=10, n_neighbors=30)\")\n",
    "axes[1,0].set_xlabel(\"Cluster\")\n",
    "axes[1,0].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_distance_matrix_std_50_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1,1])\n",
    "axes[1,1].set_title(\"Normalized Std. Dev. Dist. Matrix (k=10, n_neighbors=50)\")\n",
    "axes[1,1].set_xlabel(\"Cluster\")\n",
    "axes[1,1].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(normalized_distance_matrix_std_100_35, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=axes[1,2])\n",
    "axes[1,2].set_title(\"Normalized Std. Dev. Dist. Matrix (k=10, n_neighbors=100)\")\n",
    "axes[1,2].set_xlabel(\"Cluster\")\n",
    "axes[1,2].set_ylabel(\"\")\n",
    "\n",
    "# Adjusted layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Smaller n_neighbors (5, 10):**\n",
    "\n",
    "- Higher normalized std. deviation distances for several cluster pairs.\n",
    "- Indicates more variability within clusters and greater sensitivity to noise or small changes in the data.\n",
    "\n",
    "\n",
    "**Larger n_neighbors (50, 100):**\n",
    "\n",
    "- Lower std. deviation distances across most cluster pairs, reflecting greater stability and reduced variability.\n",
    "Indicates that larger n_neighbors values smooth out small-scale variations and lead to more consistent cluster shapes across runs.\n",
    "\n",
    "**Insights:**\n",
    "\n",
    "- Smaller n_neighbors: Capture more local variability, which may be useful for detecting finer structural differences but might introduce instability.\n",
    "- Larger n_neighbors: Produce more stable clusters with lower variability, ideal for applications requiring consistent global structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a dictionary with mean distance matrices and their corresponding n_neighbors\n",
    "# mean_distance_matrices = {\n",
    "#     \"n_neighbors=5\": mean_distance_matrix_5_35,\n",
    "#     \"n_neighbors=10\": mean_distance_matrix_10_35,\n",
    "#     \"n_neighbors=20\": mean_distance_matrix_20_35,\n",
    "#     \"n_neighbors=30\": mean_distance_matrix_50_35,\n",
    "#     \"n_neighbors=50\": mean_distance_matrix_50_35,\n",
    "#     \"n_neighbors=100\": mean_distance_matrix_100_35,\n",
    "# }\n",
    "\n",
    "# # Normalize each matrix and store them in a new dictionary\n",
    "# normalized_matrices = {\n",
    "#     key: (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n",
    "#     for key, matrix in mean_distance_matrices.items()\n",
    "# }\n",
    "\n",
    "# # Set up the figure with 2 rows and 3 columns\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# # Flatten the axes array for easier iteration\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Plot each normalized matrix\n",
    "# for ax, (key, matrix) in zip(axes, normalized_matrices.items()):\n",
    "#     sns.heatmap(matrix, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5, ax=ax)\n",
    "#     ax.set_title(f\"Normalized Mean Dist. Matrix ({key})\")\n",
    "#     ax.set_xlabel(\"Cluster\")\n",
    "#     ax.set_ylabel(\"Cluster\" if \"n_neighbors=5\" in key else \"\")\n",
    "\n",
    "# # Adjust layout\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_neighbors = 5**\n",
    "- High Local Connectivity: n_neighbors small value, more focus on local structure. Clusters close in the original high-dimensional space remain close.\n",
    "- Bbroader range of values (from around 0.00 to nearly 1.00), indicating that some clusters are very close, while others are more distant. UMAP capture detailed local variations, creating a more distinct separation between clusters with less overlap.\n",
    "\n",
    "**n_neighbors = 10**\n",
    "- Mix of both local and global relationships. Moderate distance range, with slightly less contrast compared to n_neighbors=5.\n",
    "- More similarity in distances across clusters, some of the values in the heatmap become more uniform.\n",
    "\n",
    "**n_neighbors = 20**\n",
    "- Emphasis on Global Structure: With n_neighbors set to 20, UMAP emphasizes global relationships between clusters. Fewer very close or very distant clusters.\n",
    "- More Homogeneous Distances: less variation, indicating that UMAP is pulling clusters closer together in the reduced space. This setting can blur local distinctions and give a more general structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "# List of file names corresponding to the saved plots\n",
    "file_names = [\n",
    "    'neighbor_counts_plot_n_5_35.png',\n",
    "    'neighbor_counts_plot_n_10_35.png',\n",
    "    'neighbor_counts_plot_n_20_35.png',\n",
    "    'neighbor_counts_plot_n_30_35.png',\n",
    "    'neighbor_counts_plot_n_50_35.png',\n",
    "    'neighbor_counts_plot_n_100_35.png'\n",
    "]\n",
    "\n",
    "# Number of rows and columns for the grid layout\n",
    "n_rows = 2\n",
    "n_cols = 3\n",
    "\n",
    "# Create a figure to display the plots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 10))\n",
    "fig.suptitle('Neighbor Counts Across Runs for Different n_neighbors Values', fontsize=16)\n",
    "\n",
    "# Iterate through the file names and axes to load and display each plot\n",
    "for ax, file_name in zip(axes.flat, file_names):\n",
    "    # Load the image\n",
    "    img = mpimg.imread(file_name)\n",
    "    # Display the image on the current axis\n",
    "    ax.imshow(img)\n",
    "    # Set the title to indicate the n_neighbors value\n",
    "    ax.set_title(file_name.split('_n_')[1].split('_35.png')[0], fontsize=14)\n",
    "    # Remove axis ticks\n",
    "    ax.axis('off')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# Show the grid of plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights from Neighbor Count Graphs**\n",
    "\n",
    "**General Stability Across Runs:**\n",
    "\n",
    "- For all n_neighbors values, the mean neighbor count remains relatively stable across the 35 runs, indicating that UMAP preserves the overall neighborhood structure across multiple projections.\n",
    "- The max neighbor count, however, shows more variability for lower n_neighbors values (e.g., n=5 and n=10), which reduces significantly as n_neighbors increases (n=50 and n=100).\n",
    "\n",
    "**Effect of n_neighbors:**\n",
    "\n",
    "- As n_neighbors increases, both the mean and max neighbor counts increase. This is expected because a higher n_neighbors value results in a broader neighborhood being captured in the UMAP embedding.\n",
    "- For smaller n_neighbors (e.g., n=5):\n",
    "Clusters are tighter, and the max neighbor count fluctuates significantly, reflecting that some clusters are smaller or more isolated.\n",
    "- For larger n_neighbors (e.g., n=100):\n",
    "- The neighborhood structure stabilizes with high max neighbor counts, suggesting that clusters are more broadly connected in the embedding.\n",
    "\n",
    "**Trend Lines:**\n",
    "\n",
    "The trend lines for both mean and max neighbor counts show slight increases for all n_neighbors, but the slope diminishes as n_neighbors grows, reflecting diminishing returns in terms of increasing neighbor counts at higher values.\n",
    "\n",
    "**Insights from Dynamic Radius and Minimum Distance**\n",
    "\n",
    "**Dynamic Radius Behavior:**\n",
    "\n",
    "- The dynamic radius decreases as n_neighbors increases. This happens because higher n_neighbors values result in clusters being more spread out, reducing the necessity for smaller, tightly-defined cluster boundaries.\n",
    "- At lower n_neighbors values, the clusters are compact, requiring smaller dynamic radii to define meaningful inter-cluster relationships.\n",
    "\n",
    "**Minimum Distance Across Runs:**\n",
    "\n",
    "- Minimum distances (used to compute the dynamic radius) are larger for lower n_neighbors values (e.g., n=5 and n=10) because clusters are tighter and more isolated, leading to clearer separations.\n",
    "- As n_neighbors increases, the minimum distances shrink slightly, indicating that clusters start to overlap more, making them harder to distinguish.\n",
    "\n",
    "**Clusters Contributing to Minimum Distance:**\n",
    "\n",
    "- The pairs of clusters contributing to the minimum distance vary across n_neighbors. This suggests that the interaction between clusters is highly sensitive to the neighborhood size. For example:\n",
    "\n",
    "n=5: Clusters (3, 8) have the smallest separation.\n",
    "\n",
    "n=100: Clusters (4, 6) show the smallest distance, possibly due to broader neighborhood connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define separate positions for each MST for better clarity\n",
    "pos_5 = nx.spring_layout(mst_5_35, seed=42)\n",
    "pos_10 = nx.spring_layout(mst_10_35, seed=42)\n",
    "pos_20 = nx.spring_layout(mst_20_35, seed=42)\n",
    "pos_50 = nx.spring_layout(mst_50_35, seed=42)\n",
    "pos_50 = nx.spring_layout(mst_50_35, seed=42)\n",
    "pos_100 = nx.spring_layout(mst_100_35, seed=42)\n",
    "\n",
    "# Set up the figure with three subplots side by side\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot MST with n_neighbors=5\n",
    "nx.draw(mst_5_35, pos_5, with_labels=True, node_color='lightblue', edge_color='red', node_size=600, font_size=9, width=2, ax=axes[0,0])\n",
    "edge_labels_5 = nx.get_edge_attributes(mst_5_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_5_35, pos_5, edge_labels=edge_labels_5, font_size=7, label_pos=0.4, ax=axes[0,0])\n",
    "axes[0,0].set_title(\"MST - n_neighbors=5\")\n",
    "\n",
    "# Plot MST with n_neighbors=10\n",
    "nx.draw(mst_10_35, pos_10, with_labels=True, node_color='lightblue', edge_color='red', node_size=600, font_size=9, width=2, ax=axes[0,1])\n",
    "edge_labels_10 = nx.get_edge_attributes(mst_10_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_10_35, pos_10, edge_labels=edge_labels_10, font_size=7, label_pos=0.4, ax=axes[0,1])\n",
    "axes[0,1].set_title(\"MST - n_neighbors=10\")\n",
    "\n",
    "# Plot MST with n_neighbors=20\n",
    "nx.draw(mst_20_35, pos_20, with_labels=True, node_color='lightblue', edge_color='red', node_size=600, font_size=9, width=2, ax=axes[0,2])\n",
    "edge_labels_20 = nx.get_edge_attributes(mst_20_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_20_35, pos_20, edge_labels=edge_labels_20, font_size=7, label_pos=0.4, ax=axes[0,2])\n",
    "axes[0,2].set_title(\"MST - n_neighbors=20\")\n",
    "\n",
    "# Plot MST with n_neighbors=30\n",
    "nx.draw(mst_50_35, pos_50, with_labels=True, node_color='lightblue', edge_color='red', node_size=600, font_size=9, width=2, ax=axes[1,0])\n",
    "edge_labels_50 = nx.get_edge_attributes(mst_50_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_50_35, pos_50, edge_labels=edge_labels_50, font_size=7, label_pos=0.4, ax=axes[1,0])\n",
    "axes[1,0].set_title(\"MST - n_neighbors=30\")\n",
    "\n",
    "# Plot MST with n_neighbors=50\n",
    "nx.draw(mst_50_35, pos_50, with_labels=True, node_color='lightblue', edge_color='red', node_size=600, font_size=9, width=2, ax=axes[1,1])\n",
    "edge_labels_50 = nx.get_edge_attributes(mst_50_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_50_35, pos_50, edge_labels=edge_labels_50, font_size=7, label_pos=0.4, ax=axes[1,1])\n",
    "axes[1,1].set_title(\"MST - n_neighbors=50\")\n",
    "\n",
    "# Plot MST with n_neighbors=100\n",
    "nx.draw(mst_100_35, pos_100, with_labels=True, node_color='lightblue', edge_color='red', node_size=600, font_size=9, width=2, ax=axes[1,2])\n",
    "edge_labels_100 = nx.get_edge_attributes(mst_100_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_100_35, pos_100, edge_labels=edge_labels_100, font_size=7, label_pos=0.4, ax=axes[1,2])\n",
    "axes[1,2].set_title(\"MST - n_neighbors=100\")\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster Connectivity:**\n",
    "\n",
    "**Low n_neighbors values (5, 10):**\n",
    "The MST shows sparse connections, with most clusters forming direct links to only one or two other clusters. This indicates strong local relationships but limited global integration. E.g., clusters 0, 1, and 9 have fewer connections in n_neighbors=5 compared to higher n_neighbors values.\n",
    "\n",
    "**High n_neighbors values (50, 100):**\n",
    "\n",
    "The MST becomes more integrated, with clusters forming a more interconnected network. This reflects stronger global relationships and less separation among clusters. E.g., cluster 0 acts as a hub in n_neighbors=100.\n",
    "\n",
    "**Edge Weights (Distances)**\n",
    "\n",
    "**Low n_neighbors values (5, 10):**\n",
    "\n",
    "Edge weights tend to be higher, indicating greater separation between clusters. Clusters are linked only when the distance is minimal due to the focus on local density.\n",
    "E.g., in n_neighbors=5, edges like between clusters 3 and 8 show higher weights compared to higher n_neighbors.\n",
    "\n",
    "**High n_neighbors values (50, 100):**\n",
    "\n",
    "Edge weights decrease as the global structure becomes more cohesive, reflecting closer relationships between clusters.\n",
    "E.g., weights such as between clusters 0 and 5 or 7 and 9 are smaller in n_neighbors=100.\n",
    "\n",
    "Hub Clusters:\n",
    "\n",
    "Clusters like 1 or 0 tend to act as central hubs in lower n_neighbors values. As n_neighbors increases, central roles shift slightly to clusters like 5 or 7, depending on their proximity and relationship with other clusters.\n",
    "This shift highlights how the parameter impacts the representation of central clusters, with larger neighborhoods prioritizing global consistency over local detail.\n",
    "\n",
    "Structural Changes:\n",
    "\n",
    "For low n_neighbors, the MST prioritizes local connections, resulting in a more tree-like and segmented structure.\n",
    "For higher n_neighbors,the tree grows denser, with more branches and inter-cluster connections. This reflects a trade-off: higher n_neighbors captures more global information but risks losing fine-grained local details.\n",
    "Key Insights:\n",
    "Effect of n_neighbors:\n",
    "\n",
    "Small n_neighbors values (5, 10): Emphasize local relationships, causing clusters to be more isolated in the MST. This approach might be useful for identifying finer-grained local structure.\n",
    "Large n_neighbors values (50, 100): Emphasize global structure, leading to tighter integration and highlighting broad patterns in the data.\n",
    "Application:\n",
    "\n",
    "For local structure analysis, choose small n_neighbors (e.g., 510).\n",
    "For global relationships, large n_neighbors (e.g., 50100) captures broader trends.\n",
    "Cluster Stability:\n",
    "\n",
    "Comparing the MST across different n_neighbors highlights how certain clusters maintain consistent connections (e.g., clusters 0 and 7 often connect), reflecting their stability across runs.\n",
    "\n",
    "Main Insights:\n",
    "Parameter Tuning: Adjusting n_neighbors influences the balance between local and global structures in the UMAP projection. For applications requiring fine-grained cluster separation, lower values of n_neighbors are better. For broader connectivity and higher-level structures, larger values are more suitable.\n",
    "Cluster Stability: The consistent appearance of certain connections (e.g., 7-8, 0-1) suggests that some relationships are robust to parameter changes, reinforcing their reliability in the dataset's structure.\n",
    "Hierarchical Patterns: As n_neighbors increases, the graph evolves into a more hierarchical structure, highlighting relationships across broader scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define separate positions for each MST for better clarity\n",
    "pos_5 = nx.spring_layout(mst_std_5_35, seed=42)\n",
    "pos_10 = nx.spring_layout(mst_std_10_35, seed=42)\n",
    "pos_20 = nx.spring_layout(mst_std_20_35, seed=42)\n",
    "pos_30 = nx.spring_layout(mst_std_30_35, seed=42)\n",
    "pos_50 = nx.spring_layout(mst_std_50_35, seed=42)\n",
    "pos_100 = nx.spring_layout(mst_std_100_35, seed=42)\n",
    "\n",
    "# Set up the figure with subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot MST with n_neighbors=5\n",
    "nx.draw(mst_std_5_35, pos_5, ax=axes[0, 0], with_labels=True, node_color='lightyellow',edge_color='green', node_size=500, font_size=10, width=2)\n",
    "edge_labels_5 = nx.get_edge_attributes(mst_std_5_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_5_35, pos_5, edge_labels=edge_labels_5, ax=axes[0, 0], font_size=8, label_pos=0.3)\n",
    "axes[0, 0].set_title(\"MST Std. Dev - n_neighbors=5\")\n",
    "\n",
    "# Plot MST with n_neighbors=10\n",
    "nx.draw(mst_std_10_35, pos_10, ax=axes[0, 1], with_labels=True, node_color='lightyellow',edge_color='green', node_size=500, font_size=10, width=2)\n",
    "edge_labels_10 = nx.get_edge_attributes(mst_std_10_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_10_35, pos_10, edge_labels=edge_labels_10, ax=axes[0, 1], font_size=8, label_pos=0.3)\n",
    "axes[0, 1].set_title(\"MST Std. Dev - n_neighbors=10\")\n",
    "\n",
    "# Plot MST with n_neighbors=20\n",
    "nx.draw(mst_std_20_35, pos_20, ax=axes[0, 2], with_labels=True, node_color='lightyellow',edge_color='green', node_size=500, font_size=10, width=2)\n",
    "edge_labels_20 = nx.get_edge_attributes(mst_std_20_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_20_35, pos_20, edge_labels=edge_labels_20, ax=axes[0, 2], font_size=8, label_pos=0.3)\n",
    "axes[0, 2].set_title(\"MST Std. Dev - n_neighbors=20\")\n",
    "\n",
    "# Plot MST with n_neighbors=30\n",
    "nx.draw(mst_std_30_35, pos_30, ax=axes[1, 0], with_labels=True, node_color='lightyellow',edge_color='green', node_size=500, font_size=10, width=2)\n",
    "edge_labels_30 = nx.get_edge_attributes(mst_std_30_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_30_35, pos_30, edge_labels=edge_labels_30, ax=axes[1, 0], font_size=8, label_pos=0.3)\n",
    "axes[1, 0].set_title(\"MST Std. Dev - n_neighbors=30\")\n",
    "\n",
    "# Plot MST with n_neighbors=50\n",
    "nx.draw(mst_std_50_35, pos_50, ax=axes[1, 1], with_labels=True, node_color='lightyellow',edge_color='green', node_size=500, font_size=10, width=2)\n",
    "edge_labels_50 = nx.get_edge_attributes(mst_std_50_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_50_35, pos_50, edge_labels=edge_labels_50, ax=axes[1, 1], font_size=8, label_pos=0.3)\n",
    "axes[1, 1].set_title(\"MST Std. Dev - n_neighbors=50\")\n",
    "\n",
    "# Plot MST with n_neighbors=100\n",
    "nx.draw(mst_std_100_35, pos_100, ax=axes[1, 2], with_labels=True, node_color='lightyellow',edge_color='green', node_size=500, font_size=10, width=2)\n",
    "edge_labels_100 = nx.get_edge_attributes(mst_std_100_35, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_100_35, pos_100, edge_labels=edge_labels_100, ax=axes[1, 2], font_size=8, label_pos=0.3)\n",
    "axes[1, 2].set_title(\"MST Std. Dev - n_neighbors=100\")\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MST for means shows the average distances between cluster centroids, the MST for standard deviation highlights how consistent these distances are.\n",
    "\n",
    "The MST for standard deviation reflects the variability or consistency of distances between clusters across multiple UMAP runs.\n",
    "\n",
    "Lower edge weights indicate less variability, implying consistent distances between clusters in repeated runs.\n",
    "\n",
    "Higher edge weights suggest greater variability, indicating clusters that are less stable in their relationship across runs.\n",
    "\n",
    "**Clusters that are consistently closer in the MST for means should also have low edge weights in the MST for standard deviation**, as consistent closeness implies stability.\n",
    "\n",
    "Conversely, if some clusters are close in the MST for means but show high edge weights in the MST for standard deviation, it suggests that their proximity varies across runs.\n",
    "\n",
    "*As n_neighbors increases:*\n",
    "The MST for standard deviation tends to become more balanced, as larger neighborhoods incorporate more data points, reducing variability in distances between clusters.\n",
    "\n",
    "The relationship between clusters becomes more stable due to the smoother structure of the UMAP embeddings with higher n_neighbors.\n",
    "\n",
    "**Specific Observations:**\n",
    "\n",
    "- **For smaller n_neighbors** (5 or 10), there are noticeable high-variability edges, especially between clusters that are far apart in the MST for means.\n",
    "\n",
    "- **For larger n_neighbors** (50 or 100), the edges in the MST for standard deviation tend to show lower weights overall, reflecting greater stability.\n",
    "\n",
    "- Clusters that are peripheral in the MST for means (e.g., clusters 6, 8, and 9) often have higher variability, as their positions can fluctuate more significantly between runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Comparison Insights:**\n",
    "\n",
    "*Stability vs. Proximity:*\n",
    "\n",
    "- Clusters connected by short edges in the MST for means should ideally have short edges in the MST for standard deviation. Deviations from this pattern can indicate unstable proximities.\n",
    "For example:\n",
    "If cluster pairs (3, 8) or (0, 9) have short edges in the means MST but long edges in the std deviation MST, it suggests their relationship fluctuates across runs.\n",
    "In contrast, pairs with short edges in both MSTs are robust and consistent.\n",
    "\n",
    "*Trend with Increasing n_neighbors:*\n",
    "\n",
    "- The MST for standard deviation generally shows fewer high-weight edges as n_neighbors increases. This aligns with the observation that higher n_neighbors smooth out the embedding space, leading to more stable relationships between clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of n_neighbors values\n",
    "n_neighbors_values = [5, 10, 20, 30, 50, 100]\n",
    "\n",
    "# Function to convert numpy.ndarray to NetworkX graph and compute MST\n",
    "def convert_and_save_msts():\n",
    "    for n in n_neighbors_values:\n",
    "        # Load the numpy.ndarray distance matrix for means\n",
    "        mean_matrix_path = f'normalized_mean_distance_matrix_{n}_35.npy'\n",
    "        std_matrix_path = f'normalized_distance_matrix_std_{n}_35.npy'\n",
    "        \n",
    "        # Load the distance matrices\n",
    "        mean_matrix = np.load(mean_matrix_path)\n",
    "        std_matrix = np.load(std_matrix_path)\n",
    "        \n",
    "        # Convert mean distance matrix to NetworkX graph and compute MST\n",
    "        G_means = nx.from_numpy_array(mean_matrix)\n",
    "        mst_means = nx.minimum_spanning_tree(G_means)\n",
    "        \n",
    "        # Save the MST for means as a pickle\n",
    "        with open(f'mst_means_{n}_35.pkl', 'wb') as f:\n",
    "            pickle.dump(mst_means, f)\n",
    "        \n",
    "        # Convert std deviation distance matrix to NetworkX graph and compute MST\n",
    "        G_std = nx.from_numpy_array(std_matrix)\n",
    "        mst_std = nx.minimum_spanning_tree(G_std)\n",
    "        \n",
    "        # Save the MST for std deviations as a pickle\n",
    "        with open(f'mst_std_{n}_35.pkl', 'wb') as f:\n",
    "            pickle.dump(mst_std, f)\n",
    "        \n",
    "        print(f\"MSTs for n_neighbors={n} have been converted and saved.\")\n",
    "\n",
    "# Run the conversion and save the MSTs\n",
    "convert_and_save_msts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute correlation between two MSTs\n",
    "def compute_mst_edge_correlation(mst_means, mst_std):\n",
    "    # Extract edges and weights\n",
    "    edges_means = nx.get_edge_attributes(mst_means, 'weight')\n",
    "    edges_std = nx.get_edge_attributes(mst_std, 'weight')\n",
    "    \n",
    "    # Match edges (both MSTs should have the same edges)\n",
    "    weights_means = []\n",
    "    weights_std = []\n",
    "    for edge in edges_means:\n",
    "        if edge in edges_std:\n",
    "            weights_means.append(edges_means[edge])\n",
    "            weights_std.append(edges_std[edge])\n",
    "    \n",
    "    # Check if there are enough matching edges\n",
    "    if len(weights_means) < 2 or len(weights_std) < 2:\n",
    "        print(f\"Insufficient overlapping edges: {len(weights_means)} edges.\")\n",
    "        return None, None  # Return None if insufficient data for correlation\n",
    "    \n",
    "    # Compute Pearson correlation\n",
    "    correlation, p_value = pearsonr(weights_means, weights_std)\n",
    "    return correlation, p_value\n",
    "\n",
    "# Load and compute correlation for each n_neighbors value\n",
    "n_neighbors_values = [5, 10, 20, 30, 50, 100]\n",
    "correlations = {}\n",
    "\n",
    "for n in n_neighbors_values:\n",
    "    # Load MSTs from .pkl files\n",
    "    try:\n",
    "        with open(f'mst_means_{n}_35.pkl', 'rb') as f:\n",
    "            mst_means = pickle.load(f)\n",
    "        with open(f'mst_std_{n}_35.pkl', 'rb') as f:\n",
    "            mst_std = pickle.load(f)\n",
    "        \n",
    "        # Compute correlation\n",
    "        correlation, p_value = compute_mst_edge_correlation(mst_means, mst_std)\n",
    "        correlations[n] = (correlation, p_value)\n",
    "        if correlation is not None:\n",
    "            print(f\"n_neighbors={n}: Correlation = {correlation:.3f}, p-value = {p_value:.3f}\")\n",
    "        else:\n",
    "            print(f\"n_neighbors={n}: Insufficient overlapping edges for correlation.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"n_neighbors={n}: MST files not found.\")\n",
    "\n",
    "# Optionally visualize the results for valid correlations\n",
    "n_neighbors_valid = [k for k, v in correlations.items() if v[0] is not None]\n",
    "correlation_values = [v[0] for k, v in correlations.items() if v[0] is not None]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_neighbors_valid, correlation_values, marker='o', color='blue', label='Pearson Correlation')\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "plt.title('Correlation Between MST Edge Weights (Mean vs Std Deviation)', fontsize=14)\n",
    "plt.xlabel('n_neighbors', fontsize=12)\n",
    "plt.ylabel('Correlation', fontsize=12)\n",
    "plt.xticks(n_neighbors_valid)\n",
    "plt.grid(True, linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lack of Overlapping Edges:**\n",
    "\n",
    "MSTs depend heavily on the edge weights derived from the distance matrices. As n_neighbors changes, the distances between clusters in the data manifold change, leading to different MST structures. When there are insufficient overlapping edges between the two graphs, the correlation computation cannot proceed.\n",
    "\n",
    "**Perfect Correlations:**\n",
    "\n",
    "For n_neighbors=30, there is perfect positive correlation. This suggests that the MSTs for means and std deviation share the same edges, and the weights are proportional across both graphs. This scenario might occur if the clusters stabilize in structure and the relationship between means and std deviation distances becomes consistent.\n",
    "\n",
    "**Edge Weight Variability:**\n",
    "\n",
    "For n_neighbors=5 and n_neighbors=100, the negative correlations are artifacts of limited overlapping edges. With only one edge to compare, a perfect correlation (positive or negative) is likely due to insufficient data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To open the MST graphs\n",
    "# import pickle\n",
    "# n_neighbors = 5  # Example\n",
    "# # Load the MSTs\n",
    "# with open(f'mst_means_{n_neighbors}_35.pkl', 'rb') as f:\n",
    "#     mst_means = pickle.load(f)\n",
    "\n",
    "# with open(f'mst_std_{n_neighbors}_35.pkl', 'rb') as f:\n",
    "#     mst_std = pickle.load(f)\n",
    "\n",
    "# # Now mst_means and mst_std are NetworkX graphs\n",
    "# print(mst_means.nodes, mst_means.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define clusters\n",
    "clusters = np.arange(10)  # Clusters from 0 to 9\n",
    "\n",
    "# Define colors for each n_neighbors\n",
    "colors = {5: \"orange\", 10: \"blue\", 20: \"yellow\", 30: \"grey\", 50: \"green\", 100: \"red\"}\n",
    "\n",
    "# Iterate over each cluster as the base cluster\n",
    "for base_cluster in clusters:\n",
    "    # Define the data for each n_neighbors, adjusted for the base cluster\n",
    "    data = {\n",
    "        5: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_5_35[base_cluster], base_cluster),  # Distances from base cluster\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_5_35[base_cluster], base_cluster),  # Lower bounds\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_5_35[base_cluster], base_cluster)   # Upper bounds\n",
    "        },\n",
    "        10: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_10_35[base_cluster], base_cluster),\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_10_35[base_cluster], base_cluster),\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_10_35[base_cluster], base_cluster)\n",
    "        },\n",
    "        20: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_20_35[base_cluster], base_cluster),\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_20_35[base_cluster], base_cluster),\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_20_35[base_cluster], base_cluster)\n",
    "        },\n",
    "        30: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_30_35[base_cluster], base_cluster),\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_30_35[base_cluster], base_cluster),\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_30_35[base_cluster], base_cluster)\n",
    "        },\n",
    "        50: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_50_35[base_cluster], base_cluster),\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_50_35[base_cluster], base_cluster),\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_50_35[base_cluster], base_cluster)\n",
    "        },\n",
    "        100: {\n",
    "            \"mean\": np.delete(mean_distance_matrix_100_35[base_cluster], base_cluster),\n",
    "            \"lower\": np.delete(lower_limit_intconf_matrix_100_35[base_cluster], base_cluster),\n",
    "            \"upper\": np.delete(upper_limit_intconf_matrix_100_35[base_cluster], base_cluster)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define clusters to be compared against (excluding the base cluster)\n",
    "    compare_clusters = np.delete(clusters, base_cluster)\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "    width = 0.15  # Bar width\n",
    "    x = np.arange(len(compare_clusters))  # X positions for clusters\n",
    "\n",
    "    for idx, (n_neighbors, values) in enumerate(data.items()):\n",
    "        # Calculate positions for the current set of bars\n",
    "        x_positions = x + (idx - len(data) / 2) * width\n",
    "\n",
    "        # Plot bars for the mean distances\n",
    "        ax.bar(\n",
    "            x_positions,\n",
    "            values[\"mean\"],  # Mean distances\n",
    "            yerr=[\n",
    "                values[\"mean\"] - values[\"lower\"],  # Lower error\n",
    "                values[\"upper\"] - values[\"mean\"]   # Upper error\n",
    "            ],\n",
    "            width=width,\n",
    "            color=colors[n_neighbors],\n",
    "            alpha=0.7,\n",
    "            label=f\"n={n_neighbors}\",\n",
    "            capsize=5\n",
    "        )\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    ax.set_xlabel(\"Clusters\", fontsize=14)\n",
    "    ax.set_ylabel(\"Distance\", fontsize=14)\n",
    "    ax.set_title(f\"Confidence Intervals of Distances from Cluster {base_cluster} to Other Clusters\", fontsize=16)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f\"{i}\" for i in compare_clusters], fontsize=12)\n",
    "    ax.legend(title=\"n_neighbors\", fontsize=10)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot MST for a given normalized distance matrix\n",
    "def plot_mst(matrix, title, ax, color='red'):\n",
    "    # Create a graph from the distance matrix\n",
    "    G = nx.from_numpy_array(np.round(matrix, 3))\n",
    "    \n",
    "    # Compute the minimum spanning tree of the graph\n",
    "    mst = nx.minimum_spanning_tree(G)\n",
    "    \n",
    "    # Define positions for all nodes\n",
    "    pos = nx.spring_layout(mst, seed=42)\n",
    "    \n",
    "    # Draw the minimum spanning tree\n",
    "    nx.draw(mst, pos, with_labels=True, node_color='lightblue', edge_color=color, node_size=500, font_size=10, width=2, ax=ax)\n",
    "    \n",
    "    # Draw edge labels (distances)\n",
    "    edge_labels = nx.get_edge_attributes(mst, 'weight')\n",
    "    nx.draw_networkx_edge_labels(mst, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=ax)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Define your matrices for each n_neighbors value\n",
    "matrices = {\n",
    "    5: {\n",
    "        \"mean\": normalized_mean_distance_matrix_5_01_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_5_01_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_5_01_35\n",
    "    },\n",
    "    10: {\n",
    "        \"mean\": normalized_mean_distance_matrix_10_01_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_10_01_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_10_01_35\n",
    "    },\n",
    "    20: {\n",
    "        \"mean\": normalized_mean_distance_matrix_20_01_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_20_01_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_20_01_35\n",
    "    },\n",
    "    30: {\n",
    "        \"mean\": normalized_mean_distance_matrix_30_01_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_30_01_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_30_01_35\n",
    "    },\n",
    "    50: {\n",
    "        \"mean\": normalized_mean_distance_matrix_50_01_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_50_01_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_50_01_35\n",
    "    },\n",
    "    100: {\n",
    "        \"mean\": normalized_mean_distance_matrix_100_01_35,\n",
    "        \"lower\": norm_lower_limit_intconf_matrix_100_01_35,\n",
    "        \"upper\": norm_upper_limit_intconf_matrix_100_01_35\n",
    "    }\n",
    "}\n",
    "\n",
    "# Open a PDF to save the plots\n",
    "with PdfPages('MST_Comparisons min_dist=0.1.pdf') as pdf:\n",
    "    for n_neighbors, matrix_set in matrices.items():\n",
    "        # Set up the figure with three subplots\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # Plot MSTs for mean, lower, and upper matrices\n",
    "        plot_mst(matrix_set[\"mean\"], f\"MST - Mean Distances (n_neighbors={n_neighbors}, min_dist= 0.1)\", axes[0], color='red')\n",
    "        plot_mst(matrix_set[\"lower\"], f\"MST - Lower Limit (n_neighbors={n_neighbors}, min_dist= 0.1)\", axes[1], color='blue')\n",
    "        plot_mst(matrix_set[\"upper\"], f\"MST - Upper Limit (n_neighbors={n_neighbors}, min_dist= 0.1)\", axes[2], color='green')\n",
    "        \n",
    "        # Adjust layout for better spacing\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the current figure to the PDF\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(\"PDF with MST Comparisons has been successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the UMAP projections\n",
    "umap_projections_20_35 = np.load(f'umap_projections_neighbors_20.npy')\n",
    "\n",
    "# To see the contents of the UMAP projections\n",
    "print(umap_projections_20_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_centroids_20 = np.load(f\"kmeans_centroids_neighbors_20.npy\")  # Load the saved centroids data\n",
    "centroid_mean_neighbors_20 = np.mean(kmeans_centroids_20, axis=0)  # Calculate centroid mean\n",
    "np.save(f'centroid_mean_neighbors_20.npy', centroid_mean_neighbors_20)\n",
    "print(centroid_mean_neighbors_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation for the x-axis and y-axis across the clusters\n",
    "std_dev_x_20= round(np.std(centroid_mean_neighbors_20[:, 0]),2)  # Standard deviation for the x-axis\n",
    "std_dev_y_20= round(np.std(centroid_mean_neighbors_20[:, 1]),2) # Standard deviation for the y-axis\n",
    "\n",
    "# Print the results\n",
    "print(f'Standard deviation for x-axis: {std_dev_x_20}')\n",
    "print(f'Standard deviation for y-axis: {std_dev_y_20}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the centroid is within the 90% range\n",
    "def is_within_90_percent(centroid, centroid_mean_neighbors_20, std_dev_x_20, std_dev_y_20):\n",
    "    # Calculate the 90% range for x-axis\n",
    "    lower_bound_x_20 = centroid_mean_neighbors_20[0] - 1.645 * std_dev_x_20\n",
    "    upper_bound_x_20 = centroid_mean_neighbors_20[0] + 1.645 * std_dev_x_20\n",
    "    \n",
    "    # Calculate the 90% range for y-axis\n",
    "    lower_bound_y_20 = centroid_mean_neighbors_20[1] - 1.645 * std_dev_y_20\n",
    "    upper_bound_y_20 = centroid_mean_neighbors_20[1] + 1.645 * std_dev_y_20\n",
    "    \n",
    "    # Check if the centroid is within both x and y ranges\n",
    "    x_in_range_20 = lower_bound_x_20 <= centroid[0] <= upper_bound_x_20\n",
    "    y_in_range_20 = lower_bound_y_20 <= centroid[1] <= upper_bound_y_20\n",
    "    \n",
    "    # Return True if both x and y are within the range, False otherwise\n",
    "    return x_in_range_20 and y_in_range_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table: Trial | Cluster | Centroid Coord | Inside the 90%\n",
    "\n",
    "result_table_20_35 = []\n",
    "n_runs = 35\n",
    "n_clusters = 10\n",
    "\n",
    "for run in range(n_runs):\n",
    "    for i in range(n_clusters):\n",
    "        centroid = kmeans_centroids_20[run][i]\n",
    "        centroid_mean = centroid_mean_neighbors_20[i]\n",
    "\n",
    "        # Check if the centroid is within the 90% range\n",
    "        inside_90 = is_within_90_percent(centroid, centroid_mean, std_dev_x_20, std_dev_y_20)\n",
    "        \n",
    "        # Append the result to the table\n",
    "        result_table_20_35.append([run + 1, i + 1, centroid, inside_90])\n",
    "\n",
    "# Convert result_table to a DataFrame for better readability\n",
    "df_results_20_35 = pd.DataFrame(result_table_20_35, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside the 90%'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_results_20_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result table to a CSV file\n",
    "df_results_20_35.to_csv(f'result_table_neighbors_{20}_35.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_20_35= pd.read_csv(f'result_table_neighbors_{20}_35.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the runs that have at least one centroid outside the 90% range\n",
    "runs_to_remove = df_results_20_35.loc[~df_results_20_35['Inside the 90%'], 'Trial'].unique()\n",
    "\n",
    "# Filter out the identified runs\n",
    "df_filtered_results_20_35 = df_results_20_35[~df_results_20_35['Trial'].isin(runs_to_remove)]\n",
    "\n",
    "# Step 3: Continue your analysis with the remaining runs\n",
    "print(f\"Runs removed: {runs_to_remove}\")\n",
    "print(f\"Remaining runs after filtering: {df_filtered_results_20_35['Trial'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot changes in X-coordinate for each cluster over all runs\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids[:, cluster, 0], marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Cluster {cluster} X-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('X Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot changes in Y-coordinate for each cluster over all runs\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids[:, cluster, 1], marker='o', linestyle='-', color='g')\n",
    "    plt.title(f'Cluster {cluster} Y-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Y Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Step: Convert the NumPy array into a DataFrame with 'Cluster', 'x_mean', and 'y_mean'\n",
    "centroid_mean_neighbors_20_df = pd.DataFrame(centroid_mean_neighbors_20, columns=['x_mean', 'y_mean'])\n",
    "centroid_mean_neighbors_20_df['Cluster'] = np.arange(10)\n",
    "\n",
    "# Step: Insert commas between the coordinates\n",
    "df_results_20_35['Centroid Coord'] = df_results_20_35['Centroid Coord'].str.replace(r'(\\-?\\d+\\.?\\d*)\\s+(\\-?\\d+\\.?\\d*)', r'\\1, \\2', regex=True)\n",
    "\n",
    "# Convert 'Centroid Coord' from string to list\n",
    "df_results_20_35['Centroid Coord'] = df_results_20_35['Centroid Coord'].apply(ast.literal_eval)\n",
    "\n",
    "# Step: Extract x and y coordinates from 'Centroid Coord' in `df_results_20_35`\n",
    "df_results_20_35[['x', 'y']] = pd.DataFrame(df_results_20_35['Centroid Coord'].tolist(), index=df_results_20_35.index)\n",
    "\n",
    "# Step: Merge the mean centroids dataframe with the results dataframe on 'Cluster'\n",
    "df_merged_20 = pd.merge(df_results_20_35, centroid_mean_neighbors_20_df, on='Cluster', how='left')\n",
    "\n",
    "# Step: Calculate the Euclidean distance between each centroid and the corresponding cluster mean\n",
    "df_merged_20['Distance_to_Mean'] = np.sqrt((df_merged_20['x'] - df_merged_20['x_mean'])**2 +\n",
    "                                           (df_merged_20['y'] - df_merged_20['y_mean'])**2)\n",
    "\n",
    "# Step: Apply an outlier threshold (e.g., 90th percentile of the distance per cluster)\n",
    "def filter_outliers(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)  # 90th percentile threshold\n",
    "    return df[df['Distance_to_Mean'] <= threshold]\n",
    "\n",
    "# Apply the filtering function for each cluster\n",
    "df_no_outliers_20 = df_merged_20.groupby('Cluster').apply(filter_outliers).reset_index(drop=True)\n",
    "\n",
    "# Step: Drop unnecessary columns if needed (like 'x' and 'y' if only the distance matters)\n",
    "df_no_outliers_cleaned_20 = df_no_outliers_20.drop(columns=['x', 'y', 'x_mean', 'y_mean'])\n",
    "\n",
    "# Step: Check the size of the resulting dataframe\n",
    "print(f\"Original DataFrame size: {df_merged_20.shape}\")\n",
    "print(f\"DataFrame size after removing outliers: {df_no_outliers_cleaned_20.shape}\")\n",
    "\n",
    "# Step: Group the dataframe by 'Cluster'\n",
    "clusters_grouped_20 = df_no_outliers_cleaned_20.groupby('Cluster')\n",
    "\n",
    "# Step: Create a dictionary to store arrays for each cluster's centroids\n",
    "clusters_centroids_20 = {}\n",
    "\n",
    "# Step: Loop through each group (cluster) and store the centroids in arrays\n",
    "for cluster, group in clusters_grouped_20:\n",
    "    # Extract centroids (x, y) as a NumPy array\n",
    "    centroids_array_20 = np.array(group['Centroid Coord'].tolist())  # Assuming 'Centroid Coord' contains [x, y] pairs\n",
    "    clusters_centroids_20[cluster] = centroids_array_20\n",
    "\n",
    "# Step: Loop through each cluster and plot\n",
    "for cluster, group in clusters_grouped_20:\n",
    "    # Extract Trial numbers, x and y coordinates from the group\n",
    "    trials = group['Trial'].values\n",
    "    centroids_x_20 = np.array([coord[0] for coord in group['Centroid Coord']])\n",
    "    centroids_y_20 = np.array([coord[1] for coord in group['Centroid Coord']])\n",
    "\n",
    "    # Step: Plot Trial vs. X-coordinate for this cluster\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(trials, centroids_x_20, marker='o', linestyle='-', label=f'Cluster {cluster} X-coordinate')\n",
    "    plt.title(f'Cluster {cluster}: Trial vs. X-coordinate')\n",
    "    plt.xlabel('Trial')\n",
    "    plt.ylabel('X-coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Step: Plot Trial vs. Y-coordinate for this cluster\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(trials, centroids_y_20, marker='o', linestyle='-', label=f'Cluster {cluster} Y-coordinate')\n",
    "    plt.title(f'Cluster {cluster}: Trial vs. Y-coordinate')\n",
    "    plt.xlabel('Trial')\n",
    "    plt.ylabel('Y-coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Step: Create a dictionary to store the size of each cluster\n",
    "cluster_sizes_20 = {cluster: len(centroids) for cluster, centroids in clusters_centroids_20.items()}\n",
    "\n",
    "# Step: Print the size of each cluster\n",
    "for cluster, size in cluster_sizes_20.items():\n",
    "    print(f\"Cluster {cluster} has {size} centroids considered.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance matrix**: elemnt d_{ij} has the distance between the center of cluster i and cluster j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store distance matrices for each run\n",
    "distance_matrices_20_35 = []\n",
    "\n",
    "# Iterate over all runs and calculate the distance matrix for each run\n",
    "for run_centroids in kmeans_centroids_20:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix_20_35 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    distance_matrices_20_35.append(distance_matrix_20_35)\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "distance_matrices_20_35 = np.array(distance_matrices_20_35)\n",
    "\n",
    "# Save the distance matrices for all runs\n",
    "np.save(f'distance_matrices_neighbors_20_all_runs.npy', distance_matrices_20_35)\n",
    "\n",
    "# Optionally, inspect the distance matrix for the first run\n",
    "print(f\"Distance matrix for the first run:\\n{distance_matrices_20_35[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all runs and print each distance matrix\n",
    "for run_idx, distance_matrix in enumerate(distance_matrices_20_35):\n",
    "    print(f\"Distance matrix for run {run_idx + 1}:\\n{distance_matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_dist_n5 = np.mean(distance_matrices_5_35, axis=0)\n",
    "avg_dist_n10 = np.mean(distance_matrices_10_35, axis=0)\n",
    "avg_dist_n20 = np.mean(distance_matrices_20_35, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of matrices and corresponding titles\n",
    "matrices = [avg_dist_n5, avg_dist_n10, avg_dist_n20]\n",
    "titles = ['Heatmap of Mean Distance Matrix (avg_dist_n5)',\n",
    "          'Heatmap of Mean Distance Matrix (avg_dist_n10)',\n",
    "          'Heatmap of Mean Distance Matrix (avg_dist_n20)']\n",
    "\n",
    "# Loop through matrices and titles to create heatmaps\n",
    "for matrix, title in zip(matrices, titles):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(matrix, annot=True, fmt=\".2f\", cmap='viridis', cbar=True)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the UMAP projections\n",
    "umap_projections_20 = np.load(f'umap_projections_neighbors_20.npy')\n",
    "\n",
    "# To see the contents of the UMAP projections\n",
    "print(umap_projections_20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_mean_neighbors_20 = centroid_mean\n",
    "# centroid_mean_neighbors_20= np.load(f'centroid_mean_neighbors_20.npy')\n",
    "kmeans_centroids_20 = np.load(f\"kmeans_centroids_neighbors_20.npy\")\n",
    "print(centroid_mean_neighbors_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation for the x-axis and y-axis across the clusters\n",
    "std_dev_x_20 = np.std(centroid_mean_neighbors_20[:, 0])  # Standard deviation for the x-axis\n",
    "std_dev_y_20 = np.std(centroid_mean_neighbors_20[:, 1])  # Standard deviation for the y-axis\n",
    "\n",
    "# Print the results\n",
    "print(f'Standard deviation for x-axis: {std_dev_x_20}')\n",
    "print(f'Standard deviation for y-axis: {std_dev_y_20}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the centroid is within the 90% range\n",
    "def is_within_90_percent(centroid, centroid_mean_neighbors_20, std_dev_x_20, std_dev_y_20):\n",
    "    # Calculate the 90% range for x-axis\n",
    "    lower_bound_x_20 = centroid_mean_neighbors_20[0] - 1.645 * std_dev_x_20\n",
    "    upper_bound_x_20 = centroid_mean_neighbors_20[0] + 1.645 * std_dev_x_20\n",
    "    \n",
    "    # Calculate the 90% range for y-axis\n",
    "    lower_bound_y_20 = centroid_mean_neighbors_20[1] - 1.645 * std_dev_y_20\n",
    "    upper_bound_y_20 = centroid_mean_neighbors_20[1] + 1.645 * std_dev_y_20\n",
    "    \n",
    "    # Check if the centroid is within both x and y ranges\n",
    "    x_in_range_20 = lower_bound_x_20 <= centroid[0] <= upper_bound_x_20\n",
    "    y_in_range_20 = lower_bound_y_20 <= centroid[1] <= upper_bound_y_20\n",
    "    \n",
    "    # Return True if both x and y are within the range, False otherwise\n",
    "    return x_in_range_20 and y_in_range_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table to store the results: Trial | Cluster | Centroid Coord | Inside the 90%\n",
    "result_table_20 = []\n",
    "n_runs = 35\n",
    "n_clusters = 10\n",
    "\n",
    "for run in range(n_runs):\n",
    "    for i in range(n_clusters):\n",
    "        centroid_20 = kmeans_centroids_20[run][i]\n",
    "        centroid_mean_20 = centroid_mean_neighbors_20[i]\n",
    "\n",
    "        # Check if the centroid is within the 90% range\n",
    "        inside_90_20 = is_within_90_percent(centroid_20, centroid_mean_20, std_dev_x_20, std_dev_y_20)\n",
    "        \n",
    "        # Append the result to the table\n",
    "        result_table_20.append([run + 1, i + 1, centroid_20, inside_90_20])\n",
    "\n",
    "# Convert result_table to a DataFrame for better readability\n",
    "df_results_20 = pd.DataFrame(result_table_20, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside the 90%'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_results_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot an ellipse representing the 90% confidence interval\n",
    "def plot_confidence_ellipse_25(ax, centroid_mean_25, std_dev_x_25, std_dev_y_25, edgecolor='blue'):\n",
    "    ellipse = Ellipse(xy=centroid_mean_25, width=2*1.645*std_dev_x_25, height=2*1.645*std_dev_y_25,\n",
    "                      edgecolor=edgecolor, fc='None', lw=2)\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "# Create a plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the centroids\n",
    "for i in range(n_clusters):\n",
    "    for run in range(n_runs):\n",
    "        centroid_25 = kmeans_centroids_25[run][i]\n",
    "        inside_90_25 = is_within_90_percent(centroid_25, centroid_mean_25, std_dev_x_25, std_dev_y_25)\n",
    "        \n",
    "        # Choose a color depending on whether the centroid is inside the 90% range\n",
    "        color = 'green' if inside_90 else 'red'\n",
    "        \n",
    "        # Plot the centroid\n",
    "        ax.scatter(centroid_25[0], centroid_25[1], color=color, label=f'Cluster {i + 1}' if run == 0 else \"\")\n",
    "\n",
    "    # Plot the 90% confidence ellipse for each cluster's mean centroid\n",
    "    plot_confidence_ellipse_25(ax, centroid_mean_neighbors_25[i], std_dev_x_25, std_dev_y_25, edgecolor='blue')\n",
    "\n",
    "# Labeling the plot\n",
    "ax.set_title('Centroids and 90% Confidence Range')\n",
    "ax.set_xlabel('UMAP X-axis')\n",
    "ax.set_ylabel('UMAP Y-axis')\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table to store the results: Trial | Cluster | Centroid Coord | Inside the 90%\n",
    "result_table = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    for i in range(n_clusters):\n",
    "        centroid = kmeans_centroids[run][i]\n",
    "        mean = centroid_mean[i]\n",
    "        std = centroid_std[i]\n",
    "        inside_90 = is_within_90_percent(centroid, mean, std)\n",
    "        result_table.append([run + 1, i, centroid, inside_90])\n",
    "\n",
    "# Convert result_table to a DataFrame for better readability\n",
    "df_results = pd.DataFrame(result_table, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside the 90%'])\n",
    "\n",
    "# Display the DataFrame using standard Pandas functions\n",
    "print(df_results)\n",
    "\n",
    "# Print mean and standard deviation for clarity\n",
    "print(\"Centroid Means (per cluster):\\n\", centroid_mean)\n",
    "print(\"Centroid Standard Deviations (per cluster):\\n\", centroid_std)\n",
    "\n",
    "# Save the result table to a CSV file\n",
    "df_results.to_csv(f'result_table_neighbors_{n_neighbors3}.csv', index=False)\n",
    "\n",
    "# Save the centroid means and standard deviations\n",
    "np.save(f'centroid_mean_neighbors_{n_neighbors3}.npy', centroid_mean)\n",
    "np.save(f'centroid_std_neighbors_{n_neighbors3}.npy', centroid_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## min_dist Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_dist = 0, n_neighbors= 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_5_0_2 = np.load(f'umap_projections_neighbors_5_0_2.npy')\n",
    "centroid_mean_5_0_2= np.load(f'centroid_mean_5_0_2.npy')\n",
    "centroid_std_5_0_2= np.load(f'centroid_std_5_0_2.npy')\n",
    "kmeans_centroids_5_0_2 = np.load(f\"kmeans_centroids_neighbors_5_0_2.npy\")\n",
    "df_results_5_0_2=pd.read_csv(f'result_table_neighbors_v2_20_35.csv')\n",
    "mean_distance_matrix_5_0_2= np.load(f'mean_distance_matrix_neighbors_5_0_2.npy')\n",
    "normalized_mean_distance_matrix_5_0_2= np.load(f'normalized_mean_distance_matrix_5_0_2.npy')\n",
    "distance_matrix_std_5_0_2= np.load(f\"distance_matrix_std_5_0_2.npy\")\n",
    "normalized_distance_matrix_std_5_0_2= np.load(f\"normalized_distance_matrix_std_5_0_2.npy\")\n",
    "mst_std_5_0_2= np.load(f'mst_std_5_0_2.npy')\n",
    "mst_5_0_2= np.load(f'mst_5_0_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of UMAP and KMeans runs\n",
    "n_runs = 35\n",
    "n_clusters = 10  # Set the number of clusters (for KMeans)\n",
    "min_dist = 0\n",
    "n_neighbors = 5\n",
    "n_components = 2\n",
    "\n",
    "# Store UMAP and KMeans results for each run\n",
    "umap_projections_5_0_2 = []\n",
    "kmeans_centroids_list_5_0_2 = []  # Use this to store centroids for each run\n",
    "\n",
    "# Define a helper function to calculate the centroid of each cluster\n",
    "def calculate_centroids(kmeans, x_umap):\n",
    "    centroids_5_0_2 = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = x_umap[kmeans.labels_ == i]\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        centroids_5_0_2.append(centroid)\n",
    "    return np.array(centroids_5_0_2)\n",
    "\n",
    "# Run UMAP and KMeans multiple times\n",
    "for run in range(n_runs):\n",
    "    # Apply UMAP with the same parameters for each run\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=None)  # No random_state to allow randomness, use random_state=None\n",
    "    x_train_umap_5_0_2 = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "     # Apply KMeans clustering on the UMAP projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_train_umap_5_0_2)\n",
    "\n",
    "    # Calculate centroids for this run\n",
    "    centroids_5_0_2 = calculate_centroids(kmeans, x_train_umap_5_0_2)\n",
    "    \n",
    "    # Store the UMAP projections and KMeans models\n",
    "    umap_projections_5_0_2.append(x_train_umap_5_0_2)\n",
    "    kmeans_centroids_list_5_0_2.append(centroids_5_0_2)\n",
    "\n",
    "# Now we calculate the mean and standard deviation of the centroids across all runs\n",
    "kmeans_centroids_5_0_2 = np.array(kmeans_centroids_list_5_0_2)  \n",
    "\n",
    "# Calculate mean and std deviation for centroids' coordinates\n",
    "centroid_mean_5_0_2 = np.mean(kmeans_centroids_5_0_2, axis=0)\n",
    "centroid_std_5_0_2 = np.std(kmeans_centroids_5_0_2, axis=0)\n",
    "\n",
    "# Save the UMAP projections and KMeans centroids\n",
    "np.save(f'umap_projections_neighbors_{n_neighbors}_{min_dist}_{n_components}.npy', np.array(umap_projections_5_0_2))\n",
    "np.save(f'kmeans_centroids_neighbors_{n_neighbors}_{min_dist}_{n_components}.npy', np.array(kmeans_centroids_list_5_0_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the UMAP projections\n",
    "umap_projections_5_0_2 = np.load(f'umap_projections_neighbors_5_0_2.npy')\n",
    "\n",
    "# To see the contents of the UMAP projections\n",
    "print(umap_projections_5_0_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "\n",
    "# Save the centroid_mean and centroid_std\n",
    "np.save(f'centroid_mean_{n_neighbors}_0_2.npy', np.array(centroid_mean_5_0_2))\n",
    "np.save(f'centroid_std_{n_neighbors}_0_2.npy', np.array(centroid_mean_5_0_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_mean_5_0_2= np.load(f'centroid_mean_5_0_2.npy')\n",
    "centroid_std_5_0_2= np.load(f'centroid_std_5_0_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_centroids_5_0_2 = np.load(f\"kmeans_centroids_neighbors_5_0_2.npy\")  # Load the saved centroids data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_5_0_2 = np.zeros(10)\n",
    "std_dev_y_5_0_2 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_5_0_2[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_5_0_2[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_5_0_2[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_5_0_2[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_5_0_2)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_5_0_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_5_0_2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_5_0_2[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_5_0_2[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x_5_0_2[cluster], mean_x + 2 * std_dev_x_5_0_2[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y_5_0_2[cluster], mean_y + 2 * std_dev_y_5_0_2[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_5_0_2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_5_0_2 = pd.DataFrame(data_5_0_2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true_5_0_2 = df_results_5_0_2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true_5_0_2 = trials_all_true_5_0_2[trials_all_true_5_0_2].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true_5_0_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false_5_0_2 = trials_all_true_5_0_2[~trials_all_true_5_0_2].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false_5_0_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Save the result table to a CSV file\n",
    "df_results_5_0_2.to_csv(f'result_table_neighbors_v2_{5}_0_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal outliers process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_5_0_2=pd.read_csv('result_table_neighbors_v2_20_35.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array into a DataFrame with 'Cluster', 'x_mean', and 'y_mean'\n",
    "centroid_mean_5_0_2_df = pd.DataFrame(centroid_mean_5_0_2, columns=['x_mean', 'y_mean'])\n",
    "centroid_mean_5_0_2_df['Cluster'] = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Add commas between numbers in 'Centroid Coord' entries if they are missing\n",
    "df_results_5_0_2['Centroid Coord'] = df_results_5_0_2['Centroid Coord'].str.replace(\n",
    "    r'(\\-?\\d+\\.\\d+)\\s+(\\-?\\d+\\.\\d+)', r'\\1, \\2', regex=True\n",
    ")\n",
    "\n",
    "# Step 2: Convert 'Centroid Coord' from string to list\n",
    "df_results_5_0_2['Centroid Coord'] = df_results_5_0_2['Centroid Coord'].apply(ast.literal_eval)\n",
    "\n",
    "# Step 3: Verify if each entry in 'Centroid Coord' is a list of length 2\n",
    "invalid_rows = df_results_5_0_2[df_results_5_0_2['Centroid Coord'].apply(lambda x: not (isinstance(x, list) and len(x) == 2))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y coordinates\n",
    "df_results_5_0_2[['x', 'y']] = pd.DataFrame(df_results_5_0_2['Centroid Coord'].tolist(), index=df_results_5_0_2.index)\n",
    "\n",
    "# Merge the mean centroids dataframe with the results dataframe on 'Cluster'\n",
    "df_merged_5_0_2 = pd.merge(df_results_5_0_2, centroid_mean_5_0_2_df, on='Cluster', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot changes in X-coordinate for each cluster over all runs\n",
    "n_runs = 35\n",
    "n_clusters = 10\n",
    "n_neighbors = 5\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_5_0_2[:, cluster, 0], marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Cluster {cluster} X-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('X Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot changes in Y-coordinate for each cluster over all runs\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_5_0_2[:, cluster, 1], marker='o', linestyle='-', color='g')\n",
    "    plt.title(f'Cluster {cluster} Y-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Y Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance from each centroid to its cluster's mean\n",
    "df_merged_5_0_2['Distance_to_Mean'] = np.sqrt((df_merged_5_0_2['x'] - df_merged_5_0_2['x_mean'])**2 + (df_merged_5_0_2['y'] - df_merged_5_0_2['y_mean'])**2)\n",
    "\n",
    "# Apply an outlier threshold (e.g., 90th percentile of the distance per cluster)\n",
    "def filter_outliers(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return df[df['Distance_to_Mean'] <= threshold]\n",
    "\n",
    "# Apply the filtering function for each cluster\n",
    "df_no_outliers_5_0_2 = df_merged_5_0_2.groupby('Cluster').apply(filter_outliers).reset_index(drop=True)\n",
    "\n",
    "# Step 7: Drop unnecessary columns if needed (like 'x' and 'y' if only the distance matters)\n",
    "df_no_outliers_cleaned_5_0_2 = df_no_outliers_5_0_2.drop(columns=['x', 'y', 'x_mean', 'y_mean'])\n",
    "\n",
    "# Step 8: Check the size of the resulting dataframe\n",
    "print(f\"Original DataFrame size: {df_merged_5_0_2.shape}\")\n",
    "print(f\"DataFrame size after removing outliers: {df_no_outliers_cleaned_5_0_2.shape}\")\n",
    "\n",
    "# Display the final dataframe to the user\n",
    "df_no_outliers_cleaned_5_0_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by 'Cluster'\n",
    "clusters_grouped_5_0_2 = df_no_outliers_cleaned_5_0_2.groupby('Cluster')\n",
    "\n",
    "# Create a dictionary to store arrays for each cluster's centroids\n",
    "clusters_centroids_5_0_2 = {}\n",
    "\n",
    "# Loop through each group (cluster) and store the centroids in arrays\n",
    "for cluster, group in clusters_grouped_5_0_2:\n",
    "    # Extract centroids (x, y) as a NumPy array\n",
    "    centroids_array = np.array(group['Centroid Coord'].tolist())  # Assuming 'Centroid Coord' contains [x, y] pairs\n",
    "    clusters_centroids_5_0_2[cluster] = centroids_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the size of each cluster\n",
    "cluster_sizes_5_0_2 = {cluster: len(centroids) for cluster, centroids in clusters_centroids_5_0_2.items()}\n",
    "\n",
    "# Print the size of each cluster\n",
    "for cluster, size in cluster_sizes_5_0_2.items():\n",
    "    print(f\"Cluster {cluster} has {size} centroids considered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to verify that if it is fine to have all clusters with the same number of centroids after filtering out outliers. This must be due to:\n",
    "- The Distance Distributions are Likely Very Similar\n",
    "- Uniform Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each cluster and plot the distribution of distances\n",
    "for cluster, group in clusters_grouped_5_0_2:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(group['Distance_to_Mean'], bins=10, edgecolor='black')\n",
    "    plt.title(f'Cluster {cluster}: Distance to Mean Distribution')\n",
    "    plt.xlabel('Distance to Mean')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentile threshold per cluster check\n",
    "def check_percentiles(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return threshold\n",
    "\n",
    "# Function applied to each cluster and print the result\n",
    "for cluster, group in clusters_grouped_5_0_2:\n",
    "    threshold = check_percentiles(group)\n",
    "    print(f\"Cluster {cluster}: 90th percentile threshold = {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster, calculate the 70th percentile of distances and filter accordingly\n",
    "for cluster, group in clusters_grouped_5_0_2:\n",
    "    # Calculate the 70th percentile threshold for the current cluster\n",
    "    threshold = np.percentile(group['Distance_to_Mean'], 70)\n",
    "    \n",
    "    # Filter centroids based on the 70th percentile\n",
    "    filtered_group = group[group['Distance_to_Mean'] <= threshold]\n",
    "    \n",
    "    # Print the size of the group before and after filtering\n",
    "    print(f\"Cluster {cluster}: Original size = {len(group)}, Filtered size = {len(filtered_group)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance matrix n=5, min_dist= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Mean matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance matrix**: elemnt d_{ij} has the distance between the center of cluster i and cluster j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store distance matrices for each run\n",
    "distance_matrices_5_0_2 = []\n",
    "\n",
    "# Iterate over all runs and calculate the distance matrix for each run\n",
    "for run_centroids in kmeans_centroids_5_0_2:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    dist_matrix = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    distance_matrices_5_0_2.append(dist_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a numpy array (35 runs, 10x10 distance matrices)\n",
    "distance_matrices_5_0_2 = np.array(distance_matrices_5_0_2)\n",
    "\n",
    "# Calculate the mean distance matrix across all runs\n",
    "mean_distance_matrix_5_0_2 = np.mean(distance_matrices_5_0_2, axis=0)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_5_0_2 = (mean_distance_matrix_5_0_2 - np.min(mean_distance_matrix_5_0_2)) / (np.max(mean_distance_matrix_20_35) - np.min(mean_distance_matrix_5_0_2))\n",
    "\n",
    "# Plot of the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_0_2, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=5, min_dist = 0)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_5_0_2_all_runs.npy', distance_matrices_5_0_2)\n",
    "np.save('mean_distance_matrix_neighbors_5_0_2.npy', mean_distance_matrix_5_0_2)\n",
    "\n",
    "# Mean distance matrix\n",
    "print(f\"Mean distance matrix across all runs:\\n{mean_distance_matrix_5_0_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MST Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_5_0_2 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_5_0_2,3))\n",
    "np.save('G_5_0_2.npy',G_5_0_2)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_5_0_2, seed=42)  # positions for all nodes\n",
    "nx.draw(G_5_0_2, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_5_0_2, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_5_0_2, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_5_0_2 = nx.minimum_spanning_tree(G_5_0_2)\n",
    "np.save('mst_5_0_2.npy', mst_5_0_2)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_5_0_2, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_5_0_2, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_5_0_2, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_5_0_2, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST - min_dist= 0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Std. dev. Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the pairwise distance matrix for the standard deviations\n",
    "distance_matrix_std_5_0_2 = cdist(centroid_std_5_0_2, centroid_std_5_0_2, metric='euclidean')\n",
    "\n",
    "# Normalize the distance matrix\n",
    "normalized_distance_matrix_std_5_0_2 = (distance_matrix_std_5_0_2 - np.min(distance_matrix_std_5_0_2)) / (np.max(distance_matrix_std_5_0_2) - np.min(distance_matrix_std_5_0_2))\n",
    "\n",
    "# Visualize the normalized distance matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(normalized_distance_matrix_std_5_0_2, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Distance Matrix for Centroid Std Deviations (min_dist= 0)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrix for later analysis\n",
    "np.save(\"distance_matrix_std_5_0_2.npy\", distance_matrix_std_5_0_2)\n",
    "np.save(\"normalized_distance_matrix_std_5_0_2.npy\", normalized_distance_matrix_std_5_0_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_std_5_0_2 = nx.from_numpy_array(np.round(normalized_distance_matrix_std_5_0_2,3))\n",
    "np.save('G_std_5_0_2.npy',G_std_5_0_2)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_std_5_0_2, seed=42)  # positions for all nodes\n",
    "nx.draw(G_std_5_0_2, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_std_5_0_2, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_std_5_0_2, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_std_5_0_2 = nx.minimum_spanning_tree(G_std_5_0_2)\n",
    "np.save('mst_std_5_0_2.npy',mst_std_5_0_2)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos_std_5_0_2 = nx.spring_layout(mst_std_5_0_2, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_std_5_0_2, pos_std_5_0_2, with_labels=True, node_color='lightyellow', edge_color='green', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels_std_5_0_2 = nx.get_edge_attributes(mst_std_5_0_2, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_5_0_2, pos_std_5_0_2, edge_labels=edge_labels_std_5_0_2, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST Std. Deviation - min_dist= 0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Edges in the original graph (G_std_5_0_2):\")\n",
    "print(G_std_5_0_2.edges(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = nx.shortest_path(mst_std_5_0_2, source=6, target=1, weight='weight')\n",
    "path_length = nx.shortest_path_length(mst_std_5_0_2, source=6, target=1, weight='weight')\n",
    "print(\"Path between nodes 6 and 1:\", path)\n",
    "print(\"Path length:\", path_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create heatmaps\n",
    "def plot_heatmaps_side_by_side(matrices, titles, figsize=(16, 8), cmap=\"viridis\", annot=True):\n",
    "    \"\"\"\n",
    "    Plots multiple heatmaps side by side for given matrices and titles.\n",
    "\n",
    "    Args:\n",
    "        matrices (list): List of 2D matrices to plot as heatmaps.\n",
    "        titles (list): List of titles corresponding to each matrix.\n",
    "        figsize (tuple): Size of the entire figure (default: (16, 8)).\n",
    "        cmap (str): Color map to use for all heatmaps (default: \"viridis\").\n",
    "        annot (bool): Whether to annotate cells with their values (default: True).\n",
    "    \"\"\"\n",
    "    n = len(matrices)  # Number of heatmaps\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "\n",
    "    for i, (matrix, title) in enumerate(zip(matrices, titles)):\n",
    "        sns.heatmap(matrix, annot=annot, cmap=cmap, fmt=\".2f\", linewidths=0.5, ax=axes[i])\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_xlabel(\"Cluster\")\n",
    "        axes[i].set_ylabel(\"Cluster\" if i == 0 else \"\")  # Only label y-axis for the first plot\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the two heatmaps\n",
    "plot_heatmaps_side_by_side(\n",
    "    matrices=[\n",
    "        normalized_distance_matrix_std_5_0_2,\n",
    "        normalized_mean_distance_matrix_5_0_2\n",
    "    ],\n",
    "    titles=[\n",
    "        \"Normalized Distance Matrix for Centroid Std Deviations (min_dist=0)\",\n",
    "        \"Normalized Mean Distance Matrix (k=10, min_dist=0)\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for \"low\" values\n",
    "threshold = 0.65\n",
    "\n",
    "# Identify pairs of clusters with low values in both matrices\n",
    "low_low_pairs = []\n",
    "for i in range(normalized_mean_distance_matrix_5_0_2.shape[0]):\n",
    "    for j in range(normalized_mean_distance_matrix_5_0_2.shape[1]):\n",
    "        if i != j:  # Skip diagonal\n",
    "            mean_value = normalized_mean_distance_matrix_5_0_2[i, j]\n",
    "            std_value = normalized_distance_matrix_std_5_0_2[i, j]\n",
    "            if mean_value < threshold and std_value < threshold:\n",
    "                low_low_pairs.append((i, j, mean_value, std_value))\n",
    "\n",
    "# Display the results\n",
    "for pair in low_low_pairs:\n",
    "    print(f\"Clusters {pair[0]} and {pair[1]}: Mean Distance = {pair[2]:.2f}, Std Distance = {pair[3]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0.65  lowest threshold so far.**\n",
    "\n",
    "Depending on the goal of the analysis we can think of it as:\n",
    "- If the aim is to identify the strongest relationships between clusters, a lower threshold would make more sense.\n",
    "- If we want to explore the broader connections, then it is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_distances_5_0_2_d= np.load(f'max_intra_cluster_distances_dynamic_5_0_2.npy')\n",
    "neighbor_counts_5_0_2_d= np.load(f'neighbor_counts_within_dynamic_radius_5_0_2.npy')\n",
    "kmeans_labels_list_5_0_2_d= np.load(f'kmeans_labels_list_5_0_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Replace with your cluster pairs from low_low_pairs\n",
    "low_low_pairs = [(1, 8, 0.65, 0.19), (3, 8, 0.65, 0.22)]\n",
    "\n",
    "# UMAP projections and cluster labels (replace with your actual data)\n",
    "umap_projections = np.load(\"umap_projections_neighbors_5_0_2.npy\")\n",
    "kmeans_labels = np.load(\"kmeans_labels_list_5_0_2.npy\")  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Function to plot clusters\n",
    "def plot_clusters(umap_projection, labels, cluster_pair, run_idx):\n",
    "    cluster_a, cluster_b = cluster_pair\n",
    "    points_a = umap_projection[labels == cluster_a]\n",
    "    points_b = umap_projection[labels == cluster_b]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(points_a[:, 0], points_a[:, 1], color=\"blue\", label=f\"Cluster {cluster_a}\", alpha=0.6)\n",
    "    plt.scatter(points_b[:, 0], points_b[:, 1], color=\"orange\", label=f\"Cluster {cluster_b}\", alpha=0.6)\n",
    "    plt.title(f\"Run {run_idx}: Cluster {cluster_a} vs. Cluster {cluster_b}\")\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze each cluster pair\n",
    "for cluster_pair in low_low_pairs:\n",
    "    cluster_a, cluster_b = cluster_pair[0], cluster_pair[1]\n",
    "    print(f\"Analyzing Cluster Pair: {cluster_a} and {cluster_b}\")\n",
    "    \n",
    "    # For simplicity, visualize them in a specific UMAP run (e.g., the first run)\n",
    "    run_idx = 0  # Use the first run for visualization\n",
    "    plot_clusters(umap_projections[run_idx], kmeans_labels[run_idx], (cluster_a, cluster_b), run_idx)\n",
    "\n",
    "    # Calculate additional statistics if needed\n",
    "    distances_a_to_b = np.linalg.norm(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a].mean(axis=0) - \n",
    "                                      umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b].mean(axis=0))\n",
    "    print(f\"Mean Centroid Distance (Run {run_idx}): {distances_a_to_b:.2f}\")\n",
    "\n",
    "    # Variability comparison\n",
    "    cluster_a_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a], axis=0)\n",
    "    cluster_b_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b], axis=0)\n",
    "    print(f\"Cluster {cluster_a} Std Dev: {cluster_a_std}\")\n",
    "    print(f\"Cluster {cluster_b} Std Dev: {cluster_b_std}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 1 and Cluster 8** have a moderate spatial relationship with visible overlap in the UMAP space. Their differing variability patterns suggest distinct structures, but the overlap points might represent shared features or transitions between the clusters.\n",
    "The large spatial separation between their centroids suggests they represent distinct structures or classes in the data.\n",
    "\n",
    "**Cluster 0 and Cluster 9** 9 appears more compact and stable, while Cluster 0 is larger and more variable.\n",
    "Their distinct regions in the UMAP space and differing standard deviations reinforce their meaningful separation.\n",
    "Insights from Variability:\n",
    "\n",
    "The variability of Cluster 0 could indicate sensitivity to UMAP parameters or noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intra class evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to track the minimum distance and corresponding clusters\n",
    "overall_min_distance_5_0_2 = float('inf')\n",
    "min_distance_clusters_5_0_2 = None\n",
    "min_distance_run_idx_5_0_2= None\n",
    "\n",
    "for run_idx, run_centroids in enumerate(kmeans_centroids_5_0_2):\n",
    "    # Compute pairwise distances between centroids\n",
    "    pairwise_distances_5_0_2 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    \n",
    "    # Get the indices of the minimum non-zero distance\n",
    "    np.fill_diagonal(pairwise_distances_5_0_2, np.inf)  # Ignore zero distances (self-comparisons)\n",
    "    min_distance = np.min(pairwise_distances_5_0_2)\n",
    "    if min_distance < overall_min_distance_5_0_2:\n",
    "        overall_min_distance_5_0_2 = min_distance\n",
    "        # Find the indices of the clusters corresponding to the minimum distance\n",
    "        cluster_indices = np.unravel_index(np.argmin(pairwise_distances_5_0_2), pairwise_distances_5_0_2.shape)\n",
    "        min_distance_clusters_5_0_2 = cluster_indices\n",
    "        min_distance_run_idx_5_0_2 = run_idx\n",
    "\n",
    "# Calculate dynamic radius\n",
    "dynamic_radius_5_0_2 = overall_min_distance_5_0_2 / 2\n",
    "print(f\"Dynamic radius: {dynamic_radius_5_0_2}\")\n",
    "print(f\"Minimum distance: {overall_min_distance_5_0_2}\")\n",
    "print(f\"Clusters contributing to minimum distance: {min_distance_clusters_5_0_2}\")\n",
    "print(f\"Run index: {min_distance_run_idx_5_0_2}\")\n",
    "\n",
    "# Save dynamic radius\n",
    "np.save('dynamic_radius_results_5_0_2.npy', dynamic_radius_5_0_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "\n",
    "# Dynamic radius, previously calculated\n",
    "radius = dynamic_radius_5_0_2\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_5_0_2_d = []\n",
    "neighbor_counts_5_0_2_d = []\n",
    "kmeans_labels_list_5_0_2_d = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_5_0_2):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_5_0_2_d = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_5_0_2_d.append(kmeans_labels_5_0_2_d)\n",
    "    \n",
    "    run_max_distances_5_0_2_d = []\n",
    "    run_neighbor_counts_5_0_2_d = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_5_0_2_d == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_5_0_2_d = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_5_0_2_d.append(max_distance_5_0_2_d)\n",
    "        \n",
    "        # Calculate number of neighbors within the dynamic radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_5_0_2_d = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_5_0_2_d.append(neighbors_within_radius_5_0_2_d)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_5_0_2_d.append(run_max_distances_5_0_2_d)\n",
    "    neighbor_counts_5_0_2_d.append(run_neighbor_counts_5_0_2_d)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_5_0_2_d = np.array(max_distances_5_0_2_d)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_5_0_2_d = np.array(neighbor_counts_5_0_2_d)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_5_0_2_d = np.array(kmeans_labels_list_5_0_2_d)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_dynamic_5_0_2.npy', max_distances_5_0_2_d)\n",
    "np.save('neighbor_counts_within_dynamic_radius_5_0_2.npy', neighbor_counts_5_0_2_d)\n",
    "np.save('kmeans_labels_list_5_0_2.npy', kmeans_labels_list_5_0_2_d)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_5_0_2_d)\n",
    "print(\"\\nNeighbor counts within dynamic radius for each run and each cluster:\\n\", neighbor_counts_5_0_2_d)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot neighbor counts for each cluster across all runs\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Iterate through each cluster\n",
    "for cluster_idx in range(neighbor_counts_5_0_2_d.shape[1]):\n",
    "    plt.subplot(2, 5, cluster_idx + 1)  # Create subplots for 10 clusters (2 rows, 5 columns)\n",
    "    plt.plot(range(1, neighbor_counts_5_0_2_d.shape[0] + 1), neighbor_counts_5_0_2_d[:, cluster_idx], marker='o')\n",
    "    plt.title(f'Cluster {cluster_idx}')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Neighbor Count')\n",
    "    plt.xticks(range(1, neighbor_counts_5_0_2_d.shape[0] + 1, 5))  # Show every 5th run on the x-axis for clarity\n",
    "    plt.grid(True)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.suptitle('min_dist=0 Neighbor Counts per Cluster Across Runs', y=1.02, fontsize=16)  # Add a global title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and max values across clusters for each run\n",
    "mean_neighbors_5_0_2 = np.mean(neighbor_counts_5_0_2_d, axis=1)  # Shape: (n_runs,)\n",
    "max_neighbors_5_0_2 = np.max(neighbor_counts_5_0_2_d, axis=1)    # Shape: (n_runs,)\n",
    "\n",
    "# Compute trend lines for mean and max\n",
    "runs = np.arange(1, len(mean_neighbors_5_0_2) + 1)\n",
    "mean_slope, mean_intercept, _, _, _ = linregress(runs, mean_neighbors_5_0_2)\n",
    "max_slope, max_intercept, _, _, _ = linregress(runs, max_neighbors_5_0_2)\n",
    "\n",
    "# Calculate trend line values\n",
    "mean_trend_5_0_2 = mean_slope * runs + mean_intercept\n",
    "max_trend_5_0_2 = max_slope * runs + max_intercept\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Mean neighbor counts\n",
    "plt.plot(runs, mean_neighbors_5_0_2, label='Mean Neighbor Count', marker='o', color='blue')\n",
    "\n",
    "# Max neighbor counts\n",
    "plt.plot(runs, max_neighbors_5_0_2, label='Max Neighbor Count', marker='s', color='orange')\n",
    "\n",
    "# Trend lines\n",
    "plt.plot(runs, mean_trend_5_0_2, linestyle='--', color='green',label='Mean Trend Line')\n",
    "plt.plot(runs, max_trend_5_0_2, linestyle='--', color='green', label='Max Trend Line')\n",
    "\n",
    "# Add labels, legend, and title\n",
    "plt.title('min_dist=0 Neighbor Counts Across Runs (Mean vs. Max with Trend Lines)', fontsize=16)\n",
    "plt.xlabel('Run', fontsize=12)\n",
    "plt.ylabel('Neighbor Count', fontsize=12)\n",
    "plt.xticks(range(1, len(mean_neighbors_5_0_2) + 1, 5))  # Show every 5th run for readability\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(f'neighbor_counts_plot_n_5_0_2.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_dist= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_5_05_2 = np.load(f'umap_projections_neighbors_5_0.5_2.npy')\n",
    "centroid_mean_5_05_2= np.load(f'centroid_mean_5_05_2.npy')\n",
    "centroid_std_5_05_2= np.load(f'centroid_std_5_05_2.npy')\n",
    "kmeans_centroids_5_05_2 = np.load(f\"kmeans_centroids_neighbors_5_0.5_2.npy\")\n",
    "df_results_5_05_2=pd.read_csv(f'result_table_neighbors_v2_20_35.csv')\n",
    "mean_distance_matrix_5_05_2= np.load(f'mean_distance_matrix_neighbors_5_05_2.npy')\n",
    "normalized_mean_distance_matrix_5_05_2= np.load(f'normalized_mean_distance_matrix_5_05_2.npy')\n",
    "distance_matrix_std_5_05_2= np.load(f\"distance_matrix_std_5_05_2.npy\")\n",
    "normalized_distance_matrix_std_5_05_2= np.load(f\"normalized_distance_matrix_std_5_05_2.npy\")\n",
    "mst_std_5_05_2= np.load(f'mst_std_5_05_2.npy')\n",
    "mst_5_05_2= np.load(f'mst_5_05_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of UMAP and KMeans runs\n",
    "n_runs = 35\n",
    "n_clusters = 10  # Set the number of clusters (for KMeans)\n",
    "min_dist = 0.5\n",
    "n_neighbors = 5\n",
    "n_components = 2\n",
    "\n",
    "# Store UMAP and KMeans results for each run\n",
    "umap_projections_5_05_2 = []\n",
    "kmeans_centroids_list_5_05_2 = []  # Use this to store centroids for each run\n",
    "\n",
    "# Define a helper function to calculate the centroid of each cluster\n",
    "def calculate_centroids(kmeans, x_umap):\n",
    "    centroids_5_05_2 = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = x_umap[kmeans.labels_ == i]\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        centroids_5_05_2.append(centroid)\n",
    "    return np.array(centroids_5_05_2)\n",
    "\n",
    "# Run UMAP and KMeans multiple times\n",
    "for run in range(n_runs):\n",
    "    # Apply UMAP with the same parameters for each run\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=None)  # No random_state to allow randomness, use random_state=None\n",
    "    x_train_umap_5_05_2 = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Apply KMeans clustering on the UMAP projection\n",
    "    kmeans_5_05_2 = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans_5_05_2.fit(x_train_umap_5_05_2)\n",
    "\n",
    "    # Calculate centroids for this run\n",
    "    centroids_5_05_2 = calculate_centroids(kmeans, x_train_umap_5_05_2)\n",
    "    \n",
    "    # Store the UMAP projections and KMeans models\n",
    "    umap_projections_5_05_2.append(x_train_umap_5_05_2)\n",
    "    kmeans_centroids_list_5_05_2.append(centroids_5_05_2)\n",
    "\n",
    "# Now we calculate the mean and standard deviation of the centroids across all runs\n",
    "kmeans_centroids_5_05_2 = np.array(kmeans_centroids_list_5_05_2)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "# Calculate mean and std deviation for centroids' coordinates\n",
    "centroid_mean_5_05_2 = np.mean(kmeans_centroids_5_05_2, axis=0)\n",
    "centroid_std_5_05_2 = np.std(kmeans_centroids_5_05_2, axis=0)\n",
    "\n",
    "# Save the UMAP projections and KMeans centroids\n",
    "np.save(f'umap_projections_neighbors_{n_neighbors}_{min_dist}_{n_components}.npy', np.array(umap_projections_5_05_2))\n",
    "np.save(f'kmeans_centroids_neighbors_{n_neighbors}_{min_dist}_{n_components}.npy', np.array(kmeans_centroids_list_5_05_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the UMAP projections\n",
    "umap_projections_5_05_2 = np.load(f'umap_projections_neighbors_5_0.5_2.npy')\n",
    "\n",
    "# To see the contents of the UMAP projections\n",
    "print(umap_projections_5_05_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "\n",
    "# Save the centroid_mean and centroid_std\n",
    "np.save(f'centroid_mean_{n_neighbors}_05_2.npy', np.array(centroid_mean_5_05_2))\n",
    "np.save(f'centroid_std_{n_neighbors}_05_2.npy', np.array(centroid_mean_5_05_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_mean_5_05_2= np.load(f'centroid_mean_5_05_2.npy')\n",
    "centroid_std_5_05_2= np.load(f'centroid_std_5_05_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_centroids_5_05_2 = np.load(f\"kmeans_centroids_neighbors_5_0.5_2.npy\")  # Load the saved centroids data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_5_05_2 = np.zeros(10)\n",
    "std_dev_y_5_05_2 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_5_05_2[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_5_05_2[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_5_05_2[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_5_05_2[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\", std_dev_x_5_05_2)\n",
    "print(\"Standard deviation of y coordinates per cluster:\", std_dev_y_5_05_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Create an empty list to hold the data for the DataFrame\n",
    "data_5_05_2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_5_05_2[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_5_05_2[cluster]\n",
    "        lower_bound_x, upper_bound_x = mean_x - 2 * std_dev_x_5_05_2[cluster], mean_x + 2 * std_dev_x_5_05_2[cluster]\n",
    "        lower_bound_y, upper_bound_y = mean_y - 2 * std_dev_y_5_05_2[cluster], mean_y + 2 * std_dev_y_5_05_2[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std = (lower_bound_x <= centroid_coord[0] <= upper_bound_x) and (lower_bound_y <= centroid_coord[1] <= upper_bound_y)\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_5_05_2.append([trial + 1, cluster, centroid_coord, inside_2_std])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_5_05_2 = pd.DataFrame(data_5_05_2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true_5_05_2 = df_results_5_05_2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true_5_05_2 = trials_all_true_5_05_2[trials_all_true_5_05_2].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true_5_05_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false_5_05_2 = trials_all_true_5_05_2[~trials_all_true_5_05_2].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false_5_05_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "# Save the result table to a CSV file\n",
    "df_results_5_05_2.to_csv(f'result_table_neighbors_v2_{5}_05_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BELOW To edit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array into a DataFrame with 'Cluster', 'x_mean', and 'y_mean'\n",
    "centroid_mean_5_05_2_df = pd.DataFrame(centroid_mean_5_05_2, columns=['x_mean', 'y_mean'])\n",
    "centroid_mean_5_05_2_df['Cluster'] = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Add commas between numbers in 'Centroid Coord' entries if they are missing\n",
    "df_results_5_05_2['Centroid Coord'] = df_results_5_05_2['Centroid Coord'].str.replace(\n",
    "    r'(\\-?\\d+\\.\\d+)\\s+(\\-?\\d+\\.\\d+)', r'\\1, \\2', regex=True\n",
    ")\n",
    "\n",
    "# Step 2: Convert 'Centroid Coord' from string to list\n",
    "df_results_5_05_2['Centroid Coord'] = df_results_5_05_2['Centroid Coord'].apply(ast.literal_eval)\n",
    "\n",
    "# Step 3: Verify if each entry in 'Centroid Coord' is a list of length 2\n",
    "invalid_rows = df_results_5_05_2[df_results_5_05_2['Centroid Coord'].apply(lambda x: not (isinstance(x, list) and len(x) == 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y coordinates\n",
    "df_results_5_05_2[['x', 'y']] = pd.DataFrame(df_results_5_05_2['Centroid Coord'].tolist(), index=df_results_5_05_2.index)\n",
    "\n",
    "# Merge the mean centroids dataframe with the results dataframe on 'Cluster'\n",
    "df_merged_5_05_2 = pd.merge(df_results_5_05_2, centroid_mean_5_05_2_df, on='Cluster', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance from each centroid to its cluster's mean\n",
    "df_merged_5_05_2['Distance_to_Mean'] = np.sqrt((df_merged_5_05_2['x'] - df_merged_5_05_2['x_mean'])**2 + (df_merged_5_05_2['y'] - df_merged_5_05_2['y_mean'])**2)\n",
    "\n",
    "# Apply an outlier threshold (e.g., 90th percentile of the distance per cluster)\n",
    "def filter_outliers(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return df[df['Distance_to_Mean'] <= threshold]\n",
    "\n",
    "# Apply the filtering function for each cluster\n",
    "df_no_outliers_5_05_2 = df_merged_5_05_2.groupby('Cluster').apply(filter_outliers).reset_index(drop=True)\n",
    "\n",
    "# Step 7: Drop unnecessary columns if needed (like 'x' and 'y' if only the distance matters)\n",
    "df_no_outliers_cleaned_5_05_2 = df_no_outliers_5_05_2.drop(columns=['x', 'y', 'x_mean', 'y_mean'])\n",
    "\n",
    "# Step 8: Check the size of the resulting dataframe\n",
    "print(f\"Original DataFrame size: {df_merged_5_05_2.shape}\")\n",
    "print(f\"DataFrame size after removing outliers: {df_no_outliers_cleaned_5_05_2.shape}\")\n",
    "\n",
    "# Display the final dataframe to the user\n",
    "df_no_outliers_cleaned_5_05_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by 'Cluster'\n",
    "clusters_grouped_5_05_2 = df_no_outliers_cleaned_5_05_2.groupby('Cluster')\n",
    "\n",
    "# Create a dictionary to store arrays for each cluster's centroids\n",
    "clusters_centroids_5_05_2 = {}\n",
    "\n",
    "# Loop through each group (cluster) and store the centroids in arrays\n",
    "for cluster, group in clusters_grouped_5_05_2:\n",
    "    # Extract centroids (x, y) as a NumPy array\n",
    "    centroids_array = np.array(group['Centroid Coord'].tolist())  # Assuming 'Centroid Coord' contains [x, y] pairs\n",
    "    clusters_centroids_5_05_2[cluster] = centroids_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the size of each cluster\n",
    "cluster_sizes_5_05_2 = {cluster: len(centroids) for cluster, centroids in clusters_centroids_5_05_2.items()}\n",
    "\n",
    "# Print the size of each cluster\n",
    "for cluster, size in cluster_sizes_5_05_2.items():\n",
    "    print(f\"Cluster {cluster} has {size} centroids considered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to verify that if it is fine to have all clusters with the same number of centroids after filtering out outliers. This must be due to:\n",
    "- The Distance Distributions are Likely Very Similar\n",
    "- Uniform Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each cluster and plot the distribution of distances\n",
    "for cluster, group in clusters_grouped_5_05_2:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(group['Distance_to_Mean'], bins=10, edgecolor='black')\n",
    "    plt.title(f'Cluster {cluster}: Distance to Mean Distribution')\n",
    "    plt.xlabel('Distance to Mean')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentile threshold per cluster check\n",
    "def check_percentiles(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return threshold\n",
    "\n",
    "# Function applied to each cluster and print the result\n",
    "for cluster, group in clusters_grouped_5_05_2:\n",
    "    threshold = check_percentiles(group)\n",
    "    print(f\"Cluster {cluster}: 90th percentile threshold = {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster, calculate the 70th percentile of distances and filter accordingly\n",
    "for cluster, group in clusters_grouped_5_05_2:\n",
    "    # Calculate the 70th percentile threshold for the current cluster\n",
    "    threshold = np.percentile(group['Distance_to_Mean'], 70)\n",
    "    \n",
    "    # Filter centroids based on the 70th percentile\n",
    "    filtered_group = group[group['Distance_to_Mean'] <= threshold]\n",
    "    \n",
    "    # Print the size of the group before and after filtering\n",
    "    print(f\"Cluster {cluster}: Original size = {len(group)}, Filtered size = {len(filtered_group)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance matrix n=5, min_dist= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Mean matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance matrix**: elemnt d_{ij} has the distance between the center of cluster i and cluster j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store distance matrices for each run\n",
    "distance_matrices_5_05_2 = []\n",
    "\n",
    "# Iterate over all runs and calculate the distance matrix for each run\n",
    "for run_centroids in kmeans_centroids_5_05_2:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    dist_matrix = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    distance_matrices_5_05_2.append(dist_matrix)\n",
    "\n",
    "# Convert the list of distance matrices to a numpy array (35 runs, 10x10 distance matrices)\n",
    "distance_matrices_5_05_2 = np.array(distance_matrices_5_05_2)\n",
    "\n",
    "# Calculate the mean distance matrix across all runs\n",
    "mean_distance_matrix_5_05_2 = np.mean(distance_matrices_5_05_2, axis=0)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_5_05_2 = (mean_distance_matrix_5_05_2 - np.min(mean_distance_matrix_5_05_2)) / (np.max(mean_distance_matrix_20_35) - np.min(mean_distance_matrix_5_05_2))\n",
    "\n",
    "# Plot of the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_5_05_2, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=5, min_dist = 0.5)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_5_05_2_all_runs.npy', distance_matrices_5_05_2)\n",
    "np.save('mean_distance_matrix_neighbors_5_05_2.npy', mean_distance_matrix_5_05_2)\n",
    "np.save('normalized_mean_distance_matrix_5_05_2', normalized_mean_distance_matrix_5_05_2)\n",
    "\n",
    "# Mean distance matrix\n",
    "print(f\"Mean distance matrix across all runs:\\n{mean_distance_matrix_5_05_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MST Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_5_05_2 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_5_05_2,3))\n",
    "np.save('G_5_05_2.npy',G_5_05_2)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_5_05_2, seed=42)  # positions for all nodes\n",
    "nx.draw(G_5_05_2, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_5_05_2, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_5_05_2, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_5_05_2 = nx.minimum_spanning_tree(G_5_05_2)\n",
    "np.save('mst_5_05_2.npy', mst_5_05_2)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_5_05_2, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_5_05_2, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_5_05_2, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_5_05_2, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST - min_dist= 0.5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Std. dev. Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the pairwise distance matrix for the standard deviations\n",
    "distance_matrix_std_5_05_2 = cdist(centroid_std_5_05_2, centroid_std_5_05_2, metric='euclidean')\n",
    "\n",
    "# Normalize the distance matrix\n",
    "normalized_distance_matrix_std_5_05_2 = (distance_matrix_std_5_05_2 - np.min(distance_matrix_std_5_05_2)) / (np.max(distance_matrix_std_5_05_2) - np.min(distance_matrix_std_5_05_2))\n",
    "\n",
    "# Visualize the normalized distance matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(normalized_distance_matrix_std_5_05_2, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Distance Matrix for Centroid Std Deviations (min_dist= 0.5)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrix for later analysis\n",
    "np.save(\"distance_matrix_std_5_05_2.npy\", distance_matrix_std_5_05_2)\n",
    "np.save(\"normalized_distance_matrix_std_5_05_2.npy\", normalized_distance_matrix_std_5_05_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_std_5_05_2 = nx.from_numpy_array(np.round(normalized_distance_matrix_std_5_05_2,3))\n",
    "np.save('G_std_5_05_2.npy',G_std_5_05_2)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_std_5_05_2, seed=42)  # positions for all nodes\n",
    "nx.draw(G_std_5_05_2, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_std_5_05_2, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_std_5_05_2, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_std_5_05_2 = nx.minimum_spanning_tree(G_std_5_05_2)\n",
    "np.save('mst_std_5_05_2.npy',mst_std_5_05_2)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos_std_5_05_2 = nx.spring_layout(mst_std_5_05_2, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_std_5_05_2, pos_std_5_05_2, with_labels=True, node_color='lightyellow', edge_color='green', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels_std_5_05_2 = nx.get_edge_attributes(mst_std_5_05_2, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_5_05_2, pos_std_5_05_2, edge_labels=edge_labels_std_5_05_2, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST Std. Deviation - min_dist= 0.5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create heatmaps\n",
    "def plot_heatmaps_side_by_side(matrices, titles, figsize=(16, 8), cmap=\"viridis\", annot=True):\n",
    "    \"\"\"\n",
    "    Plots multiple heatmaps side by side for given matrices and titles.\n",
    "\n",
    "    Args:\n",
    "        matrices (list): List of 2D matrices to plot as heatmaps.\n",
    "        titles (list): List of titles corresponding to each matrix.\n",
    "        figsize (tuple): Size of the entire figure (default: (16, 8)).\n",
    "        cmap (str): Color map to use for all heatmaps (default: \"viridis\").\n",
    "        annot (bool): Whether to annotate cells with their values (default: True).\n",
    "    \"\"\"\n",
    "    n = len(matrices)  # Number of heatmaps\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "\n",
    "    for i, (matrix, title) in enumerate(zip(matrices, titles)):\n",
    "        sns.heatmap(matrix, annot=annot, cmap=cmap, fmt=\".2f\", linewidths=0.5, ax=axes[i])\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_xlabel(\"Cluster\")\n",
    "        axes[i].set_ylabel(\"Cluster\" if i == 0 else \"\")  # Only label y-axis for the first plot\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the two heatmaps\n",
    "plot_heatmaps_side_by_side(\n",
    "    matrices=[\n",
    "        normalized_distance_matrix_std_5_05_2,\n",
    "        normalized_mean_distance_matrix_5_05_2\n",
    "    ],\n",
    "    titles=[\n",
    "        \"Normalized Distance Matrix for Centroid Std Deviations (min_dist=0.5)\",\n",
    "        \"Normalized Mean Distance Matrix (k=10, min_dist=0.5)\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for \"low\" values\n",
    "threshold = 0.4\n",
    "\n",
    "# Identify pairs of clusters with low values in both matrices\n",
    "low_low_pairs = []\n",
    "for i in range(normalized_mean_distance_matrix_5_05_2.shape[0]):\n",
    "    for j in range(normalized_mean_distance_matrix_5_05_2.shape[1]):\n",
    "        if i != j:  # Skip diagonal\n",
    "            mean_value = normalized_mean_distance_matrix_5_05_2[i, j]\n",
    "            std_value = normalized_distance_matrix_std_5_05_2[i, j]\n",
    "            if mean_value < threshold and std_value < threshold:\n",
    "                low_low_pairs.append((i, j, mean_value, std_value))\n",
    "\n",
    "# Display the results\n",
    "for pair in low_low_pairs:\n",
    "    print(f\"Clusters {pair[0]} and {pair[1]}: Mean Distance = {pair[2]:.2f}, Std Distance = {pair[3]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0.4 lowest threshold so far. The limit is in 0.35 but there is only one matching**\n",
    "\n",
    "Depending on the goal of the analysis we can think of it as:\n",
    "- If the aim is to identify the strongest relationships between clusters, a lower threshold would make more sense.\n",
    "- If we want to explore the broader connections, then it is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "\n",
    "# Dynamic radius, previously calculated\n",
    "radius = dynamic_radius_5_05_2\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_5_05_2_d = []\n",
    "neighbor_counts_5_05_2_d = []\n",
    "kmeans_labels_list_5_05_2_d = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_5_05_2):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_5_05_2_d = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_5_05_2_d.append(kmeans_labels_5_05_2_d)\n",
    "    \n",
    "    run_max_distances_5_05_2_d = []\n",
    "    run_neighbor_counts_5_05_2_d = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_5_05_2_d == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_5_05_2_d = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_5_05_2_d.append(max_distance_5_05_2_d)\n",
    "        \n",
    "        # Calculate number of neighbors within the dynamic radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_5_05_2_d = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_5_05_2_d.append(neighbors_within_radius_5_05_2_d)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_5_05_2_d.append(run_max_distances_5_05_2_d)\n",
    "    neighbor_counts_5_05_2_d.append(run_neighbor_counts_5_05_2_d)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_5_05_2_d = np.array(max_distances_5_05_2_d)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_5_05_2_d = np.array(neighbor_counts_5_05_2_d)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_5_05_2_d = np.array(kmeans_labels_list_5_05_2_d)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_dynamic_5_05_2.npy', max_distances_5_05_2_d)\n",
    "np.save('neighbor_counts_within_dynamic_radius_5_05_2.npy', neighbor_counts_5_05_2_d)\n",
    "np.save('kmeans_labels_list_5_05_2.npy', kmeans_labels_list_5_05_2_d)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_5_05_2_d)\n",
    "print(\"\\nNeighbor counts within dynamic radius for each run and each cluster:\\n\", neighbor_counts_5_05_2_d)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_distances_5_05_2_d= np.load(f'max_intra_cluster_distances_dynamic_5_05_2.npy')\n",
    "neighbor_counts_5_05_2_d= np.load(f'neighbor_counts_within_dynamic_radius_5_05_2.npy')\n",
    "kmeans_labels_list_5_05_2_d= np.load(f'kmeans_labels_list_5_05_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Replace with your cluster pairs from low_low_pairs\n",
    "low_low_pairs = [(0, 8, 0.4, 0.21), (0, 7, 0.3, 0.27), (0, 6, 0.4, 0.2)]\n",
    "\n",
    "# UMAP projections and cluster labels (replace with your actual data)\n",
    "umap_projections = np.load(\"umap_projections_neighbors_5_0.5_2.npy\")\n",
    "kmeans_labels = np.load(\"kmeans_labels_list_5_05_2.npy\")  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Function to plot clusters\n",
    "def plot_clusters(umap_projection, labels, cluster_pair, run_idx):\n",
    "    cluster_a, cluster_b = cluster_pair\n",
    "    points_a = umap_projection[labels == cluster_a]\n",
    "    points_b = umap_projection[labels == cluster_b]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(points_a[:, 0], points_a[:, 1], color=\"blue\", label=f\"Cluster {cluster_a}\", alpha=0.6)\n",
    "    plt.scatter(points_b[:, 0], points_b[:, 1], color=\"orange\", label=f\"Cluster {cluster_b}\", alpha=0.6)\n",
    "    plt.title(f\"Run {run_idx}: Cluster {cluster_a} vs. Cluster {cluster_b}\")\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze each cluster pair\n",
    "for cluster_pair in low_low_pairs:\n",
    "    cluster_a, cluster_b = cluster_pair[0], cluster_pair[1]\n",
    "    print(f\"Analyzing Cluster Pair: {cluster_a} and {cluster_b}\")\n",
    "    \n",
    "    # For simplicity, visualize them in a specific UMAP run (e.g., the first run)\n",
    "    run_idx = 0  # Use the first run for visualization\n",
    "    plot_clusters(umap_projections[run_idx], kmeans_labels[run_idx], (cluster_a, cluster_b), run_idx)\n",
    "\n",
    "    # Calculate additional statistics if needed\n",
    "    distances_a_to_b = np.linalg.norm(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a].mean(axis=0) - \n",
    "                                      umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b].mean(axis=0))\n",
    "    print(f\"Mean Centroid Distance (Run {run_idx}): {distances_a_to_b:.2f}\")\n",
    "\n",
    "    # Variability comparison\n",
    "    cluster_a_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a], axis=0)\n",
    "    cluster_b_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b], axis=0)\n",
    "    print(f\"Cluster {cluster_a} Std Dev: {cluster_a_std}\")\n",
    "    print(f\"Cluster {cluster_b} Std Dev: {cluster_b_std}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 1 and Cluster 8** have a moderate spatial relationship with visible overlap in the UMAP space. Their differing variability patterns suggest distinct structures, but the overlap points might represent shared features or transitions between the clusters.\n",
    "The large spatial separation between their centroids suggests they represent distinct structures or classes in the data.\n",
    "\n",
    "**Cluster 0 and Cluster 9** 9 appears more compact and stable, while Cluster 0 is larger and more variable.\n",
    "Their distinct regions in the UMAP space and differing standard deviations reinforce their meaningful separation.\n",
    "Insights from Variability:\n",
    "\n",
    "The variability of Cluster 0 could indicate sensitivity to UMAP parameters or noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intra class evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to track the minimum distance and corresponding clusters\n",
    "overall_min_distance_5_05_2 = float('inf')\n",
    "min_distance_clusters_5_05_2 = None\n",
    "min_distance_run_idx_5_05_2= None\n",
    "\n",
    "for run_idx, run_centroids in enumerate(kmeans_centroids_5_05_2):\n",
    "    # Compute pairwise distances between centroids\n",
    "    pairwise_distances_5_05_2 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    \n",
    "    # Get the indices of the minimum non-zero distance\n",
    "    np.fill_diagonal(pairwise_distances_5_05_2, np.inf)  # Ignore zero distances (self-comparisons)\n",
    "    min_distance = np.min(pairwise_distances_5_05_2)\n",
    "    if min_distance < overall_min_distance_5_05_2:\n",
    "        overall_min_distance_5_05_2 = min_distance\n",
    "        # Find the indices of the clusters corresponding to the minimum distance\n",
    "        cluster_indices = np.unravel_index(np.argmin(pairwise_distances_5_05_2), pairwise_distances_5_05_2.shape)\n",
    "        min_distance_clusters_5_05_2 = cluster_indices\n",
    "        min_distance_run_idx_5_05_2 = run_idx\n",
    "\n",
    "# Calculate dynamic radius\n",
    "dynamic_radius_5_05_2 = overall_min_distance_5_05_2 / 2\n",
    "print(f\"Dynamic radius: {dynamic_radius_5_05_2}\")\n",
    "print(f\"Minimum distance: {overall_min_distance_5_05_2}\")\n",
    "print(f\"Clusters contributing to minimum distance: {min_distance_clusters_5_05_2}\")\n",
    "print(f\"Run index: {min_distance_run_idx_5_05_2}\")\n",
    "\n",
    "# Save dynamic radius\n",
    "np.save('dynamic_radius_results_5_05_2.npy', dynamic_radius_5_05_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot neighbor counts for each cluster across all runs\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Iterate through each cluster\n",
    "for cluster_idx in range(neighbor_counts_5_05_2_d.shape[1]):\n",
    "    plt.subplot(2, 5, cluster_idx + 1)  # Create subplots for 10 clusters (2 rows, 5 columns)\n",
    "    plt.plot(range(1, neighbor_counts_5_05_2_d.shape[0] + 1), neighbor_counts_5_05_2_d[:, cluster_idx], marker='o')\n",
    "    plt.title(f'Cluster {cluster_idx}')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Neighbor Count')\n",
    "    plt.xticks(range(1, neighbor_counts_5_05_2_d.shape[0] + 1, 5))  # Show every 5th run on the x-axis for clarity\n",
    "    plt.grid(True)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.suptitle('min_dist=0.5 Neighbor Counts per Cluster Across Runs', y=1.02, fontsize=16)  # Add a global title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and max values across clusters for each run\n",
    "mean_neighbors_5_05_2 = np.mean(neighbor_counts_5_05_2_d, axis=1)  # Shape: (n_runs,)\n",
    "max_neighbors_5_05_2 = np.max(neighbor_counts_5_05_2_d, axis=1)    # Shape: (n_runs,)\n",
    "\n",
    "# Compute trend lines for mean and max\n",
    "runs = np.arange(1, len(mean_neighbors_5_05_2) + 1)\n",
    "mean_slope, mean_intercept, _, _, _ = linregress(runs, mean_neighbors_5_05_2)\n",
    "max_slope, max_intercept, _, _, _ = linregress(runs, max_neighbors_5_05_2)\n",
    "\n",
    "# Calculate trend line values\n",
    "mean_trend_5_05_2 = mean_slope * runs + mean_intercept\n",
    "max_trend_5_05_2 = max_slope * runs + max_intercept\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Mean neighbor counts\n",
    "plt.plot(runs, mean_neighbors_5_05_2, label='Mean Neighbor Count', marker='o', color='blue')\n",
    "\n",
    "# Max neighbor counts\n",
    "plt.plot(runs, max_neighbors_5_05_2, label='Max Neighbor Count', marker='s', color='orange')\n",
    "\n",
    "# Trend lines\n",
    "plt.plot(runs, mean_trend_5_05_2, linestyle='--', color='green',label='Mean Trend Line')\n",
    "plt.plot(runs, max_trend_5_05_2, linestyle='--', color='green', label='Max Trend Line')\n",
    "\n",
    "# Add labels, legend, and title\n",
    "plt.title('min_dist=0.5 Neighbor Counts Across Runs (Mean vs. Max with Trend Lines)', fontsize=16)\n",
    "plt.xlabel('Run', fontsize=12)\n",
    "plt.ylabel('Neighbor Count', fontsize=12)\n",
    "plt.xticks(range(1, len(mean_neighbors_5_05_2) + 1, 5))  # Show every 5th run for readability\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(f'neighbor_counts_plot_n_5_05_2.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_dist= 0, n=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_projections_10_0_2 = np.load(f'umap_projections_n_neighbors_10_min_dist_0_n_comp_2.npy')\n",
    "centroid_mean_10_0_2= np.load(f'centroid_mean_n_neighbors_10_min_dist_0_n_comp_2.npy')\n",
    "centroid_std_10_0_2= np.load(f'centroid_std_n_neighbors_10_min_dist_0_n_comp_2.npy')\n",
    "kmeans_centroids_10_0_2= np.load(f'kmeans_centroids_n_neighbors_10_min_dist_0_n_comp_2.npy')\n",
    "df_results_10_0_2=pd.read_csv(f'df_results_10_0_2.csv')\n",
    "# mean_distance_matrix_5_05_2= np.load(f'mean_distance_matrix_neighbors_5_05_2.npy')\n",
    "# normalized_mean_distance_matrix_5_05_2= np.load(f'normalized_mean_distance_matrix_5_05_2.npy')\n",
    "# distance_matrix_std_5_05_2= np.load(f\"distance_matrix_std_5_05_2.npy\")\n",
    "# normalized_distance_matrix_std_5_05_2= np.load(f\"normalized_distance_matrix_std_5_05_2.npy\")\n",
    "# mst_std_5_05_2= np.load(f'mst_std_5_05_2.npy')\n",
    "# mst_5_05_2= np.load(f'mst_5_05_2.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the experiment\n",
    "n_runs = 35\n",
    "n_clusters = 10  # Number of clusters for KMeans\n",
    "min_dist = 0\n",
    "n_neighbors = 10\n",
    "n_components = 2\n",
    "\n",
    "# Initialize lists to store results\n",
    "umap_projections_list = []  # Store UMAP projections for each run\n",
    "kmeans_centroids_list = []  # Store KMeans centroids for each run\n",
    "\n",
    "# Define a reusable function to calculate cluster centroids\n",
    "def calculate_centroids(labels, x_umap, n_clusters):\n",
    "    \"\"\"Calculate centroids of clusters.\"\"\"\n",
    "    centroids = [np.mean(x_umap[labels == i], axis=0) for i in range(n_clusters)]\n",
    "    return np.array(centroids)\n",
    "\n",
    "# Perform UMAP and KMeans clustering for each run\n",
    "for run in range(n_runs):\n",
    "    # UMAP projection\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=None)\n",
    "    x_umap = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    \n",
    "    # Calculate centroids\n",
    "    centroids = calculate_centroids(kmeans.labels_, x_umap, n_clusters)\n",
    "    \n",
    "    # Store results\n",
    "    umap_projections_list.append(x_umap)\n",
    "    kmeans_centroids_list.append(centroids)\n",
    "\n",
    "# Convert results to numpy arrays for further analysis\n",
    "umap_projections_array = np.array(umap_projections_list)  # Shape: (n_runs, n_samples, 2)\n",
    "kmeans_centroids_array = np.array(kmeans_centroids_list)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "# Compute mean and standard deviation for centroids\n",
    "centroid_mean_array = np.mean(kmeans_centroids_array, axis=0)  # Shape: (n_clusters, 2)\n",
    "centroid_std_array = np.std(kmeans_centroids_array, axis=0)    # Shape: (n_clusters, 2)\n",
    "\n",
    "# Save results with descriptive filenames\n",
    "np.save(f'umap_projections_n_neighbors_{n_neighbors}_min_dist_{min_dist}_n_comp_{n_components}.npy', umap_projections_array)\n",
    "np.save(f'kmeans_centroids_n_neighbors_{n_neighbors}_min_dist_{min_dist}_n_comp_{n_components}.npy', kmeans_centroids_array)\n",
    "np.save(f'centroid_mean_n_neighbors_{n_neighbors}_min_dist_{min_dist}_n_comp_{n_components}.npy', centroid_mean_array)\n",
    "np.save(f'centroid_std_n_neighbors_{n_neighbors}_min_dist_{min_dist}_n_comp_{n_components}.npy', centroid_std_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the UMAP projections\n",
    "# umap_projections_10_0_2 = np.load(f'umap_projections_n_neighbors_10_min_dist_0_n_comp_2.npy')\n",
    "\n",
    "# # To see the contents of the UMAP projections\n",
    "# print(umap_projections_10_0_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO NEED TO RE RUN ###\n",
    "\n",
    "# Save the centroid_mean and centroid_std\n",
    "# np.save(f'centroid_mean_10_0_2.npy', np.array(centroid_mean_10_0_2))\n",
    "# np.save(f'centroid_std_10_0_2.npy', np.array(centroid_mean_10_0_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans_centroids_10_0_2 = np.load(f\"kmeans_centroids_neighbors_10_0_2.npy\")  # Load the saved centroids data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store standard deviations\n",
    "std_dev_x_neighbors_10_min_dist_0_n_components_2 = np.zeros(10)\n",
    "std_dev_y_neighbors_10_min_dist_0_n_components_2 = np.zeros(10)\n",
    "\n",
    "# Loop through each cluster to calculate std deviation for x and y coordinates\n",
    "for i in range(10):\n",
    "    # Extract all x and y coordinates for the i-th cluster over all runs\n",
    "    cluster_x_coords = kmeans_centroids_10_0_2[:, i, 0]  # All x coords for cluster i\n",
    "    cluster_y_coords = kmeans_centroids_10_0_2[:, i, 1]  # All y coords for cluster i\n",
    "    \n",
    "    # Calculate standard deviation in x and y\n",
    "    std_dev_x_neighbors_10_min_dist_0_n_components_2[i] = np.std(cluster_x_coords)\n",
    "    std_dev_y_neighbors_10_min_dist_0_n_components_2[i] = np.std(cluster_y_coords)\n",
    "\n",
    "# Output the results\n",
    "print(\"Standard deviation of x coordinates per cluster:\",\n",
    "      std_dev_x_neighbors_10_min_dist_0_n_components_2)\n",
    "print(\"Standard deviation of y coordinates per cluster:\",\n",
    "      std_dev_y_neighbors_10_min_dist_0_n_components_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold the data for the DataFrame \n",
    "data_neighbors_10_min_dist_0_n_components_2 = []\n",
    "\n",
    "# Loop through each trial and each cluster to evaluate the condition\n",
    "for trial in range(35):\n",
    "    for cluster in range(10):\n",
    "        # Extract the centroid coordinates for the current trial and cluster\n",
    "        centroid_coord = kmeans_centroids_10_0_2[trial, cluster]\n",
    "        \n",
    "        # Calculate the bounds for the 2 standard deviations range for x and y\n",
    "        mean_x, mean_y = centroid_mean_10_0_2[cluster]\n",
    "        lower_bound_x = mean_x - 2 * std_dev_x_neighbors_10_min_dist_0_n_components_2[cluster]\n",
    "        upper_bound_x = mean_x + 2 * std_dev_x_neighbors_10_min_dist_0_n_components_2[cluster]\n",
    "        lower_bound_y = mean_y - 2 * std_dev_y_neighbors_10_min_dist_0_n_components_2[cluster]\n",
    "        upper_bound_y = mean_y + 2 * std_dev_y_neighbors_10_min_dist_0_n_components_2[cluster]\n",
    "        \n",
    "        # Check if the centroid is inside the 2 std range\n",
    "        inside_2_std_neighbors_10_min_dist_0_n_components_2 = ((lower_bound_x <= centroid_coord[0] <= upper_bound_x) and(lower_bound_y <= centroid_coord[1] <= upper_bound_y)        )\n",
    "        \n",
    "        # Append the data as a new row in the list\n",
    "        data_neighbors_10_min_dist_0_n_components_2.append([trial + 1, cluster, centroid_coord, inside_2_std_neighbors_10_min_dist_0_n_components_2])\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "df_results_neighbors_10_min_dist_0_n_components_2 = pd.DataFrame(data_neighbors_10_min_dist_0_n_components_2, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside 2 std'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by Trial and check if all clusters in each trial are True for 'Inside 2 std'\n",
    "trials_all_true_10_0_2 = df_results_neighbors_10_min_dist_0_n_components_2.groupby('Trial')['Inside 2 std'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where all clusters were True\n",
    "trials_with_all_true_10_0_2 = trials_all_true_10_0_2[trials_all_true_10_0_2].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the list of trials\n",
    "print(\"Trials where all clusters were True:\", trials_with_all_true_10_0_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the trials where not all clusters were True\n",
    "trials_with_some_false_10_0_2 = trials_all_true_10_0_2[~trials_all_true_10_0_2].index.tolist()\n",
    "\n",
    "# Output the list of trials where some clusters were False\n",
    "print(\"Trials where some clusters were False:\", trials_with_some_false_10_0_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result table to a CSV file\n",
    "df_results_neighbors_10_min_dist_0_n_components_2.to_csv(f'df_results_10_0_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal outliers process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array into a DataFrame with 'Cluster', 'x_mean', and 'y_mean'\n",
    "centroid_mean_10_0_2_df = pd.DataFrame(centroid_mean_10_0_2, columns=['x_mean', 'y_mean'])\n",
    "centroid_mean_10_0_2_df['Cluster'] = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_mean_10_0_2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y coordinates\n",
    "df_results_10_0_2[['x', 'y']] = pd.DataFrame(df_results_10_0_2['Centroid Coord'].tolist(), index=df_results_10_0_2.index)\n",
    "\n",
    "# Merge the mean centroids dataframe with the results dataframe on 'Cluster'\n",
    "df_merged = pd.merge(df_results_10_0_2, centroid_mean_10_0_2_df, on='Cluster', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot changes in X-coordinate for each cluster over all runs\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_10[:, cluster, 0], marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Cluster {cluster} X-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('X Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot changes in Y-coordinate for each cluster over all runs\n",
    "for cluster in range(n_clusters):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_runs + 1), kmeans_centroids_10[:, cluster, 1], marker='o', linestyle='-', color='g')\n",
    "    plt.title(f'Cluster {cluster} Y-coordinate Change Across Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Y Centroid Coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance from each centroid to its cluster's mean\n",
    "df_merged['Distance_to_Mean'] = np.sqrt((df_merged['x'] - df_merged['x_mean'])**2 + (df_merged['y'] - df_merged['y_mean'])**2)\n",
    "\n",
    "# Apply an outlier threshold (e.g., 90th percentile of the distance per cluster)\n",
    "def filter_outliers(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return df[df['Distance_to_Mean'] <= threshold]\n",
    "\n",
    "# Apply the filtering function for each cluster\n",
    "df_no_outliers = df_merged.groupby('Cluster').apply(filter_outliers).reset_index(drop=True)\n",
    "\n",
    "# Step 7: Drop unnecessary columns if needed (like 'x' and 'y' if only the distance matters)\n",
    "df_no_outliers_cleaned_10_0_2 = df_no_outliers.drop(columns=['x', 'y', 'x_mean', 'y_mean'])\n",
    "\n",
    "# Step 8: Check the size of the resulting dataframe\n",
    "print(f\"Original DataFrame size: {df_merged.shape}\")\n",
    "print(f\"DataFrame size after removing outliers: {df_no_outliers_cleaned.shape}\")\n",
    "\n",
    "# Display the final dataframe\n",
    "df_no_outliers_cleaned_10_0_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by 'Cluster'\n",
    "clusters_grouped_10_0_2 = df_no_outliers_cleaned_10_0_2.groupby('Cluster')\n",
    "\n",
    "# Create a dictionary to store arrays for each cluster's centroids\n",
    "clusters_centroids_10_0_2 = {}\n",
    "\n",
    "# Loop through each group (cluster) and store the centroids in arrays\n",
    "for cluster, group in clusters_grouped_10_0_2:\n",
    "    # Extract centroids (x, y) as a NumPy array\n",
    "    centroids_array = np.array(group['Centroid Coord'].tolist())  # Assuming 'Centroid Coord' contains [x, y] pairs\n",
    "    clusters_centroids_10_0_2[cluster] = centroids_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the size of each cluster\n",
    "cluster_sizes_10_0_2 = {cluster: len(centroids) for cluster, centroids in clusters_centroids_10_0_2.items()}\n",
    "\n",
    "# Print the size of each cluster\n",
    "for cluster, size in cluster_sizes_10_0_2.items():\n",
    "    print(f\"Cluster {cluster} has {size} centroids considered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to verify that if it is fine to have all clusters with the same number of centroids after filtering out outliers. This must be due to:\n",
    "- The Distance Distributions are Likely Very Similar\n",
    "- Uniform Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each cluster and plot the distribution of distances\n",
    "for cluster, group in clusters_grouped:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(group['Distance_to_Mean'], bins=10, edgecolor='black')\n",
    "    plt.title(f'Cluster {cluster}: Distance to Mean Distribution')\n",
    "    plt.xlabel('Distance to Mean')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentile threshold per cluster check\n",
    "def check_percentiles(df):\n",
    "    threshold = np.percentile(df['Distance_to_Mean'], 90)\n",
    "    return threshold\n",
    "\n",
    "# Function applied to each cluster and print the result\n",
    "for cluster, group in clusters_grouped:\n",
    "    threshold = check_percentiles(group)\n",
    "    print(f\"Cluster {cluster}: 90th percentile threshold = {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster, calculate the 70th percentile of distances and filter accordingly\n",
    "for cluster, group in clusters_grouped:\n",
    "    # Calculate the 70th percentile threshold for the current cluster\n",
    "    threshold = np.percentile(group['Distance_to_Mean'], 70)\n",
    "    \n",
    "    # Filter centroids based on the 70th percentile\n",
    "    filtered_group = group[group['Distance_to_Mean'] <= threshold]\n",
    "    \n",
    "    # Print the size of the group before and after filtering\n",
    "    print(f\"Cluster {cluster}: Original size = {len(group)}, Filtered size = {len(filtered_group)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance matrix n=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Mean Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance matrix**: elemnt d_{ij} has the distance between the center of cluster i and cluster j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store distance matrices for each run\n",
    "distance_matrices_10_0_2 = []\n",
    "\n",
    "# Iterate over all runs and calculate the distance matrix for each run\n",
    "for run_centroids in kmeans_centroids_10:\n",
    "    # Calculate the pairwise Euclidean distance between centroids for this run\n",
    "    distance_matrix_10_0_2 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    distance_matrices_10_0_2.append(distance_matrix_10_0_2)\n",
    "\n",
    "# Convert the list of distance matrices to a numpy array (35 runs, 10x10 distance matrices)\n",
    "distance_matrices_10_0_2 = np.array(distance_matrices_10_0_2)\n",
    "\n",
    "# Calculate the mean distance matrix across all runs\n",
    "mean_distance_matrix_10_0_2 = np.mean(distance_matrices_10_0_2, axis=0)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_10_0_2 = (mean_distance_matrix_10_0_2 - np.min(mean_distance_matrix_10_0_2)) / (np.max(mean_distance_matrix_10_0_2) - np.min(mean_distance_matrix_5_35))\n",
    "\n",
    "# Plot of the normalized mean distance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_mean_distance_matrix_10_0_2, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Mean Distance Matrix (k=10, n_neighbors=10)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrices and mean distance matrix\n",
    "np.save('distance_matrices_neighbors_10_all_runs.npy', distance_matrices_10_0_2)\n",
    "np.save('mean_distance_matrix_neighbors_10_0_2.npy', mean_distance_matrix_10_0_2)\n",
    "\n",
    "# Mean distance matrix\n",
    "print(f\"Mean distance matrix across all runs:\\n{mean_distance_matrix_10_0_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_distance_matrix_10_0_2= np.load(f'mean_distance_matrix_neighbors_10_0_2.npy')\n",
    "# mean_distance_matrix_10_0_2=np.round(mean_distance_matrix_10_0_2,3)\n",
    "\n",
    "# Normalize the mean distance matrix\n",
    "normalized_mean_distance_matrix_10_0_2 = (mean_distance_matrix_10_0_2 - np.min(mean_distance_matrix_10_0_2)) / (np.max(mean_distance_matrix_10_0_2) - np.min(mean_distance_matrix_10_0_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_10_0_2 = nx.from_numpy_array(np.round(normalized_mean_distance_matrix_10_0_2,3))\n",
    "np.save('G_10_0_2.npy',G_10_0_2)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_10_0_2, seed=42)  # positions for all nodes\n",
    "nx.draw(G_10_0_2, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_10_0_2, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_10_0_2, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum spanning tree\n",
    "\n",
    "# Compute the minimum spanning tree of the graph\n",
    "mst_10_0_2 = nx.minimum_spanning_tree(G_10_0_2)\n",
    "np.save('mst_10_0_2.npy',mst_10_0_2)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos = nx.spring_layout(mst_10_0_2, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_10_0_2, pos, with_labels=True, node_color='lightblue', edge_color='red', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels = nx.get_edge_attributes(mst_10_0_2, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_10_0_2, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST - n_neighbors=10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance Std. dev. Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the pairwise distance matrix for the standard deviations\n",
    "distance_matrix_std_10_0_2 = cdist(centroid_std_10_0_2, centroid_std_10_0_2, metric='euclidean')\n",
    "\n",
    "# Normalize the distance matrix\n",
    "normalized_distance_matrix_std_10_0_2 = (distance_matrix_std_10_0_2 - np.min(distance_matrix_std_10_0_2)) / (np.max(distance_matrix_std_10_0_2) - np.min(distance_matrix_std_10_0_2))\n",
    "\n",
    "# Visualize the normalized distance matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(normalized_distance_matrix_std_10_0_2, annot=True, cmap=\"viridis\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=10)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the distance matrix for later analysis\n",
    "np.save(\"distance_matrix_std_10_0_2.npy\", distance_matrix_std_10_0_2)\n",
    "np.save(\"normalized_distance_matrix_std_10_0_2.npy\", normalized_distance_matrix_std_10_0_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the distance matrix\n",
    "G_std_10_0_2 = nx.from_numpy_array(np.round(normalized_distance_matrix_std_10_0_2,3))\n",
    "np.save('G_std_10_0_2.npy', G_std_10_0_2)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G_std_10_0_2, seed=42)  # positions for all nodes\n",
    "nx.draw(G_std_10_0_2, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=800, font_size=10)\n",
    "\n",
    "# Draw edge labels (distances)\n",
    "edge_labels = nx.get_edge_attributes(G_std_10_0_2, 'weight')\n",
    "nx.draw_networkx_edge_labels(G_std_10_0_2, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum spanning tree of the graph\n",
    "mst_std_10_0_2 = nx.minimum_spanning_tree(G_std_10_0_2)\n",
    "np.save('mst_std_10_0_2.npy', mst_std_10_0_2)\n",
    "\n",
    "# Define positions for all nodes\n",
    "pos_std_10_0_2 = nx.spring_layout(mst_std_10_0_2, seed=42)\n",
    "\n",
    "# Draw the minimum spanning tree only\n",
    "nx.draw(mst_std_10_0_2, pos_std_10_0_2, with_labels=True, node_color='lightyellow', edge_color='green', node_size=500, font_size=10, width=2)\n",
    "\n",
    "# Draw edge labels (distances) for the MST\n",
    "edge_labels_std_10_0_2 = nx.get_edge_attributes(mst_std_10_0_2, 'weight')\n",
    "nx.draw_networkx_edge_labels(mst_std_10_0_2, pos_std_10_0_2, edge_labels=edge_labels_std_10_0_2, font_size=8, label_pos=0.3)\n",
    "\n",
    "plt.title(\"MST Std. Deviation - n_neighbors=10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(matrix, title, xlabel, ylabel, figsize=(10, 8), cmap=\"viridis\", annot=True):\n",
    "    \"\"\"\n",
    "    Plots a heatmap for a given matrix with customizable parameters.\n",
    "\n",
    "    Args:\n",
    "        matrix (ndarray): The 2D matrix to plot as a heatmap.\n",
    "        title (str): Title of the heatmap.\n",
    "        xlabel (str): Label for the x-axis.\n",
    "        ylabel (str): Label for the y-axis.\n",
    "        figsize (tuple): Size of the figure (default: (10, 8)).\n",
    "        cmap (str): Color map to use (default: \"viridis\").\n",
    "        annot (bool): Whether to annotate cells with their values (default: True).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(matrix, annot=annot, cmap=cmap, fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function for both heatmaps\n",
    "plot_heatmap(\n",
    "    normalized_distance_matrix_std_10_0_2,\n",
    "    title=\"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=10)\",\n",
    "    xlabel=\"Cluster\",\n",
    "    ylabel=\"Cluster\",\n",
    "    figsize=(8, 6)\n",
    ")\n",
    "\n",
    "plot_heatmap(\n",
    "    normalized_mean_distance_matrix_10_0_2,\n",
    "    title=\"Normalized Mean Distance Matrix (k=10, n_neighbors=10)\",\n",
    "    xlabel=\"Cluster\",\n",
    "    ylabel=\"Cluster\",\n",
    "    figsize=(8, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create heatmaps\n",
    "def plot_heatmaps_side_by_side(matrices, titles, figsize=(16, 8), cmap=\"viridis\", annot=True):\n",
    "    \"\"\"\n",
    "    Plots multiple heatmaps side by side for given matrices and titles.\n",
    "\n",
    "    Args:\n",
    "        matrices (list): List of 2D matrices to plot as heatmaps.\n",
    "        titles (list): List of titles corresponding to each matrix.\n",
    "        figsize (tuple): Size of the entire figure (default: (16, 8)).\n",
    "        cmap (str): Color map to use for all heatmaps (default: \"viridis\").\n",
    "        annot (bool): Whether to annotate cells with their values (default: True).\n",
    "    \"\"\"\n",
    "    n = len(matrices)  # Number of heatmaps\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "\n",
    "    for i, (matrix, title) in enumerate(zip(matrices, titles)):\n",
    "        sns.heatmap(matrix, annot=annot, cmap=cmap, fmt=\".2f\", linewidths=0.5, ax=axes[i])\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_xlabel(\"Cluster\")\n",
    "        axes[i].set_ylabel(\"Cluster\" if i == 0 else \"\")  # Only label y-axis for the first plot\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the two heatmaps\n",
    "plot_heatmaps_side_by_side(\n",
    "    matrices=[\n",
    "        normalized_distance_matrix_std_10_0_2,\n",
    "        normalized_mean_distance_matrix_10_0_2\n",
    "    ],\n",
    "    titles=[\n",
    "        \"Normalized Distance Matrix for Centroid Std Deviations (n_neighbors=5)\",\n",
    "        \"Normalized Mean Distance Matrix (k=10, n_neighbors=5)\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for \"low\" values\n",
    "threshold = 0.65\n",
    "\n",
    "# Identify pairs of clusters with low values in both matrices\n",
    "low_low_pairs = []\n",
    "for i in range(normalized_mean_distance_matrix_10_0_2.shape[0]):\n",
    "    for j in range(normalized_mean_distance_matrix_10_0_2.shape[1]):\n",
    "        if i != j:  # Skip diagonal\n",
    "            mean_value = normalized_mean_distance_matrix_10_0_2[i, j]\n",
    "            std_value = normalized_distance_matrix_std_10_0_2[i, j]\n",
    "            if mean_value < threshold and std_value < threshold:\n",
    "                low_low_pairs.append((i, j, mean_value, std_value))\n",
    "\n",
    "# Display the results\n",
    "for pair in low_low_pairs:\n",
    "    print(f\"Clusters {pair[0]} and {pair[1]}: Mean Distance = {pair[2]:.2f}, Std Distance = {pair[3]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.65 can seem like a high value, since it is on the upper-mid range.\n",
    "\n",
    "Depending on the goal of the analysis we can think of it as:\n",
    "- If the aim is to identify the strongest relationships between clusters, a lower threshold would make more sense.\n",
    "- If we want to explore the broader connections, then it is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Replace with your cluster pairs from low_low_pairs\n",
    "low_low_pairs = [(0, 9, 0.64, 0.18), (0, 6, 0.64, 0.23),(7, 9, 0.62, 0.26)]  # Example cluster pairs\n",
    "\n",
    "# UMAP projections and cluster labels (replace with your actual data)\n",
    "umap_projections = np.load(\"umap_projections_neighbors_10.npy\")\n",
    "kmeans_labels = np.load(\"kmeans_labels_list_10_0_2.npy\")  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Function to plot clusters\n",
    "def plot_clusters(umap_projection, labels, cluster_pair, run_idx):\n",
    "    cluster_a, cluster_b = cluster_pair\n",
    "    points_a = umap_projection[labels == cluster_a]\n",
    "    points_b = umap_projection[labels == cluster_b]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(points_a[:, 0], points_a[:, 1], color=\"blue\", label=f\"Cluster {cluster_a}\", alpha=0.6)\n",
    "    plt.scatter(points_b[:, 0], points_b[:, 1], color=\"orange\", label=f\"Cluster {cluster_b}\", alpha=0.6)\n",
    "    plt.title(f\"Run {run_idx}: Cluster {cluster_a} vs. Cluster {cluster_b}\")\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze each cluster pair\n",
    "for cluster_pair in low_low_pairs:\n",
    "    cluster_a, cluster_b = cluster_pair[0], cluster_pair[1]\n",
    "    print(f\"Analyzing Cluster Pair: {cluster_a} and {cluster_b}\")\n",
    "    \n",
    "    # For simplicity, visualize them in a specific UMAP run (e.g., the first run)\n",
    "    run_idx = 0  # Use the first run for visualization\n",
    "    plot_clusters(umap_projections[run_idx], kmeans_labels[run_idx], (cluster_a, cluster_b), run_idx)\n",
    "\n",
    "    # Calculate additional statistics if needed\n",
    "    distances_a_to_b = np.linalg.norm(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a].mean(axis=0) - \n",
    "                                      umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b].mean(axis=0))\n",
    "    print(f\"Mean Centroid Distance (Run {run_idx}): {distances_a_to_b:.2f}\")\n",
    "\n",
    "    # Variability comparison\n",
    "    cluster_a_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_a], axis=0)\n",
    "    cluster_b_std = np.std(umap_projections[run_idx][kmeans_labels[run_idx] == cluster_b], axis=0)\n",
    "    print(f\"Cluster {cluster_a} Std Dev: {cluster_a_std}\")\n",
    "    print(f\"Cluster {cluster_b} Std Dev: {cluster_b_std}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 1 and Cluster 8** have a moderate spatial relationship with visible overlap in the UMAP space. Their differing variability patterns suggest distinct structures, but the overlap points might represent shared features or transitions between the clusters.\n",
    "The large spatial separation between their centroids suggests they represent distinct structures or classes in the data.\n",
    "\n",
    "**Cluster 0 and Cluster 9** 9 appears more compact and stable, while Cluster 0 is larger and more variable.\n",
    "Their distinct regions in the UMAP space and differing standard deviations reinforce their meaningful separation.\n",
    "Insights from Variability:\n",
    "\n",
    "The variability of Cluster 0 could indicate sensitivity to UMAP parameters or noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_mnist_overlap(umap_projection, kmeans_labels, mnist_labels, cluster_pair, run_idx):\n",
    "    cluster_a, cluster_b = cluster_pair\n",
    "\n",
    "    # Get points in Cluster A and Cluster B\n",
    "    points_a_indices = np.where(kmeans_labels == cluster_a)[0]\n",
    "    points_b_indices = np.where(kmeans_labels == cluster_b)[0]\n",
    "\n",
    "    # Find the overlapping points (indices)\n",
    "    overlap_indices = np.intersect1d(points_a_indices, points_b_indices)\n",
    "\n",
    "    # Get the original labels of overlapping points\n",
    "    overlap_labels = np.array(mnist_labels)[overlap_indices]\n",
    "\n",
    "    # Analyze the original labels\n",
    "    overlap_label_counts = pd.Series(overlap_labels).value_counts()\n",
    "\n",
    "    # Display the overlap statistics\n",
    "    print(f\"Overlap between Cluster {cluster_a} and Cluster {cluster_b} (Run {run_idx}):\")\n",
    "    print(f\"Number of overlapping points: {len(overlap_indices)}\")\n",
    "    print(f\"Original label distribution of overlapping points:\\n{overlap_label_counts}\")\n",
    "\n",
    "    # Visualize the overlap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(umap_projection[points_a_indices, 0], umap_projection[points_a_indices, 1], color=\"blue\", label=f\"Cluster {cluster_a}\", alpha=0.5)\n",
    "    plt.scatter(umap_projection[points_b_indices, 0], umap_projection[points_b_indices, 1], color=\"orange\", label=f\"Cluster {cluster_b}\", alpha=0.5)\n",
    "    if len(overlap_indices) > 0:\n",
    "        plt.scatter(umap_projection[overlap_indices, 0], umap_projection[overlap_indices, 1], color=\"red\", label=\"Overlap\", alpha=0.7)\n",
    "    plt.title(f\"Cluster Overlap: Cluster {cluster_a} vs. Cluster {cluster_b} (Run {run_idx})\")\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Load your MNIST data\n",
    "dataloader = MnistDataloader(\n",
    "    training_images_filepath=\"train-images.idx3-ubyte\",\n",
    "    training_labels_filepath=\"train-labels.idx1-ubyte\",\n",
    "    test_images_filepath=\"t10k-images.idx3-ubyte\",\n",
    "    test_labels_filepath=\"t10k-labels.idx1-ubyte\"\n",
    ")\n",
    "\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = dataloader.load_data()\n",
    "\n",
    "# Flatten the training images for UMAP (if needed for alignment with projections)\n",
    "x_train_flattened = np.array([np.array(img).flatten() for img in x_train])\n",
    "\n",
    "# Example variables (replace these with your actual data)\n",
    "run_idx = 0  # Analyze the first UMAP run\n",
    "cluster_pair = (1, 8)  # Compare Cluster 1 and Cluster 8\n",
    "umap_projection = umap_projections[run_idx]  # UMAP projection for the given run\n",
    "kmeans_labels = kmeans_labels[run_idx]  # KMeans labels for the given run\n",
    "\n",
    "# Examine overlap\n",
    "examine_mnist_overlap(umap_projection, kmeans_labels, y_train, cluster_pair, run_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intra class evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "radius = 0.5\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_10_0_2 = []\n",
    "neighbor_counts_10_0_2 = []\n",
    "kmeans_labels_list_10_0_2 = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_10):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_10_0_2 = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_10_0_2.append(kmeans_labels_10_0_2)\n",
    "    \n",
    "    run_max_distances_10_0_2 = []\n",
    "    run_neighbor_counts_10_0_2 = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_10_0_2 == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_10_0_2 = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_10_0_2.append(max_distance_10_0_2)\n",
    "        \n",
    "        # Calculate number of neighbors within the radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_10_0_2 = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_10_0_2.append(neighbors_within_radius_10_0_2)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_10_0_2.append(run_max_distances_10_0_2)\n",
    "    neighbor_counts_10_0_2.append(run_neighbor_counts_10_0_2)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_10_0_2 = np.array(max_distances_10_0_2)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_10_0_2 = np.array(neighbor_counts_10_0_2)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_10_0_2 = np.array(kmeans_labels_list_10_0_2)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_10_0_2.npy', max_distances_10_0_2)\n",
    "np.save('neighbor_counts_within_radius_10_0_2.npy', neighbor_counts_10_0_2)\n",
    "np.save('kmeans_labels_list_10_0_2 .npy', kmeans_labels_list_10_0_2)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_10_0_2)\n",
    "print(\"\\nNeighbor counts within radius for each run and each cluster:\\n\", neighbor_counts_10_0_2)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_distances_10_0_2_d= np.load(f'max_intra_cluster_distances_dynamic_10_0_2.npy')\n",
    "neighbor_counts_10_0_2_d= np.load(f'neighbor_counts_within_dynamic_radius_10_0_2.npy')\n",
    "kmeans_labels_list_10_0_2_d= np.load(f'kmeans_labels_list_10_0_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to track the minimum distance and corresponding clusters\n",
    "overall_min_distance_10 = float('inf')\n",
    "min_distance_clusters_10 = None\n",
    "min_distance_run_idx_10= None\n",
    "\n",
    "for run_idx, run_centroids in enumerate(kmeans_centroids_10):\n",
    "    # Compute pairwise distances between centroids\n",
    "    pairwise_distances_10 = cdist(run_centroids, run_centroids, metric='euclidean')\n",
    "    \n",
    "    # Get the indices of the minimum non-zero distance\n",
    "    np.fill_diagonal(pairwise_distances_10, np.inf)  # Ignore zero distances (self-comparisons)\n",
    "    min_distance = np.min(pairwise_distances_10)\n",
    "    if min_distance < overall_min_distance_10:\n",
    "        overall_min_distance_10 = min_distance\n",
    "        # Find the indices of the clusters corresponding to the minimum distance\n",
    "        cluster_indices = np.unravel_index(np.argmin(pairwise_distances_10), pairwise_distances_10.shape)\n",
    "        min_distance_clusters_10 = cluster_indices\n",
    "        min_distance_run_idx_10 = run_idx\n",
    "\n",
    "# Calculate dynamic radius\n",
    "dynamic_radius_10_0_2 = overall_min_distance_10 / 2\n",
    "print(f\"Dynamic radius: {dynamic_radius_10_0_2}\")\n",
    "print(f\"Minimum distance: {overall_min_distance_10}\")\n",
    "print(f\"Clusters contributing to minimum distance: {min_distance_clusters_10}\")\n",
    "print(f\"Run index: {min_distance_run_idx_10}\")\n",
    "\n",
    "# Save dynamic radius\n",
    "np.save('dynamic_radius_results_10_0_2.npy', dynamic_radius_10_0_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_clusters = 10\n",
    "\n",
    "# Dynamic radius, previously calculated\n",
    "radius = dynamic_radius_10_0_2\n",
    "\n",
    "# Lists to store max distances, neighbor counts, and KMeans labels for each cluster in each run\n",
    "max_distances_10_0_2_d = []\n",
    "neighbor_counts_10_0_2_d = []\n",
    "kmeans_labels_list_10_0_2_d = []\n",
    "\n",
    "# Calculate intra-cluster metrics for each run and each cluster\n",
    "for run_idx, x_umap in enumerate(umap_projections_10):\n",
    "    # Re-run KMeans to get labels for each projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_umap)\n",
    "    kmeans_labels_10_0_2_d = kmeans.labels_\n",
    "    \n",
    "    # Store the labels for this run\n",
    "    kmeans_labels_list_10_0_2_d.append(kmeans_labels_10_0_2_d)\n",
    "    \n",
    "    run_max_distances_10_0_2_d = []\n",
    "    run_neighbor_counts_10_0_2_d = []\n",
    "    \n",
    "    # For each cluster, calculate max intra-cluster distance and neighbor count around centroid\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get all points in the current cluster\n",
    "        cluster_points = x_umap[kmeans_labels_10_0_2_d == cluster_idx]\n",
    "        \n",
    "        # Calculate pairwise distances within the cluster\n",
    "        intra_distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        \n",
    "        # Max distance within the cluster\n",
    "        max_distance_10_0_2_d = np.max(intra_distances) if len(cluster_points) > 1 else 0\n",
    "        run_max_distances_10_0_2_d.append(max_distance_10_0_2_d)\n",
    "        \n",
    "        # Calculate number of neighbors within the dynamic radius around the centroid\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        distances_to_centroid = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "        neighbors_within_radius_10_0_2_d = np.sum(distances_to_centroid <= radius)\n",
    "        \n",
    "        run_neighbor_counts_10_0_2_d.append(neighbors_within_radius_10_0_2_d)\n",
    "    \n",
    "    # Append results for this run\n",
    "    max_distances_10_0_2_d.append(run_max_distances_10_0_2_d)\n",
    "    neighbor_counts_10_0_2_d.append(run_neighbor_counts_10_0_2_d)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis if needed\n",
    "max_distances_10_0_2_d = np.array(max_distances_10_0_2_d)  # Shape: (n_runs, n_clusters)\n",
    "neighbor_counts_10_0_2_d = np.array(neighbor_counts_10_0_2_d)  # Shape: (n_runs, n_clusters)\n",
    "kmeans_labels_list_10_0_2_d = np.array(kmeans_labels_list_10_0_2_d)  # Shape: (n_runs, n_samples)\n",
    "\n",
    "# Save the max distances, neighbor counts, and KMeans labels arrays\n",
    "np.save('max_intra_cluster_distances_dynamic_10_0_2.npy', max_distances_10_0_2_d)\n",
    "np.save('neighbor_counts_within_dynamic_radius_10_0_2.npy', neighbor_counts_10_0_2_d)\n",
    "np.save('kmeans_labels_list_10_0_2.npy', kmeans_labels_list_10_0_2_d)\n",
    "\n",
    "# Output the results\n",
    "print(\"Max intra-cluster distances for each run and each cluster:\\n\", max_distances_10_0_2_d)\n",
    "print(\"\\nNeighbor counts within dynamic radius for each run and each cluster:\\n\", neighbor_counts_10_0_2_d)\n",
    "print(\"\\nKMeans labels saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot neighbor counts for each cluster across all runs\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Iterate through each cluster\n",
    "for cluster_idx in range(neighbor_counts_10_0_2_d.shape[1]):\n",
    "    plt.subplot(2, 5, cluster_idx + 1)  # Create subplots for 10 clusters (2 rows, 5 columns)\n",
    "    plt.plot(range(1, neighbor_counts_10_0_2_d.shape[0] + 1), neighbor_counts_10_0_2_d[:, cluster_idx], marker='o')\n",
    "    plt.title(f'Cluster {cluster_idx}')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Neighbor Count')\n",
    "    plt.xticks(range(1, neighbor_counts_10_0_2_d.shape[0] + 1, 5))  # Show every 5th run on the x-axis for clarity\n",
    "    plt.grid(True)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.suptitle('N=10 Neighbor Counts per Cluster Across Runs', y=1.02, fontsize=16)  # Add a global title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and max values across clusters for each run\n",
    "mean_neighbors_10 = np.mean(neighbor_counts_10_0_2_d, axis=1)  # Shape: (n_runs,)\n",
    "max_neighbors_10 = np.max(neighbor_counts_10_0_2_d, axis=1)    # Shape: (n_runs,)\n",
    "\n",
    "# Compute trend lines for mean and max\n",
    "runs = np.arange(1, len(mean_neighbors_10) + 1)\n",
    "mean_slope, mean_intercept, _, _, _ = linregress(runs, mean_neighbors_10)\n",
    "max_slope, max_intercept, _, _, _ = linregress(runs, max_neighbors_10)\n",
    "\n",
    "# Calculate trend line values\n",
    "mean_trend_10 = mean_slope * runs + mean_intercept\n",
    "max_trend_10 = max_slope * runs + max_intercept\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Mean neighbor counts\n",
    "plt.plot(runs, mean_neighbors_10, label='Mean Neighbor Count', marker='o', color='blue')\n",
    "\n",
    "# Max neighbor counts\n",
    "plt.plot(runs, max_neighbors_10, label='Max Neighbor Count', marker='s', color='orange')\n",
    "\n",
    "# Trend lines\n",
    "plt.plot(runs, mean_trend_10, linestyle='--', color='green',label='Mean Trend Line')\n",
    "plt.plot(runs, max_trend_10, linestyle='--', color='green', label='Max Trend Line')\n",
    "\n",
    "# Add labels, legend, and title\n",
    "plt.title('N=10 Neighbor Counts Across Runs (Mean vs. Max with Trend Lines)', fontsize=16)\n",
    "plt.xlabel('Run', fontsize=12)\n",
    "plt.ylabel('Neighbor Count', fontsize=12)\n",
    "plt.xticks(range(1, len(mean_neighbors_10) + 1, 5))  # Show every 5th run for readability\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(f'neighbor_counts_plot_n_10_0_2.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epochs Trials\n",
    "\n",
    "- n_epochs=200: Faster but potentially less precise.\n",
    "- n_epochs=500: Close to default, balanced performance (used for previous analysises)\n",
    "- n_epochs=1000: Slower but potentially more precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_epochs= 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of UMAP and KMeans runs\n",
    "n_runs = 35\n",
    "n_clusters = 10  # Set the number of clusters (for KMeans)\n",
    "min_dist = 0.1\n",
    "n_neighbors = 5\n",
    "n_components = 2\n",
    "n_epochs = 200\n",
    "\n",
    "# Store UMAP and KMeans results for each run\n",
    "umap_projections_5_01_2_200 = []\n",
    "kmeans_centroids_list_5_01_2_200 = []  # Use this to store centroids for each run\n",
    "\n",
    "# Define a helper function to calculate the centroid of each cluster\n",
    "def calculate_centroids(kmeans, x_umap):\n",
    "    centroids_5_01_2_200 = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = x_umap[kmeans.labels_ == i]\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        centroids_5_01_2_200.append(centroid)\n",
    "    return np.array(centroids_5_01_2_200)\n",
    "\n",
    "# Run UMAP and KMeans multiple times\n",
    "for run in range(n_runs):\n",
    "    # Apply UMAP with the same parameters for each run\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, n_epochs=n_epochs, random_state=None)  # No random_state to allow randomness, use random_state=None\n",
    "    x_train_umap_5_01_2_200 = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Apply KMeans clustering on the UMAP projection\n",
    "    kmeans_5_01_2_200 = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans_5_01_2_200.fit(x_train_umap_5_01_2_200)\n",
    "\n",
    "    # Calculate centroids for this run\n",
    "    centroids_5_01_2_200 = calculate_centroids(kmeans, x_train_umap_5_01_2_200)\n",
    "    \n",
    "    # Store the UMAP projections and KMeans models\n",
    "    umap_projections.append(x_train_umap_5_01_2_200)\n",
    "    kmeans_centroids_list_5_01_2_200.append(centroids_5_01_2_200)\n",
    "\n",
    "# Now we calculate the mean and standard deviation of the centroids across all runs\n",
    "kmeans_centroids_5_01_2_200 = np.array(kmeans_centroids_list_5_01_2_200)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "# Calculate mean and std deviation for centroids' coordinates\n",
    "centroid_mean_5_01_2_200 = np.mean(kmeans_centroids_5_01_2_200, axis=0)\n",
    "centroid_std_5_01_2_200 = np.std(kmeans_centroids_5_01_2_200, axis=0)\n",
    "\n",
    "# Save the UMAP projections and KMeans centroids\n",
    "np.save(f'umap_projections_neighbors_{n_neighbors}_{min_dist}_{n_components}_{n_epochs}.npy', np.array(umap_projections_5_01_2_200))\n",
    "np.save(f'kmeans_centroids_neighbors_{n_neighbors}_{min_dist}_{n_components}{n_epochs}.npy', np.array(kmeans_centroids_list_5_01_2_200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table to store the results: Trial | Cluster | Centroid Coord | Inside the 90%\n",
    "result_table_200epochs = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    for i in range(n_clusters):\n",
    "        centroid = kmeans_centroids[run][i]\n",
    "        mean = centroid_mean[i]\n",
    "        std = centroid_std[i]\n",
    "        inside_90 = is_within_90_percent(centroid, mean, std)\n",
    "        result_table_200epochs.append([run + 1, i, centroid, inside_90])\n",
    "\n",
    "# Convert result_table to a DataFrame for better readability\n",
    "df_results_200epochs = pd.DataFrame(result_table_200epochs, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside the 90%'])\n",
    "\n",
    "# Display the DataFrame using standard Pandas functions\n",
    "print(df_results_200epochs)\n",
    "\n",
    "# Print mean and standard deviation for clarity\n",
    "print(\"Centroid Means (per cluster):\\n\", centroid_mean)\n",
    "print(\"Centroid Standard Deviations (per cluster):\\n\", centroid_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_epochs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of UMAP and KMeans runs\n",
    "n_runs = 10\n",
    "n_clusters = 10  # Set the number of clusters (for KMeans)\n",
    "\n",
    "# Store UMAP and KMeans results for each run\n",
    "umap_projections = []\n",
    "kmeans_centroids_list = []  # Use this to store centroids for each run\n",
    "\n",
    "# Define a helper function to calculate the centroid of each cluster\n",
    "def calculate_centroids(kmeans, x_umap):\n",
    "    centroids = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = x_umap[kmeans.labels_ == i]\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        centroids.append(centroid)\n",
    "    return np.array(centroids)\n",
    "\n",
    "# Run UMAP and KMeans multiple times\n",
    "for run in range(n_runs):\n",
    "    # Apply UMAP with the same parameters for each run\n",
    "    umap_model = umap.UMAP(n_neighbors=5, min_dist=0.1, n_components=2, n_epochs=1000, random_state=None)  # No random_state to allow randomness, use random_state=None\n",
    "    x_train_umap = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Apply KMeans clustering on the UMAP projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_train_umap)\n",
    "\n",
    "    # Calculate centroids for this run\n",
    "    centroids = calculate_centroids(kmeans, x_train_umap)\n",
    "    \n",
    "    # Store the UMAP projections and KMeans models\n",
    "    umap_projections.append(x_train_umap)\n",
    "    kmeans_centroids_list.append(centroids)\n",
    "\n",
    "# Now we calculate the mean and standard deviation of the centroids across all runs\n",
    "kmeans_centroids = np.array(kmeans_centroids_list)  # Shape: (n_runs, n_clusters, 2)\n",
    "\n",
    "# Calculate mean and std deviation for centroids' coordinates\n",
    "centroid_mean = np.mean(kmeans_centroids, axis=0)\n",
    "centroid_std = np.std(kmeans_centroids, axis=0)\n",
    "\n",
    "# Define a function to check if a centroid is within the 90% range\n",
    "def is_within_90_percent(centroid, mean, std):\n",
    "    lower_bound = mean - 1.645 * std  # 90% interval lower bound\n",
    "    upper_bound = mean + 1.645 * std  # 90% interval upper bound\n",
    "    return np.all((centroid >= lower_bound) & (centroid <= upper_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table to store the results: Trial | Cluster | Centroid Coord | Inside the 90%\n",
    "result_table_1000epochs = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    for i in range(n_clusters):\n",
    "        centroid = kmeans_centroids[run][i]\n",
    "        mean = centroid_mean[i]\n",
    "        std = centroid_std[i]\n",
    "        inside_90 = is_within_90_percent(centroid, mean, std)\n",
    "        result_table_1000epochs.append([run + 1, i, centroid, inside_90])\n",
    "\n",
    "# Convert result_table to a DataFrame for better readability\n",
    "df_results_1000epochs = pd.DataFrame(result_table_1000epochs, columns=['Trial', 'Cluster', 'Centroid Coord', 'Inside the 90%'])\n",
    "\n",
    "# Display the DataFrame using standard Pandas functions\n",
    "print(df_results_1000epochs)\n",
    "\n",
    "# Print mean and standard deviation for clarity\n",
    "print(\"Centroid Means (per cluster):\\n\", centroid_mean)\n",
    "print(\"Centroid Standard Deviations (per cluster):\\n\", centroid_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the runs that have at least one centroid outside the 90% range\n",
    "runs_to_remove = df_results_1000epochs.loc[~df_results_1000epochs['Inside the 90%'], 'Trial'].unique()\n",
    "\n",
    "# Filter out the identified runs\n",
    "df_filtered_results = df_results_1000epochs[~df_results_1000epochs['Trial'].isin(runs_to_remove)]\n",
    "\n",
    "# Step 3: Continue your analysis with the remaining runs\n",
    "print(f\"Runs removed: {runs_to_remove}\")\n",
    "print(f\"Remaining runs after filtering: {df_filtered_results['Trial'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhoute score analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of how well UMAP clusters the digits. The silhouette score measures how similar each point is to its own cluster compared to other clusters. \n",
    "Higher values indicate better clustering.\n",
    " - Different 'n_neighbors' and 'min_dist' can maximize the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Silhouette score analysis\n",
    "# umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "# x_train_umap = umap_model.fit_transform(x_train_flattened)\n",
    "\n",
    "# # Calculate silhouette score (labels should be the ground truth digit labels)\n",
    "# score = silhouette_score(x_train_umap, y_train)\n",
    "# print(f'Silhouette Score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP - PCA - TSNE Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply PCA\n",
    "# pca_model = PCA(n_components=2)\n",
    "# x_train_pca = pca_model.fit_transform(x_train_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply t-SNE\n",
    "# tsne_model = TSNE(n_components=2, random_state=42)\n",
    "# x_train_tsne = tsne_model.fit_transform(x_train_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot PCA\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.scatter(x_train_pca[:, 0], x_train_pca[:, 1], c=y_train, cmap='Spectral', s=0.1)\n",
    "# plt.title('PCA projection of MNIST data')\n",
    "\n",
    "# # Plot t-SNE\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.scatter(x_train_tsne[:, 0], x_train_tsne[:, 1], c=y_train, cmap='Spectral', s=0.1)\n",
    "# plt.title('t-SNE projection of MNIST data')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA silhouette score (labels should be the ground truth digit labels)\n",
    "# score = silhouette_score(x_train_pca, y_train)\n",
    "# print(f'Silhouette Score: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TSNE silhouette score (labels should be the ground truth digit labels)\n",
    "# score = silhouette_score(x_train_tsne, y_train)\n",
    "# print(f'Silhouette Score: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Silhouette score analysis\n",
    "# umap_model = umap.UMAP(n_neighbors=15, min_dist=0.2, n_components=2, random_state=42)\n",
    "# x_train_umap = umap_model.fit_transform(x_train_flattened)\n",
    "\n",
    "# # Calculate silhouette score (labels should be the ground truth digit labels)\n",
    "# score = silhouette_score(x_train_umap, y_train)\n",
    "# print(f'Silhouette Score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans: Cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (digits in MNIST)\n",
    "n_clusters = 10\n",
    "\n",
    "# KMeans clustering on the UMAP projection\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(x_train_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the centroids of each cluster\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Assign clusters to each point in UMAP space\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the spread (standard deviation) of points around the centroid for each cluster\n",
    "cluster_spreads = []\n",
    "for cluster_idx in range(n_clusters):\n",
    "    # Get the points in the current cluster\n",
    "    cluster_points = x_train_umap[cluster_labels == cluster_idx]\n",
    "    \n",
    "    # Calculate the spread (standard deviation of x and y)\n",
    "    spread_x = np.std(cluster_points[:, 0])\n",
    "    spread_y = np.std(cluster_points[:, 1])\n",
    "    \n",
    "    cluster_spreads.append({\n",
    "        'cluster': cluster_idx,\n",
    "        'spread_x': spread_x,\n",
    "        'spread_y': spread_y\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame for easier visualization\n",
    "spread_df = pd.DataFrame(cluster_spreads)\n",
    "\n",
    "# Display centroids and spreads\n",
    "print(\"Cluster centroids:\\n\", centroids)\n",
    "print(\"\\nCluster spreads:\\n\", spread_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "# Calculate the pairwise distances between centroids\n",
    "distances = cdist(centroids, centroids, metric='euclidean')\n",
    "\n",
    "distances_rounded = np.round(distances, 3)\n",
    "\n",
    "# # Convert the distance matrix to a DataFrame for easier visualization\n",
    "distance_df = pd.DataFrame(distances_rounded, columns=[f'Centroid {i}' for i in range(n_clusters)])\n",
    "\n",
    "# Save Centroid Distances in npy format.\n",
    "np.save('centroid_distance_matrix.npy', distances_rounded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Display DataFrame using tabulate\n",
    "print(tabulate(distance_df, headers='keys', tablefmt='pretty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming x_train_umap is your UMAP projection and kmeans is the KMeans model\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot the UMAP-projected points and color them based on the cluster label from KMeans\n",
    "plt.scatter(x_train_umap[:, 0], x_train_umap[:, 1], c=kmeans.labels_, cmap='Spectral', s=0.1, alpha=0.5)\n",
    "\n",
    "# Plot the centroids of each cluster\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, marker='x', label='Centroids')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('UMAP Clusters with Centroids')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of UMAP and KMeans runs\n",
    "n_runs = 5\n",
    "#n_clusters = 10  # Set the number of clusters (for KMeans)\n",
    "\n",
    "# Store UMAP and KMeans results for each run\n",
    "umap_projections = []\n",
    "kmeans_models = []\n",
    "\n",
    "# Run UMAP and KMeans multiple times\n",
    "for run in range(n_runs):\n",
    "    # Apply UMAP with the same parameters for each run\n",
    "    umap_model = umap.UMAP(n_neighbors=5, min_dist=0.1, n_components=2, random_state=None)  # No random_state to allow randomness, use random_state=None\n",
    "    x_train_umap = umap_model.fit_transform(x_train_flattened)\n",
    "    \n",
    "    # Apply KMeans clustering on the UMAP projection\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(x_train_umap)\n",
    "    \n",
    "    # Store the UMAP projections and KMeans models\n",
    "    umap_projections.append(x_train_umap)\n",
    "    kmeans_models.append(kmeans)\n",
    "\n",
    "# Plot the results for each run\n",
    "fig, axes = plt.subplots(1, n_runs, figsize=(20, 5))\n",
    "\n",
    "for i in range(n_runs):\n",
    "    axes[i].scatter(umap_projections[i][:, 0], umap_projections[i][:, 1], c=kmeans_models[i].labels_, cmap='Spectral', s=0.1, alpha=0.5)\n",
    "    axes[i].scatter(kmeans_models[i].cluster_centers_[:, 0], kmeans_models[i].cluster_centers_[:, 1], c='red', s=200, marker='x')\n",
    "    axes[i].set_title(f'Run {i+1}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('UMAP Projections and KMeans Centroids (5 Runs)', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Dist Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### min_dist Variation\n",
    "Controls how tightly UMAP packs points together. Lower values lead to more compact clusters, while higher values spread the clusters apart.\n",
    "min_dist_values = [0.01, 0.1, 0.2]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 3))\n",
    "\n",
    "for idx, min_dist in enumerate(min_dist_values):\n",
    "    umap_model = umap.UMAP(n_neighbors=15, min_dist=min_dist, n_components=2, random_state=42)\n",
    "    x_train_umap = umap_model.fit_transform(x_train_flattened)\n",
    "    scatter = axes[idx].scatter(x_train_umap[:, 0], x_train_umap[:, 1], c=y_train, cmap='Spectral', s=0.1)\n",
    "    axes[idx].set_title(f'min_dist = {min_dist}')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "fig.colorbar(scatter, ax=axes, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "plt.suptitle('UMAP with Different min_dist Values', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procrustes analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import procrustes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the UMAP projections and KMeans centroids for a specific n_neighbors value\n",
    "umap_projections_5 = np.load(f'umap_projections_neighbors_5.npy', allow_pickle=True)\n",
    "kmeans_centroids_5 = np.load(f'kmeans_centroids_neighbors_5.npy', allow_pickle=True)\n",
    "\n",
    "umap_projections_10 = np.load(f'umap_projections_neighbors_10.npy', allow_pickle=True)\n",
    "kmeans_centroids_10 = np.load(f'kmeans_centroids_neighbors_10.npy', allow_pickle=True)\n",
    "\n",
    "umap_projections_25 = np.load(f'umap_projections_neighbors_25.npy', allow_pickle=True)\n",
    "kmeans_centroids_25 = np.load(f'kmeans_centroids_neighbors_103.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Procrustes distance\n",
    "def procrustes_analysis(embedding1, embedding2):\n",
    "    mtx1, mtx2, disparity = procrustes(embedding1, embedding2)\n",
    "    return disparity\n",
    "\n",
    "# Compare UMAP embeddings across different n_neighbors values\n",
    "disparity_5_10 = procrustes_analysis(umap_projections_5[0], umap_projections_10[0])\n",
    "disparity_10_25 = procrustes_analysis(umap_projections_10[0], umap_projections_25[0])\n",
    "disparity_5_25 = procrustes_analysis(umap_projections_5[0], umap_projections_25[0])\n",
    "\n",
    "print(f\"Procrustes distance between n_neighbors=5 and n_neighbors=10: {disparity_5_10}\")\n",
    "print(f\"Procrustes distance between n_neighbors=10 and n_neighbors=25: {disparity_10_25}\")\n",
    "print(f\"Procrustes distance between n_neighbors=5 and n_neighbors=25: {disparity_5_25}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP for Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YES ###\n",
    "\n",
    "class EmotionsDataloader(object):\n",
    "    def __init__(self, image_dir, label_file):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_file = label_file\n",
    "\n",
    "    def read_images_labels(self):  \n",
    "        train_images, train_labels = [], []\n",
    "        test_images, test_labels = [], []\n",
    "\n",
    "        # Read the label file and match labels to images\n",
    "        with open(self.label_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 2:\n",
    "                    image_name, label = parts[0], int(parts[1])\n",
    "                    aligned_image_name = f\"{image_name.split('.')[0]}_aligned.jpg\"\n",
    "                    image_path = os.path.join(self.image_dir, aligned_image_name)\n",
    "\n",
    "                    if os.path.exists(image_path):\n",
    "                        image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "                        image = image.resize((48, 48))  # Resize to 48x48\n",
    "\n",
    "                        # Check if the label belongs to train or test set\n",
    "                        if \"train\" in image_name:\n",
    "                            train_images.append(np.array(image))\n",
    "                            train_labels.append(label)\n",
    "                        elif \"test\" in image_name:\n",
    "                            test_images.append(np.array(image))\n",
    "                            test_labels.append(label)\n",
    "                    else:\n",
    "                        print(f\"Image not found: {aligned_image_name}\")\n",
    "\n",
    "        print(f\"Loaded {len(train_images)} training images and {len(test_images)} test images.\")\n",
    "        print(f\"Loaded {len(train_labels)} training labels and {len(test_labels)} test labels.\")\n",
    "\n",
    "        return (\n",
    "            (np.array(train_images), np.array(train_labels)),\n",
    "            (np.array(test_images), np.array(test_labels))\n",
    "        )\n",
    "\n",
    "    def load_data(self):\n",
    "        return self.read_images_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YES ###\n",
    "\n",
    "# Set file paths\n",
    "input_path = 'C:/Users/Lorenzo/OneDrive/Documents/DTU/Python/2024 Fall/MSc Thesis'\n",
    "image_dir = os.path.join(input_path, 'extracted_data/Image/aligned')\n",
    "label_file = os.path.join(input_path, 'extracted_data/EmoLabel/list_patition_label.txt')\n",
    "\n",
    "# Instantiate and load the dataset\n",
    "emotions_dataloader = EmotionsDataloader(image_dir, label_file)\n",
    "(x_train, y_train), (x_test, y_test) = emotions_dataloader.load_data()\n",
    "\n",
    "# Print dataset shapes\n",
    "print(f\"Training set shape: {x_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set shape: {x_test.shape}, {y_test.shape}\")\n",
    "\n",
    "# Display some random train and test images\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images) / cols) + 1\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (image, title) in enumerate(zip(images, title_texts)):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        plt.title(title, fontsize=12)\n",
    "        plt.axis('off')\n",
    "\n",
    "# Show some random train and test images\n",
    "images_to_show = []\n",
    "titles_to_show = []\n",
    "\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(0, len(x_train))\n",
    "    images_to_show.append(x_train[idx])\n",
    "    titles_to_show.append(f\"Train[{idx}] = {y_train[idx]}\")\n",
    "\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(0, len(x_test))\n",
    "    images_to_show.append(x_test[idx])\n",
    "    titles_to_show.append(f\"Test[{idx}] = {y_test[idx]}\")\n",
    "\n",
    "show_images(images_to_show, titles_to_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YES ###\n",
    "\n",
    "assert len(x_train) == len(y_train), \"Mismatch in training images and labels!\"\n",
    "assert len(x_test) == len(y_test), \"Mismatch in test images and labels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YES ###\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Flatten the Images into 1D Vectors\n",
    "x_train_flattened = x_train.reshape(x_train.shape[0], -1)  # Flatten to (num_samples, 2304)\n",
    "x_test_flattened = x_test.reshape(x_test.shape[0], -1)    # Flatten to (num_samples, 2304)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Shape of x_train_flattened:\", x_train_flattened.shape)  # (num_train_samples, 2304)\n",
    "print(\"Shape of x_test_flattened:\", x_test_flattened.shape)    # (num_test_samples, 2304)\n",
    "\n",
    "# Step 2: Normalize the Flattened Data\n",
    "scaler = StandardScaler()\n",
    "x_train_emotion_norm = scaler.fit_transform(x_train_flattened)\n",
    "x_test_emotion_norm = scaler.transform(x_test_flattened)\n",
    "\n",
    "# Verify normalization\n",
    "print(\"x_train_norma mean:\", x_train_emotion_norm.mean(axis=0).mean())  # ~0\n",
    "print(\"x_train_norma std:\", x_test_emotion_norm.std(axis=0).mean())    # ~1\n",
    "\n",
    "# Print final shapes\n",
    "print(\"Final shape of x_train_norm:\", x_train_emotion_norm.shape)\n",
    "print(\"Final shape of x_test_norm:\", x_test_emotion_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YES ###\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"Missing values in x_train_emotion_norm: {np.isnan(x_train_emotion_norm).sum()}\")\n",
    "print(f\"Missing values in x_test_emotion_norm: {np.isnan(x_test_emotion_norm).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion as labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YES ###\n",
    "\n",
    "# Initialize UMAP\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.01, n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform training data\n",
    "umap_emotion_train_embedding = reducer.fit_transform(x_train_emotion_norm)\n",
    "\n",
    "# Optionally, transform test data\n",
    "umap_emotion_test_embedding = reducer.transform(x_test_emotion_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"umap_15_01_emotion_test_embedding.npy\", umap_emotion_test_embedding)\n",
    "umap_emotion_test_embedding=np.load(f'umap_15_01_emotion_test_embedding.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for training data embeddings\n",
    "plt.scatter(umap_emotion_train_embedding[:, 0], umap_emotion_train_embedding[:, 1], c=y_train, cmap='Spectral', s=5)\n",
    "plt.colorbar(label=\"Emotion Label\")\n",
    "plt.title(\"UMAP n=15, m_dis=0.1 Projection of Training Data (2D)\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Silhouette Score\n",
    "sil_score = silhouette_score(umap_emotion_train_embedding, y_train)\n",
    "print(f\"Silhouette Score: {sil_score:.2f}\")\n",
    "\n",
    "# Calculate Davies-Bouldin Index (lower is better)\n",
    "db_score = davies_bouldin_score(umap_emotion_train_embedding, y_train)\n",
    "print(f\"Davies-Bouldin Index: {db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot comparing training and test embeddings\n",
    "plt.scatter(umap_emotion_train_embedding[:, 0], umap_emotion_train_embedding[:, 1], c=y_train_emotion, cmap='Spectral', s=5, alpha=0.7, label=\"Train\")\n",
    "plt.scatter(umap_emotion_test_embedding[:, 0], umap_emotion_test_embedding[:, 1], c=y_test_emotion, cmap='Spectral', s=20, edgecolor='k', label=\"Test\")\n",
    "plt.legend()\n",
    "plt.title(\"UMAP Projection: Train vs Test\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender as labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize image data\n",
    "x_train_gender_norm = x_train_gender / 255.0\n",
    "x_test_gender_norm = x_test_gender / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(f\"Missing values in x_train_gender_norm: {np.isnan(x_train_gender_norm).sum()}\")\n",
    "print(f\"Missing values in x_test_gender_norm: {np.isnan(x_test_gender_norm).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize UMAP\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.01, n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform training data\n",
    "umap_gender_train_embedding = reducer.fit_transform(x_train_gender_norm)\n",
    "\n",
    "# Optionally, transform test data\n",
    "umap_gender_test_embedding = reducer.transform(x_test_gender_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for training data embeddings\n",
    "plt.scatter(umap_gender_train_embedding[:, 0], umap_gender_train_embedding[:, 1], c=y_train_gender, cmap='Spectral', s=5)\n",
    "plt.colorbar(label=\"gender Label\")\n",
    "plt.title(\"UMAP Gender Projection of Training Data (2D)\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Silhouette Score\n",
    "sil_score = silhouette_score(umap_gender_train_embedding, y_train_gender)\n",
    "print(f\"Silhouette Score: {sil_score:.2f}\")\n",
    "\n",
    "# Calculate Davies-Bouldin Index (lower is better)\n",
    "db_score = davies_bouldin_score(umap_gender_train_embedding, y_train_gender)\n",
    "print(f\"Davies-Bouldin Index: {db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_gender.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP 10 runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "raf_unsup_umap_projections_train_10_01= np.load('raf_unsup_umap_projections_train_10_01.npy')\n",
    "raf_mean_unsup_umap_projection_train_10_01= np.load('raf_mean_unsup_umap_projection_train_10_01.npy')\n",
    "raf_std_unsup_umap_projection_train_10_01= np.load('raf_std_unsup_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "raf_unsup_umap_projections_test_10_01= np.load('raf_unsup_umap_projections_test_10_01.npy')\n",
    "raf_mean_unsup_umap_projection_test_10_01= np.load('raf_mean_unsup_umap_projection_test_10_01.npy')\n",
    "raf_std_unsup_umap_projection_test_10_01= np.load('raf_std_unsup_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "raf_unsup_umap_projections_train_10_01 = []\n",
    "raf_unsup_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_train_emotion_norm)\n",
    "    raf_unsup_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_emotion_norm)\n",
    "    raf_unsup_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_unsup_umap_projections_train_10_01 = np.array(raf_unsup_umap_projections_train_10_01)\n",
    "raf_unsup_umap_projections_test_10_01 = np.array(raf_unsup_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_unsup_umap_projection_train_10_01 = np.mean(raf_unsup_umap_projections_train_10_01, axis=0)\n",
    "raf_std_unsup_umap_projection_train_10_01 = np.std(raf_unsup_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_unsup_umap_projection_test_10_01 = np.mean(raf_unsup_umap_projections_test_10_01, axis=0)\n",
    "raf_std_unsup_umap_projection_test_10_01 = np.std(raf_unsup_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_unsup_umap_projections_train_10_01.npy', raf_unsup_umap_projections_train_10_01)\n",
    "np.save('raf_mean_unsup_umap_projection_train_10_01.npy', raf_mean_unsup_umap_projection_train_10_01)\n",
    "np.save('raf_std_unsup_umap_projection_train_10_01.npy', raf_std_unsup_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_unsup_umap_projections_test_10_01.npy', raf_unsup_umap_projections_test_10_01)\n",
    "np.save('raf_mean_unsup_umap_projection_test_10_01.npy', raf_mean_unsup_umap_projection_test_10_01)\n",
    "np.save('raf_std_unsup_umap_projection_test_10_01.npy', raf_std_unsup_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the projections, mean, and standard deviation for the training set\n",
    "raf_unsup_umap_projections_train_10_01= np.load('raf_unsup_umap_projections_train_10_01.npy')\n",
    "raf_mean_unsup_umap_projection_train_10_01= np.load('raf_mean_unsup_umap_projection_train_10_01.npy')\n",
    "raf_std_unsup_umap_projection_train_10_01= np.load('raf_std_unsup_umap_projection_train_10_01.npy')\n",
    "\n",
    "# Load the projections, mean, and standard deviation for the test set\n",
    "raf_unsup_umap_projections_test_10_01= np.load('raf_unsup_umap_projections_test_10_01.npy')\n",
    "raf_mean_unsup_umap_projection_test_10_01= np.load('raf_mean_unsup_umap_projection_test_10_01.npy')\n",
    "raf_std_unsup_umap_projection_test_10_01= np.load('raf_std_unsup_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_unsup_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_unsup_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Unsupervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_umap_unsup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_umap_projection_test_10_01)) # second argument is y_test_pred_pca\n",
    "print(f\"ARI: {ari_raf_umap_unsup_10_01:.4f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_umap_unsup_10_01 = silhouette_score(raf_mean_unsup_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_umap_unsup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_unsup_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_mean_unsup_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_unsup_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_mean_unsup_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Load the saved mean UMAP projections\n",
    "# mean_projection = np.load('raf_mean_unsup_umap_projection_train_10_01.npy')  # Shape: (n_samples, 2)\n",
    "\n",
    "# # Step 2: Separate the mean projection by class\n",
    "# classes = np.unique(y_train)\n",
    "# class_gaussians = {}\n",
    "\n",
    "# # Calculate the mean and covariance for each class\n",
    "# for c in classes:\n",
    "#     class_points = mean_projection[y_train == c]  # Filter by class\n",
    "#     mean = np.mean(class_points, axis=0)\n",
    "#     cov = np.cov(class_points, rowvar=False)\n",
    "#     class_gaussians[c] = {\"mean\": mean, \"cov\": cov}\n",
    "\n",
    "# # Step 3: Visualize Gaussian distributions\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# # Plot UMAP embeddings for each class\n",
    "# for c in classes:\n",
    "#     class_points = mean_projection[y_train == c]\n",
    "#     plt.scatter(class_points[:, 0], class_points[:, 1], label=f\"Class {c}\", alpha=0.5, s=10)\n",
    "\n",
    "#     # Plot Gaussian contours\n",
    "#     mean = class_gaussians[c][\"mean\"]\n",
    "#     cov = class_gaussians[c][\"cov\"]\n",
    "#     x, y = np.meshgrid(\n",
    "#         np.linspace(mean[0] - 3, mean[0] + 3, 100), \n",
    "#         np.linspace(mean[1] - 3, mean[1] + 3, 100)\n",
    "#     )\n",
    "#     pos = np.dstack((x, y))\n",
    "#     rv = multivariate_normal(mean, cov)\n",
    "#     plt.contour(x, y, rv.pdf(pos), levels=5, alpha=0.8)\n",
    "\n",
    "# plt.title(\"UMAP Mean Projections with Gaussian Distributions per Class\")\n",
    "# plt.xlabel(\"UMAP Component 1\")\n",
    "# plt.ylabel(\"UMAP Component 2\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Step 4: Evaluate likelihood for a random point\n",
    "# random_point = np.array([0, 0])  # Example point in UMAP space\n",
    "# likelihoods = {c: multivariate_normal(class_gaussians[c][\"mean\"], class_gaussians[c][\"cov\"]).pdf(random_point)\n",
    "#                for c in classes}\n",
    "\n",
    "# print(\"Likelihoods for Random Point:\", likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "raf_sup_umap_projections_train_10_01= np.load('raf_sup_umap_projections_train_10_01.npy')\n",
    "raf_mean_sup_umap_projection_train_10_01= np.load('raf_mean_sup_umap_projection_train_10_01.npy')\n",
    "raf_std_sup_umap_projection_train_10_01= np.load('raf_std_sup_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "raf_sup_umap_projections_test_10_01= np.load('raf_sup_umap_projections_test_10_01.npy')\n",
    "raf_mean_sup_umap_projection_test_10_01= np.load('raf_mean_sup_umap_projection_test_10_01.npy')\n",
    "raf_std_sup_umap_projection_test_10_01= np.load('raf_std_sup_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for training and test sets\n",
    "raf_sup_umap_projections_train_10_01 = []\n",
    "raf_sup_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running Supervised UMAP - Iteration {run + 1}/{n_runs}...\")\n",
    "\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, \n",
    "        min_dist=min_dist, \n",
    "        n_components=n_components, \n",
    "        random_state=run\n",
    "    )\n",
    "\n",
    "    # Fit and transform the training data with labels\n",
    "    projection_train = umap_model.fit_transform(x_train_emotion_norm, y_train)\n",
    "    raf_sup_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_emotion_norm)\n",
    "    raf_sup_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_sup_umap_projections_train_10_01 = np.array(raf_sup_umap_projections_train_10_01)\n",
    "raf_sup_umap_projections_test_10_01 = np.array(raf_sup_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_sup_umap_projection_train_10_01 = np.mean(raf_sup_umap_projections_train_10_01, axis=0)\n",
    "raf_std_sup_umap_projection_train_10_01 = np.std(raf_sup_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_sup_umap_projection_test_10_01 = np.mean(raf_sup_umap_projections_test_10_01, axis=0)\n",
    "raf_std_sup_umap_projection_test_10_01 = np.std(raf_sup_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_sup_umap_projections_train_10_01.npy', raf_sup_umap_projections_train_10_01)\n",
    "np.save('raf_mean_sup_umap_projection_train_10_01.npy', raf_mean_sup_umap_projection_train_10_01)\n",
    "np.save('raf_std_sup_umap_projection_train_10_01.npy', raf_std_sup_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_sup_umap_projections_test_10_01.npy', raf_sup_umap_projections_test_10_01)\n",
    "np.save('raf_mean_sup_umap_projection_test_10_01.npy', raf_mean_sup_umap_projection_test_10_01)\n",
    "np.save('raf_std_sup_umap_projection_test_10_01.npy', raf_std_sup_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"Supervised UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_sup_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_sup_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Supervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_umap_sup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_umap_projection_train_10_01, y_train).predict(raf_mean_sup_umap_projection_test_10_01)) # second argument is y_test_pred_pca\n",
    "print(f\"ARI: {ari_raf_umap_sup_10_01:.4f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_umap_sup_10_01 = silhouette_score(raf_mean_sup_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_umap_projection_train_10_01, y_train).predict(raf_mean_sup_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_umap_sup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_sup_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_mean_sup_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_sup_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_mean_sup_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Load the saved mean UMAP projections\n",
    "# mean_projection = np.load('raf_mean_sup_umap_projection_10_01.npy')  # Shape: (n_samples, 2)\n",
    "\n",
    "# # Step 2: Separate the mean projection by class\n",
    "# classes = np.unique(y_train)\n",
    "# class_gaussians = {}\n",
    "\n",
    "# # Calculate the mean and covariance for each class\n",
    "# for c in classes:\n",
    "#     class_points = mean_projection[y_train == c]  # Filter by class\n",
    "#     mean = np.mean(class_points, axis=0)\n",
    "#     cov = np.cov(class_points, rowvar=False)\n",
    "#     class_gaussians[c] = {\"mean\": mean, \"cov\": cov}\n",
    "\n",
    "# # Step 3: Visualize Gaussian distributions\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# # Plot UMAP embeddings for each class\n",
    "# for c in classes:\n",
    "#     class_points = mean_projection[y_train == c]\n",
    "#     plt.scatter(class_points[:, 0], class_points[:, 1], label=f\"Class {c}\", alpha=0.5, s=10)\n",
    "\n",
    "#     # Plot Gaussian contours\n",
    "#     mean = class_gaussians[c][\"mean\"]\n",
    "#     cov = class_gaussians[c][\"cov\"]\n",
    "#     x, y = np.meshgrid(\n",
    "#         np.linspace(mean[0] - 3, mean[0] + 3, 100), \n",
    "#         np.linspace(mean[1] - 3, mean[1] + 3, 100)\n",
    "#     )\n",
    "#     pos = np.dstack((x, y))\n",
    "#     rv = multivariate_normal(mean, cov)\n",
    "#     plt.contour(x, y, rv.pdf(pos), levels=5, alpha=0.8)\n",
    "\n",
    "# plt.title(\"UMAP Mean Projections with Gaussian Distributions per Class\")\n",
    "# plt.xlabel(\"UMAP Component 1\")\n",
    "# plt.ylabel(\"UMAP Component 2\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Step 4: Evaluate likelihood for a random point\n",
    "# random_point = np.array([0, 0])  # Example point in UMAP space\n",
    "# likelihoods = {c: multivariate_normal(class_gaussians[c][\"mean\"], class_gaussians[c][\"cov\"]).pdf(random_point)\n",
    "#                for c in classes}\n",
    "\n",
    "# print(\"Likelihoods for Random Point:\", likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Before UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply PCA\n",
    "pca = PCA(0.95)\n",
    "x_train_pca_emotions = pca.fit_transform(x_train_emotion_norm)\n",
    "x_test_pca_emotions = pca.transform(x_test_emotion_norm)\n",
    "\n",
    "# print(f\"Original number of features: {x_train_emotion_norm.shape[1]}\")\n",
    "# print(f\"Reduced number of features: {x_train_pca_emotions.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original number of features: {x_train_emotion_norm.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the projections, mean, and standard deviation\n",
    "np.save('x_train_raf_pca_emotions.npy', x_train_pca_emotions)\n",
    "np.save('x_test_raf_pca_emotions.npy', x_test_pca_emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA + UMAP Unsupervised 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "raf_unsup_pca_umap_projections_train_10_01= np.load('raf_unsup_pca_umap_projections_train_10_01.npy')\n",
    "raf_mean_unsup_pca_umap_projection_train_10_01= np.load('raf_mean_unsup_pca_umap_projection_train_10_01.npy')\n",
    "raf_std_unsup_pca_umap_projection_train_10_01= np.load('raf_std_unsup_pca_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "raf_unsup_pca_umap_projections_test_10_01= np.load('raf_unsup_pca_umap_projections_test_10_01.npy')\n",
    "raf_mean_unsup_pca_umap_projection_test_10_01= np.load('raf_mean_unsup_pca_umap_projection_test_10_01.npy')\n",
    "raf_std_unsup_pca_umap_projection_test_10_01= np.load('raf_std_unsup_pca_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "raf_unsup_pca_umap_projections_train_10_01 = []\n",
    "raf_unsup_pca_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_train_pca_emotions)\n",
    "    raf_unsup_pca_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_pca_emotions)\n",
    "    raf_unsup_pca_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_unsup_pca_umap_projections_train_10_01 = np.array(raf_unsup_pca_umap_projections_train_10_01)\n",
    "raf_unsup_pca_umap_projections_test_10_01 = np.array(raf_unsup_pca_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_unsup_pca_umap_projection_train_10_01 = np.mean(raf_unsup_pca_umap_projections_train_10_01, axis=0)\n",
    "raf_std_unsup_pca_umap_projection_train_10_01 = np.std(raf_unsup_pca_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_unsup_pca_umap_projection_test_10_01 = np.mean(raf_unsup_pca_umap_projections_test_10_01, axis=0)\n",
    "raf_std_unsup_pca_umap_projection_test_10_01 = np.std(raf_unsup_pca_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_unsup_pca_umap_projections_train_10_01.npy', raf_unsup_pca_umap_projections_train_10_01)\n",
    "np.save('raf_mean_unsup_pca_umap_projection_train_10_01.npy', raf_mean_unsup_pca_umap_projection_train_10_01)\n",
    "np.save('raf_std_unsup_pca_umap_projection_train_10_01.npy', raf_std_unsup_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_unsup_pca_umap_projections_test_10_01.npy', raf_unsup_pca_umap_projections_test_10_01)\n",
    "np.save('raf_mean_unsup_pca_umap_projection_test_10_01.npy', raf_mean_unsup_pca_umap_projection_test_10_01)\n",
    "np.save('raf_std_unsup_pca_umap_projection_test_10_01.npy', raf_std_unsup_pca_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_unsup_pca_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_unsup_pca_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"PCA + Unsupervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_pca_umap_unsup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_pca_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_pca_umap_projection_test_10_01)) # second argument is y_test_pred_pca\n",
    "print(f\"ARI: {ari_raf_pca_umap_unsup_10_01:.2f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_pca_umap_unsup_10_01 = silhouette_score(raf_mean_unsup_pca_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_pca_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_pca_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_pca_umap_unsup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_unsup_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_mean_unsup_pca_umap_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_unsup_pca_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_mean_unsup_pca_umap_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define parameters\n",
    "# n_neighbors = 10\n",
    "# min_dist = 0.1\n",
    "# n_components = 2\n",
    "# n_runs = 10  # Number of runs\n",
    "\n",
    "# # Store UMAP projections for each run\n",
    "# raf_pca_unsup_umap_projections_10_01 = []\n",
    "\n",
    "# # Run UMAP multiple times\n",
    "# for run in range(n_runs):\n",
    "#     # Create UMAP model\n",
    "#     umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=None)\n",
    "    \n",
    "#     # Fit and transform the data\n",
    "#     raf_pca_unsup_umap_projection = umap_model.fit_transform(x_train_pca_emotions)\n",
    "    \n",
    "#     # Store the projection\n",
    "#     raf_pca_unsup_umap_projections_10_01.append(raf_pca_unsup_umap_projection)\n",
    "\n",
    "# # Convert the list of projections to a numpy array\n",
    "# raf_pca_unsup_umap_projections_10_01 = np.array(raf_pca_unsup_umap_projections_10_01)\n",
    "\n",
    "# # Calculate mean and standard deviation of projections across runs\n",
    "# raf_mean_pca_unsup_umap_projection_10_01 = np.mean(raf_pca_unsup_umap_projections_10_01, axis=0)\n",
    "# raf_std_pca_unsup_umap_projection_10_01 = np.std(raf_pca_unsup_umap_projections_10_01, axis=0)\n",
    "\n",
    "# # Save the projections, mean, and standard deviation\n",
    "# np.save('raf_pca_unsup_umap_projections_10_01.npy', raf_pca_unsup_umap_projections_10_01)\n",
    "# np.save('raf_mean_pca_unsup_umap_projection_10_01.npy', raf_mean_pca_unsup_umap_projection_10_01)\n",
    "# np.save('raf_std_pca_unsup_umap_projection_10_01.npy', raf_std_pca_unsup_umap_projection_10_01)\n",
    "\n",
    "# # Output confirmation\n",
    "# print(\"UMAP projections, mean, and standard deviation have been saved with identifiers '_10_01'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate Silhouette Score\n",
    "# sil_score_raf_std_pca_unsup_umap = silhouette_score(raf_mean_pca_unsup_umap_projection_10_01, y_train)\n",
    "# print(f\"Silhouette Score: {sil_score_raf_std_pca_unsup_umap:.2f}\")\n",
    "\n",
    "# # Calculate Davies-Bouldin Index (lower is better)\n",
    "# db_score_raf_std_pca_unsup_umap = davies_bouldin_score(raf_mean_pca_unsup_umap_projection_10_01, y_train)\n",
    "# print(f\"Davies-Bouldin Index: {db_score_raf_std_pca_unsup_umap:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check with SEBA maybe it is interesting to share it like this\n",
    "\n",
    "# # Iterate over each UMAP projection run\n",
    "# sil_scores_pca_umap = []\n",
    "# db_scores_pca_umap = []\n",
    "\n",
    "# for i, projection in enumerate(raf_pca_unsup_umap_projections_10_01):\n",
    "#     sil_score_pca_umap = silhouette_score(projection, y_train)\n",
    "#     db_score_pca_umap = davies_bouldin_score(projection, y_train)\n",
    "#     sil_scores_pca_umap.append(sil_score_pca_umap)\n",
    "#     db_scores_pca_umap.append(db_score_pca_umap)\n",
    "#     print(f\"Run {i+1}: Silhouette Score = {sil_score_pca_umap:.2f}, Davies-Bouldin Index = {db_score_pca_umap:.2f}\")\n",
    "\n",
    "# # Calculate mean and standard deviation of scores across runs\n",
    "# mean_sil_score = np.mean(sil_scores_pca_umap)\n",
    "# std_sil_score = np.std(sil_scores_pca_umap)\n",
    "# mean_db_score = np.mean(db_scores_pca_umap)\n",
    "# std_db_score = np.std(db_scores_pca_umap)\n",
    "\n",
    "# print(f\"Mean Silhouette Score: {mean_sil_score:.2f}  {std_sil_score:.2f}\")\n",
    "# print(f\"Mean Davies-Bouldin Index: {mean_db_score:.2f}  {std_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA + UMAP Supervised 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Apply PCA\n",
    "# pca = PCA(0.95)\n",
    "# x_train_pca_emotions = pca.fit_transform(x_train_emotion_norm, y_train)\n",
    "# #x_test_pca_emotions = pca.transform(x_test_emotion_norm, y_test)\n",
    "\n",
    "# #print(f\"Original number of features: {x_test_emotion_norm.shape[1]}\")\n",
    "# print(f\"Reduced number of features: {x_train_pca_emotions.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "raf_sup_pca_umap_projections_train_10_01= np.load('raf_sup_pca_umap_projections_train_10_01.npy')\n",
    "raf_mean_sup_pca_umap_projection_train_10_01= np.load('raf_mean_sup_pca_umap_projection_train_10_01.npy')\n",
    "raf_std_sup_pca_umap_projection_train_10_01= np.load('raf_std_sup_pca_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "raf_sup_pca_umap_projections_test_10_01= np.load('raf_sup_pca_umap_projections_test_10_01.npy')\n",
    "raf_mean_sup_pca_umap_projection_test_10_01= np.load('raf_mean_sup_pca_umap_projection_test_10_01.npy')\n",
    "raf_std_sup_pca_umap_projection_test_10_01= np.load('raf_std_sup_pca_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for training and test sets\n",
    "raf_sup_pca_umap_projections_train_10_01 = []\n",
    "raf_sup_pca_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running Supervised UMAP - Iteration {run + 1}/{n_runs}...\")\n",
    "\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, \n",
    "        min_dist=min_dist, \n",
    "        n_components=n_components, \n",
    "        random_state=run\n",
    "    )\n",
    "\n",
    "    # Fit and transform the training data with labels\n",
    "    projection_train = umap_model.fit_transform(x_train_pca_emotions, y_train)\n",
    "    raf_sup_pca_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_pca_emotions)\n",
    "    raf_sup_pca_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_sup_pca_umap_projections_train_10_01 = np.array(raf_sup_pca_umap_projections_train_10_01)\n",
    "raf_sup_pca_umap_projections_test_10_01 = np.array(raf_sup_pca_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_sup_pca_umap_projection_train_10_01 = np.mean(raf_sup_pca_umap_projections_train_10_01, axis=0)\n",
    "raf_std_sup_pca_umap_projection_train_10_01 = np.std(raf_sup_pca_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_sup_pca_umap_projection_test_10_01 = np.mean(raf_sup_pca_umap_projections_test_10_01, axis=0)\n",
    "raf_std_sup_pca_umap_projection_test_10_01 = np.std(raf_sup_pca_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_sup_pca_umap_projections_train_10_01.npy', raf_sup_pca_umap_projections_train_10_01)\n",
    "np.save('raf_mean_sup_pca_umap_projection_train_10_01.npy', raf_mean_sup_pca_umap_projection_train_10_01)\n",
    "np.save('raf_std_sup_pca_umap_projection_train_10_01.npy', raf_std_sup_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_sup_pca_umap_projections_test_10_01.npy', raf_sup_pca_umap_projections_test_10_01)\n",
    "np.save('raf_mean_sup_pca_umap_projection_test_10_01.npy', raf_mean_sup_pca_umap_projection_test_10_01)\n",
    "np.save('raf_std_sup_pca_umap_projection_test_10_01.npy', raf_std_sup_pca_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"Supervised UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_sup_pca_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_sup_pca_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"PCA + Supervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_pca_umap_sup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_pca_umap_projection_train_10_01, y_train).predict(raf_mean_sup_pca_umap_projection_test_10_01)) # second argument is y_test_pred_pca\n",
    "print(f\"ARI: {ari_raf_pca_umap_sup_10_01:.2f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_pca_umap_sup_10_01 = silhouette_score(raf_mean_sup_pca_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_pca_umap_projection_train_10_01, y_train).predict(raf_mean_sup_pca_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_pca_umap_sup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_sup_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_mean_sup_pca_umap_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_sup_pca_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_mean_sup_pca_umap_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLE  Before UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for LLE\n",
    "n_neighbors = 10\n",
    "n_components = 147\n",
    "\n",
    "# Initialize the LLE model\n",
    "lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components, method='standard', random_state=42)\n",
    "\n",
    "# Fit and transform the training data\n",
    "print(\"Running LLE on the training set...\")\n",
    "x_train_raf_lle = lle.fit_transform(x_train_emotion_norm)\n",
    "print(\"LLE transformation on training set completed.\")\n",
    "\n",
    "# Transform the test data using the fitted LLE model\n",
    "print(\"Running LLE on the test set...\")\n",
    "x_test_raf_lle = lle.transform(x_test_emotion_norm)\n",
    "print(\"LLE transformation on test set completed.\")\n",
    "\n",
    "# Print shapes of transformed data\n",
    "print(f\"Shape of LLE-transformed training data: {x_train_raf_lle.shape}\")\n",
    "print(f\"Shape of LLE-transformed test data: {x_test_raf_lle.shape}\")\n",
    "\n",
    "# Optional: Save the LLE-transformed data for later use\n",
    "np.save('x_train_lle.npy', x_train_raf_lle)\n",
    "np.save('x_test_lle.npy', x_test_raf_lle)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"LLE-transformed data has been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLE + UMAP Unsupervised 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the projections, mean, and standard deviation for the training set\n",
    "raf_unsup_lle_umap_projections_train_10_01= np.load('raf_unsup_lle_umap_projections_train_10_01.npy')\n",
    "raf_mean_unsup_lle_umap_projection_train_10_01= np.load('raf_mean_unsup_lle_umap_projection_train_10_01.npy')\n",
    "raf_std_unsup_lle_umap_projection_train_10_01= np.load('raf_std_unsup_lle_umap_projection_train_10_01.npy')\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "raf_unsup_lle_umap_projections_test_10_01= np.load('raf_unsup_lle_umap_projections_test_10_01.npy')\n",
    "raf_mean_unsup_lle_umap_projection_test_10_01= np.load('raf_mean_unsup_lle_umap_projection_test_10_01.npy')\n",
    "raf_std_unsup_lle_umap_projection_test_10_01= np.load('raf_std_unsup_lle_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "raf_unsup_lle_umap_projections_train_10_01 = []\n",
    "raf_unsup_lle_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_train_raf_lle)\n",
    "    raf_unsup_lle_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_raf_lle)\n",
    "    raf_unsup_lle_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_unsup_lle_umap_projections_train_10_01 = np.array(raf_unsup_lle_umap_projections_train_10_01)\n",
    "raf_unsup_lle_umap_projections_test_10_01 = np.array(raf_unsup_lle_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_unsup_lle_umap_projection_train_10_01 = np.mean(raf_unsup_lle_umap_projections_train_10_01, axis=0)\n",
    "raf_std_unsup_lle_umap_projection_train_10_01 = np.std(raf_unsup_lle_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_unsup_lle_umap_projection_test_10_01 = np.mean(raf_unsup_lle_umap_projections_test_10_01, axis=0)\n",
    "raf_std_unsup_lle_umap_projection_test_10_01 = np.std(raf_unsup_lle_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_unsup_lle_umap_projections_train_10_01.npy', raf_unsup_lle_umap_projections_train_10_01)\n",
    "np.save('raf_mean_unsup_lle_umap_projection_train_10_01.npy', raf_mean_unsup_lle_umap_projection_train_10_01)\n",
    "np.save('raf_std_unsup_lle_umap_projection_train_10_01.npy', raf_std_unsup_lle_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_unsup_lle_umap_projections_test_10_01.npy', raf_unsup_lle_umap_projections_test_10_01)\n",
    "np.save('raf_mean_unsup_lle_umap_projection_test_10_01.npy', raf_mean_unsup_lle_umap_projection_test_10_01)\n",
    "np.save('raf_std_unsup_lle_umap_projection_test_10_01.npy', raf_std_unsup_lle_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_unsup_lle_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_unsup_lle_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"LLE + Unsupervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_lle_umap_unsup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_lle_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_lle_umap_projection_test_10_01)) # second argument is y_test_pred_lle\n",
    "print(f\"ARI: {ari_raf_lle_umap_unsup_10_01:.2f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_lle_umap_unsup_10_01 = silhouette_score(raf_mean_unsup_lle_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_lle_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_lle_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_lle_umap_unsup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_unsup_lle_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_lle_umap_unsup_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_unsup_lle_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_lle_umap_unsup_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLE + UMAP Supervised 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the projections, mean, and standard deviation for the training set\n",
    "raf_sup_lle_umap_projections_train_10_01= np.load('raf_sup_lle_umap_projections_train_10_01.npy')\n",
    "raf_mean_sup_lle_umap_projection_train_10_01= np.load('raf_mean_sup_lle_umap_projection_train_10_01.npy')\n",
    "raf_std_sup_lle_umap_projection_train_10_01= np.load('raf_std_sup_lle_umap_projection_train_10_01.npy')\n",
    "\n",
    "# Load the projections, mean, and standard deviation for the test set\n",
    "raf_sup_lle_umap_projections_test_10_01= np.load('raf_sup_lle_umap_projections_test_10_01.npy')\n",
    "raf_mean_sup_lle_umap_projection_test_10_01= np.load('raf_mean_sup_lle_umap_projection_test_10_01.npy')\n",
    "raf_std_sup_lle_umap_projection_test_10_01= np.load('raf_std_sup_lle_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "raf_sup_lle_umap_projections_train_10_01 = []\n",
    "raf_sup_lle_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_train_raf_lle,y_train)\n",
    "    raf_sup_lle_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_test_raf_lle)\n",
    "    raf_sup_lle_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_sup_lle_umap_projections_train_10_01 = np.array(raf_sup_lle_umap_projections_train_10_01)\n",
    "raf_sup_lle_umap_projections_test_10_01 = np.array(raf_sup_lle_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_sup_lle_umap_projection_train_10_01 = np.mean(raf_sup_lle_umap_projections_train_10_01, axis=0)\n",
    "raf_std_sup_lle_umap_projection_train_10_01 = np.std(raf_sup_lle_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_sup_lle_umap_projection_test_10_01 = np.mean(raf_sup_lle_umap_projections_test_10_01, axis=0)\n",
    "raf_std_sup_lle_umap_projection_test_10_01 = np.std(raf_sup_lle_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_sup_lle_umap_projections_train_10_01.npy', raf_sup_lle_umap_projections_train_10_01)\n",
    "np.save('raf_mean_sup_lle_umap_projection_train_10_01.npy', raf_mean_sup_lle_umap_projection_train_10_01)\n",
    "np.save('raf_std_sup_lle_umap_projection_train_10_01.npy', raf_std_sup_lle_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_sup_lle_umap_projections_test_10_01.npy', raf_sup_lle_umap_projections_test_10_01)\n",
    "np.save('raf_mean_sup_lle_umap_projection_test_10_01.npy', raf_mean_sup_lle_umap_projection_test_10_01)\n",
    "np.save('raf_std_sup_lle_umap_projection_test_10_01.npy', raf_std_sup_lle_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_sup_lle_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_sup_lle_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"LLE + Supervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_lle_umap_sup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_lle_umap_projection_train_10_01, y_train).predict(raf_mean_sup_lle_umap_projection_test_10_01)) # second argument is y_test_pred_lle\n",
    "print(f\"ARI: {ari_raf_lle_umap_sup_10_01:.2f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_lle_umap_sup_10_01 = silhouette_score(raf_mean_sup_lle_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_lle_umap_projection_train_10_01, y_train).predict(raf_mean_sup_lle_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_lle_umap_sup_10_01:.2f}\")\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_sup_lle_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_lle_umap_sup_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_sup_lle_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_lle_umap_sup_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set very imbalanced.\n",
    "\n",
    "**SMOTE** (Synthetic Minority Oversampling Technique) is a method for addressing class imbalance by generating synthetic samples for minority classes. It works by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Split into train and test sets if not already split\n",
    "# (In your case, you already have x_train_emotion_norm and y_train_emotion)\n",
    "\n",
    "# Apply SMOTE to the training set\n",
    "smote = SMOTE(random_state=42)  # Random state for reproducibility\n",
    "x_train_balanced, y_train_balanced = smote.fit_resample(x_train_emotion_norm, y_train_emotion)\n",
    "\n",
    "# Verify the new class distribution\n",
    "from collections import Counter\n",
    "print(f\"Original class distribution: {Counter(y_train_emotion)}\")\n",
    "print(f\"Balanced class distribution: {Counter(y_train_balanced)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.95)\n",
    "x_train_emotion2_pca = pca.fit_transform(x_train_balanced)\n",
    "\n",
    "# Step 2: UMAP on PCA-transformed data\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "umap_train_embedding = reducer.fit_transform(x_train_emotion2_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isomap before UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Downsample the Dataset Consistently\n",
    "def downsample_consistent(x_train, y_train, sample_fraction=0.35):\n",
    "    \"\"\"\n",
    "    Downsample the dataset consistently, ensuring label distribution is preserved.\n",
    "    Returns sampled indices to extract data points and labels.\n",
    "    \"\"\"\n",
    "    sampled_indices = []\n",
    "    unique_labels = np.unique(y_train)\n",
    "    for label in unique_labels:\n",
    "        # Get indices for the current label\n",
    "        label_indices = np.where(y_train == label)[0]\n",
    "        # Sample a fraction of points for this label\n",
    "        sampled_indices_label = resample(\n",
    "            label_indices, n_samples=int(len(label_indices) * sample_fraction), replace=False\n",
    "        )\n",
    "        sampled_indices.extend(sampled_indices_label)\n",
    "    return np.array(sampled_indices)\n",
    "\n",
    "# Downsample x_train and y_train\n",
    "sample_fraction = 0.35\n",
    "sampled_indices = downsample_consistent(x_train, y_train, sample_fraction=sample_fraction)\n",
    "\n",
    "# Extract downsampled data\n",
    "x_train_sampled = x_train[sampled_indices]\n",
    "y_train_sampled = y_train[sampled_indices]\n",
    "\n",
    "print(\"Downsampled x_train shape:\", x_train_sampled.shape)\n",
    "print(\"Downsampled y_train shape:\", y_train_sampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Downsample the Dataset Consistently\n",
    "def downsample_consistent(x_test, y_test, sample_fraction=0.35):\n",
    "    \"\"\"\n",
    "    Downsample the dataset consistently, ensuring label distribution is preserved.\n",
    "    Returns sampled indices to extract data points and labels.\n",
    "    \"\"\"\n",
    "    sampled_indices = []\n",
    "    unique_labels = np.unique(y_test)\n",
    "    for label in unique_labels:\n",
    "        # Get indices for the current label\n",
    "        label_indices = np.where(y_test == label)[0]\n",
    "        # Sample a fraction of points for this label\n",
    "        sampled_indices_label = resample(\n",
    "            label_indices, n_samples=int(len(label_indices) * sample_fraction), replace=False\n",
    "        )\n",
    "        sampled_indices.extend(sampled_indices_label)\n",
    "    return np.array(sampled_indices)\n",
    "\n",
    "# Downsample x_test and y_test\n",
    "sample_fraction = 0.35\n",
    "sampled_indices = downsample_consistent(x_test, y_test, sample_fraction=sample_fraction)\n",
    "\n",
    "# Extract downsampled data\n",
    "x_test_sampled = x_test[sampled_indices]\n",
    "y_test_sampled = y_test[sampled_indices]\n",
    "\n",
    "print(\"Downsampled x_test shape:\", x_test_sampled.shape)\n",
    "print(\"Downsampled y_test shape:\", y_test_sampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Flatten the Images into 1D Vectors\n",
    "x_train_flattened = x_train_sampled.reshape(x_train_sampled.shape[0], -1)  # Flatten to (num_samples, 2304)\n",
    "x_test_flattened = x_test_sampled.reshape(x_test_sampled.shape[0], -1)    # Flatten to (num_samples, 2304)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Shape of x_train_flattened:\", x_train_flattened.shape)  # (num_train_samples, 2304)\n",
    "print(\"Shape of x_test_flattened:\", x_test_flattened.shape)    # (num_test_samples, 2304)\n",
    "\n",
    "# Step 2: Normalize the Sampled Data\n",
    "scaler = StandardScaler()\n",
    "x_train_sampled_scaled = scaler.fit_transform(x_train_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply Isomap for Dimensionality Reduction\n",
    "n_neighbors_isomap = 10  # Number of neighbors for Isomap\n",
    "n_components_isomap = 50  # Reduce to 50 dimensions before UMAP\n",
    "isomap = Isomap(n_neighbors=n_neighbors_isomap, n_components=n_components_isomap)\n",
    "x_train_isomap = isomap.fit_transform(x_train_sampled_scaled)\n",
    "\n",
    "print(\"Isomap reduced shape:\", x_train_isomap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Apply UMAP for Further Reduction to 2D\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "x_train_umap = reducer.fit_transform(x_train_isomap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize the Results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(\n",
    "    x_train_umap[:, 0],\n",
    "    x_train_umap[:, 1],\n",
    "    c=y_train_sampled,  # Color by ground truth labels\n",
    "    cmap=\"tab10\",\n",
    "    s=5,\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title(\"Isomap + UMAP Projection: Downsampled Training Data\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "plt.colorbar(label=\"Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# Apply KMeans to the UMAP-reduced data\n",
    "n_clusters = len(np.unique(y_train_sampled))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "predicted_labels = kmeans.fit_predict(x_train_umap)\n",
    "\n",
    "# Calculate Silhouette Score\n",
    "sil_score = silhouette_score(x_train_umap, predicted_labels)\n",
    "print(f\"Silhouette Score (with KMeans): {sil_score:.2f}\")\n",
    "\n",
    "# Calculate Davies-Bouldin Index\n",
    "db_score = davies_bouldin_score(x_train_umap, predicted_labels)\n",
    "print(f\"Davies-Bouldin Index (with KMeans): {db_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "ari = adjusted_rand_score(y_train_sampled, predicted_labels)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.2f}\")\n",
    "\n",
    "nmi = normalized_mutual_info_score(y_train_sampled, predicted_labels)\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using HDBScan instead of Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10, metric='euclidean')\n",
    "predicted_labels = clusterer.fit_predict(x_train_umap)\n",
    "\n",
    "# Evaluate ARI and NMI\n",
    "ari_score = adjusted_rand_score(y_train_sampled, predicted_labels)\n",
    "nmi_score = normalized_mutual_info_score(y_train_sampled, predicted_labels)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_score:.2f}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet before UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and reshape data for ResNet\n",
    "x_train_reshaped = x_train_emotion.reshape(-1, 64, 64, 1).repeat(3, axis=-1)  # ResNet requires 3 channels\n",
    "x_train_preprocessed = preprocess_input(x_train_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ResNet50 and extract features\n",
    "resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "x_train_features = resnet.predict(x_train_preprocessed)\n",
    "x_train_features_flat = x_train_features.reshape(x_train_features.shape[0], -1)  # Flatten\n",
    "\n",
    "# Use these features as input for UMAP\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "umap_train_embedding_cnn = reducer.fit_transform(x_train_features_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(umap_train_embedding_cnn[:, 0], umap_train_embedding_cnn[:, 1], c=y_train_emotion, cmap='Spectral', s=5)\n",
    "plt.colorbar(label=\"Emotion Label\")\n",
    "plt.title(\"UMAP Projection of ResNet Features\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Silhouette Score\n",
    "sil_score = silhouette_score(umap_train_embedding_cnn, y_train_emotion)\n",
    "print(f\"Silhouette Score: {sil_score:.2f}\")\n",
    "\n",
    "# Calculate Davies-Bouldin Index (lower is better)\n",
    "db_score = davies_bouldin_score(umap_train_embedding_cnn, y_train_emotion)\n",
    "print(f\"Davies-Bouldin Index: {db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG before UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and reshape data for VGG16\n",
    "# Reshape the grayscale images to 224x224x3 (required for VGG16)\n",
    "#x_train_reshaped = x_train_emotion.reshape(-1, 64, 64, 1).repeat(3, axis=-1)  # Convert to 3 channels -- Done it for ResNet\n",
    "x_train_resized = np.array([np.resize(img, (224, 224, 3)) for img in x_train_reshaped])\n",
    "\n",
    "# Preprocess input for VGG16\n",
    "x_train_preprocessed = preprocess_input(x_train_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained VGG16 model and extract features\n",
    "vgg = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x_train_features = vgg.predict(x_train_preprocessed)\n",
    "\n",
    "# Flatten the features for UMAP\n",
    "x_train_features_flat = x_train_features.reshape(x_train_features.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP to the VGG16 features\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "umap_train_embedding_vgg = reducer.fit_transform(x_train_features_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(umap_train_embedding_vgg[:, 0], umap_train_embedding_vgg[:, 1], c=y_train_emotion, cmap='Spectral', s=5)\n",
    "plt.colorbar(label=\"Emotion Label\")\n",
    "plt.title(\"UMAP Projection of VGG16 Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply PCA\n",
    "    pca = PCA(0.95)\n",
    "    x_train_pca_emotions = pca.fit_transform(x_train_emotion_norm)\n",
    "    x_test_pca_emotions = pca.transform(x_test_emotion_norm)\n",
    "\n",
    "    print(f\"Original number of features: {x_test_emotion_norm.shape[1]}\")\n",
    "    print(f\"Reduced number of features: {x_train_pca_emotions.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the explained variance ratio\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title('Explained Variance vs Number of Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=10000)\n",
    "clf.fit(x_train_pca_emotions, y_train_emotion)\n",
    "\n",
    "# Predictions\n",
    "y_pred = clf.predict(x_test_pca_emotions)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test_emotion, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_emotion, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_emotion, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_emotion, y_pred)\n",
    "\n",
    "# Map the numeric labels (1 to 7) to their emotion names\n",
    "emotion_labels = list(emotion_map.values())  # [\"Surprise\", \"Fear\", ..., \"Neutral\"]\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=emotion_labels, \n",
    "            yticklabels=emotion_labels)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2D projection with cluster labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=x_train_pca_emotions[:, 0], y=x_train_pca_emotions[:, 1], hue=y_train_emotion, palette='tab10', s=10, legend='full')\n",
    "plt.title(\"2D Scatter Plot of PCA-reduced Data with Cluster Labels\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the images\n",
    "x_train_flat = x_train.reshape(len(x_train), -1)\n",
    "x_test_flat = x_test.reshape(len(x_test), -1)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_flat)\n",
    "x_test_scaled = scaler.transform(x_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "n_components = 100  # Number of principal components to retain\n",
    "pca = PCA(n_components=n_components)\n",
    "x_train_pca = pca.fit_transform(x_train_scaled)\n",
    "x_test_pca = pca.transform(x_test_scaled)\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(f\"Explained variance by the first {n_components} components: {sum(pca.explained_variance_ratio_):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_train_pca[:, 0], x_train_pca[:, 1], c=y_train, cmap='viridis', s=2, alpha=0.5)\n",
    "plt.colorbar()\n",
    "plt.title('First Two Principal Components (Train Set)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (assuming 7 emotions based on your dataset)\n",
    "n_clusters = 7\n",
    "\n",
    "# Apply KMeans to the PCA-transformed data\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "y_train_pred = kmeans.fit_predict(x_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the clustering\n",
    "silhouette_avg = silhouette_score(x_train_pca, y_train_pred)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.2f}\")\n",
    "\n",
    "# Compare clustering to ground truth using Adjusted Rand Index (ARI)\n",
    "ari = adjusted_rand_score(y_train, y_train_pred)\n",
    "print(f\"Adjusted Rand Index: {ari:.2f}\")\n",
    "\n",
    "# Visualize the clusters in the first two PCA dimensions\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_train_pca[:, 0], x_train_pca[:, 1], c=y_train_pred, cmap='viridis', s=5, alpha=0.5)\n",
    "plt.colorbar(label=\"Cluster ID\")\n",
    "plt.title(f\"KMeans Clustering on PCA-reduced Data ({n_clusters} Clusters)\")\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = UMAP(n_neighbors=15, n_components=2, random_state=42)\n",
    "x_train_umap = umap.fit_transform(x_train_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize UMAP projection\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_train_umap[:, 0], x_train_umap[:, 1], c=y_train, cmap='viridis', s=5, alpha=0.5)\n",
    "plt.colorbar(label=\"True Label\")\n",
    "plt.title('UMAP Projection of Training Data')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gabor + PCA + UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gabor + PCA + Unsupervised UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "raf_unsup_gabor_pca_umap_projections_train_10_01= np.load('raf_unsup_gabor_pca_umap_projections_train_10_01.npy')\n",
    "raf_mean_unsup_gabor_pca_umap_projection_train_10_01= np.load('raf_mean_unsup_gabor_pca_umap_projection_train_10_01.npy')\n",
    "raf_std_unsup_gabor_pca_umap_projection_train_10_01= np.load('raf_std_unsup_gabor_pca_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "raf_unsup_gabor_pca_umap_projections_test_10_01= np.load('raf_unsup_gabor_pca_umap_projections_test_10_01.npy')\n",
    "raf_mean_unsup_gabor_pca_umap_projection_test_10_01= np.load('raf_mean_unsup_gabor_pca_umap_projection_test_10_01.npy')\n",
    "raf_std_unsup_gabor_pca_umap_projection_test_10_01= np.load('raf_std_unsup_gabor_pca_umap_projection_test_10_01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gabor Kernels\n",
    "def create_gabor_kernels():\n",
    "    \"\"\"Generates a set of Gabor kernels with different orientations and frequencies.\"\"\"\n",
    "    kernels = []\n",
    "    ksize = 31  # Kernel size\n",
    "    sigma = 4.0  # Standard deviation of the Gaussian envelope\n",
    "    lambd = 10.0  # Wavelength of the sinusoidal factor\n",
    "    gamma = 0.5  # Spatial aspect ratio\n",
    "    for theta in np.arange(0, np.pi, np.pi / 4):  # 8 orientations\n",
    "        kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lambd, gamma, psi=0, ktype=cv2.CV_32F)\n",
    "        kernels.append(kernel)\n",
    "    return kernels\n",
    "\n",
    "# Apply Gabor Filters\n",
    "def apply_gabor_filters(images, kernels):\n",
    "    \"\"\"Applies a set of Gabor filters to a batch of images.\"\"\"\n",
    "    gabor_features = []\n",
    "    for image in images:\n",
    "        image_2d = image.reshape(48, 48)  # Reshape back to 2D (assumes 48x48 images)\n",
    "        responses = []\n",
    "        for kernel in kernels:\n",
    "            filtered = cv2.filter2D(image_2d, cv2.CV_32F, kernel)  # Apply Gabor filter\n",
    "            responses.append(filtered.flatten())  # Flatten the filtered image\n",
    "        gabor_features.append(np.concatenate(responses))  # Concatenate all filter responses\n",
    "    return np.array(gabor_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Gabor kernels\n",
    "gabor_kernels = create_gabor_kernels()\n",
    "print(f\"Generated {len(gabor_kernels)} Gabor kernels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Gabor filters to the training and test sets\n",
    "x_train_gabor = apply_gabor_filters(x_train_emotion_norm, gabor_kernels)\n",
    "x_test_gabor = apply_gabor_filters(x_test_emotion_norm, gabor_kernels)\n",
    "\n",
    "print(f\"Train Gabor feature shape: {x_train_gabor.shape}\")\n",
    "print(f\"Test Gabor feature shape: {x_test_gabor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_raf_train_gabor = scaler.fit_transform(x_train_gabor)\n",
    "x_raf_test_gabor = scaler.transform(x_test_gabor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying PCA to Gabor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(0.95)\n",
    "x_raf_train_pca_gabor = pca.fit_transform(x_raf_train_gabor)\n",
    "x_raf_test_pca_gabor = pca.transform(x_raf_test_gabor)\n",
    "\n",
    "print(f\"Reduced train shape: {x_raf_train_pca_gabor.shape}\")\n",
    "print(f\"Reduced test shape: {x_raf_test_pca_gabor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_raf_train_gabor_pca.npy', x_raf_train_pca_gabor)\n",
    "np.save('x_raf_test_gabor_pca.npy', x_raf_test_pca_gabor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for each run (train and test)\n",
    "raf_unsup_gabor_pca_umap_projections_train_10_01 = []\n",
    "raf_unsup_gabor_pca_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times for the training set\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running UMAP on Training Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, random_state=run)\n",
    "    \n",
    "    # Fit and transform the training data\n",
    "    projection_train = umap_model.fit_transform(x_raf_train_pca_gabor)\n",
    "    raf_unsup_gabor_pca_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the same fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_raf_test_pca_gabor)\n",
    "    raf_unsup_gabor_pca_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_unsup_gabor_pca_umap_projections_train_10_01 = np.array(raf_unsup_gabor_pca_umap_projections_train_10_01)\n",
    "raf_unsup_gabor_pca_umap_projections_test_10_01 = np.array(raf_unsup_gabor_pca_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_unsup_gabor_pca_umap_projection_train_10_01 = np.mean(raf_unsup_gabor_pca_umap_projections_train_10_01, axis=0)\n",
    "raf_std_unsup_gabor_pca_umap_projection_train_10_01 = np.std(raf_unsup_gabor_pca_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_unsup_gabor_pca_umap_projection_test_10_01 = np.mean(raf_unsup_gabor_pca_umap_projections_test_10_01, axis=0)\n",
    "raf_std_unsup_gabor_pca_umap_projection_test_10_01 = np.std(raf_unsup_gabor_pca_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_unsup_gabor_pca_umap_projections_train_10_01.npy', raf_unsup_gabor_pca_umap_projections_train_10_01)\n",
    "np.save('raf_mean_unsup_gabor_pca_umap_projection_train_10_01.npy', raf_mean_unsup_gabor_pca_umap_projection_train_10_01)\n",
    "np.save('raf_std_unsup_gabor_pca_umap_projection_train_10_01.npy', raf_std_unsup_gabor_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_unsup_gabor_pca_umap_projections_test_10_01.npy', raf_unsup_gabor_pca_umap_projections_test_10_01)\n",
    "np.save('raf_mean_unsup_gabor_pca_umap_projection_test_10_01.npy', raf_mean_unsup_gabor_pca_umap_projection_test_10_01)\n",
    "np.save('raf_std_unsup_gabor_pca_umap_projection_test_10_01.npy', raf_std_unsup_gabor_pca_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_unsup_gabor_pca_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_unsup_gabor_pca_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Gabor + PCA + Unsupervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_gabor_pca_umap_unsup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_gabor_pca_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_gabor_pca_umap_projection_test_10_01)) # second argument is y_test_pred_gabor_pca\n",
    "print(f\"ARI: {ari_raf_gabor_pca_umap_unsup_10_01:.2f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_gabor_pca_umap_unsup_10_01 = silhouette_score(raf_mean_unsup_gabor_pca_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_unsup_gabor_pca_umap_projection_train_10_01, y_train).predict(raf_mean_unsup_gabor_pca_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_gabor_pca_umap_unsup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_unsup_gabor_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_unsup_gabor_pca_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_unsup_gabor_pca_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_unsup_gabor_pca_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gabor + PCA + UMAP Supervised 10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the projections, mean, and standard deviation for the training set\n",
    "raf_sup_gabor_pca_umap_projections_train_10_01= np.load('raf_sup_gabor_pca_umap_projections_train_10_01.npy')\n",
    "raf_mean_sup_gabor_pca_umap_projection_train_10_01= np.load('raf_mean_sup_gabor_pca_umap_projection_train_10_01.npy')\n",
    "raf_std_sup_gabor_pca_umap_projection_train_10_01= np.load('raf_std_sup_gabor_pca_umap_projection_train_10_01.npy')\n",
    "\n",
    "# load the projections, mean, and standard deviation for the test set\n",
    "raf_sup_gabor_pca_umap_projections_test_10_01= np.load('raf_sup_gabor_pca_umap_projections_test_10_01.npy')\n",
    "raf_mean_sup_gabor_pca_umap_projection_test_10_01= np.load('raf_mean_sup_gabor_pca_umap_projection_test_10_01.npy')\n",
    "raf_std_sup_gabor_pca_umap_projection_test_10_01= np.load('raf_std_sup_gabor_pca_umap_projection_test_10_01.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_neighbors = 10\n",
    "min_dist = 0.1\n",
    "n_components = 2\n",
    "n_runs = 10  # Number of runs\n",
    "\n",
    "# Store UMAP projections for training and test sets\n",
    "raf_sup_gabor_pca_umap_projections_train_10_01 = []\n",
    "raf_sup_gabor_pca_umap_projections_test_10_01 = []\n",
    "\n",
    "# Run UMAP multiple times\n",
    "for run in range(n_runs):\n",
    "    print(f\"Running Supervised UMAP - Iteration {run + 1}/{n_runs}...\")\n",
    "\n",
    "    # Create UMAP model\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, \n",
    "        min_dist=min_dist, \n",
    "        n_components=n_components, \n",
    "        random_state=run\n",
    "    )\n",
    "\n",
    "    # Fit and transform the training data with labels\n",
    "    projection_train = umap_model.fit_transform(x_raf_train_pca_gabor, y_train)\n",
    "    raf_sup_gabor_pca_umap_projections_train_10_01.append(projection_train)\n",
    "    \n",
    "    # Transform the test data using the fitted model\n",
    "    print(f\"Running UMAP on Test Set - Iteration {run + 1}/{n_runs}...\")\n",
    "    projection_test = umap_model.transform(x_raf_test_pca_gabor)\n",
    "    raf_sup_gabor_pca_umap_projections_test_10_01.append(projection_test)\n",
    "\n",
    "# Convert the list of projections to numpy arrays\n",
    "raf_sup_gabor_pca_umap_projections_train_10_01 = np.array(raf_sup_gabor_pca_umap_projections_train_10_01)\n",
    "raf_sup_gabor_pca_umap_projections_test_10_01 = np.array(raf_sup_gabor_pca_umap_projections_test_10_01)\n",
    "\n",
    "# Calculate mean and standard deviation of projections across runs (train and test)\n",
    "raf_mean_sup_gabor_pca_umap_projection_train_10_01 = np.mean(raf_sup_gabor_pca_umap_projections_train_10_01, axis=0)\n",
    "raf_std_sup_gabor_pca_umap_projection_train_10_01 = np.std(raf_sup_gabor_pca_umap_projections_train_10_01, axis=0)\n",
    "\n",
    "raf_mean_sup_gabor_pca_umap_projection_test_10_01 = np.mean(raf_sup_gabor_pca_umap_projections_test_10_01, axis=0)\n",
    "raf_std_sup_gabor_pca_umap_projection_test_10_01 = np.std(raf_sup_gabor_pca_umap_projections_test_10_01, axis=0)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the training set\n",
    "np.save('raf_sup_gabor_pca_umap_projections_train_10_01.npy', raf_sup_gabor_pca_umap_projections_train_10_01)\n",
    "np.save('raf_mean_sup_gabor_pca_umap_projection_train_10_01.npy', raf_mean_sup_gabor_pca_umap_projection_train_10_01)\n",
    "np.save('raf_std_sup_gabor_pca_umap_projection_train_10_01.npy', raf_std_sup_gabor_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Save the projections, mean, and standard deviation for the test set\n",
    "np.save('raf_sup_gabor_pca_umap_projections_test_10_01.npy', raf_sup_gabor_pca_umap_projections_test_10_01)\n",
    "np.save('raf_mean_sup_gabor_pca_umap_projection_test_10_01.npy', raf_mean_sup_gabor_pca_umap_projection_test_10_01)\n",
    "np.save('raf_std_sup_gabor_pca_umap_projection_test_10_01.npy', raf_std_sup_gabor_pca_umap_projection_test_10_01)\n",
    "\n",
    "# Output confirmation\n",
    "print(\"Supervised UMAP projections for training and test sets, mean, and standard deviations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust colormap to have exactly 7 colors\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 7)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    raf_mean_sup_gabor_pca_umap_projection_train_10_01[:, 0],\n",
    "    raf_mean_sup_gabor_pca_umap_projection_train_10_01[:, 1],\n",
    "    c=y_train, cmap=cmap, s=5, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Gabor + PCA + Supervised UMAP Projection of RAFDB Training Data (10 Runs)\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "\n",
    "# Add and configure the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_ticks(range(1, 8))  # Ensure ticks align with labels\n",
    "cbar.set_ticklabels([f\"Emotion {label}\" for label in range(1, 8)])  # Customize labels\n",
    "cbar.set_label(\"Emotion Labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_raf_gabor_pca_umap_sup_10_01 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_gabor_pca_umap_projection_train_10_01, y_train).predict(raf_mean_sup_gabor_pca_umap_projection_test_10_01)) # second argument is y_test_pred_gabor_pca\n",
    "print(f\"ARI: {ari_raf_gabor_pca_umap_sup_10_01:.2f}\")\n",
    "# Silhouette Score\n",
    "silhouette_raf_gabor_pca_umap_sup_10_01 = silhouette_score(raf_mean_sup_gabor_pca_umap_projection_test_10_01, KNeighborsClassifier(n_neighbors=1).fit(raf_mean_sup_gabor_pca_umap_projection_train_10_01, y_train).predict(raf_mean_sup_gabor_pca_umap_projection_test_10_01))\n",
    "print(f\"Silhouette Score: {silhouette_raf_gabor_pca_umap_sup_10_01:.2f}\")\n",
    "\n",
    "# Use KMeans for clustering\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit KMeans on the training set UMAP projections\n",
    "predicted_labels = kmeans.fit_predict(raf_mean_sup_gabor_pca_umap_projection_train_10_01)\n",
    "\n",
    "# Compute Davies-Bouldin Index\n",
    "raf_sup_gabor_pca_10_01_db_score = davies_bouldin_score(\n",
    "    raf_mean_sup_gabor_pca_umap_projection_train_10_01,  # Training set mean projections\n",
    "    predicted_labels  # Labels from KMeans clustering\n",
    ")\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {raf_sup_gabor_pca_10_01_db_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring using Supervised UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42, metric=\"euclidean\")\n",
    "x_train_pca_gabor_supumap = reducer.fit_transform(x_train_pca_gabor, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_train_pca_gabor_supumap.npy',x_train_pca_gabor_supumap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training set UMAP embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_train_pca_gabor_supumap[:, 0], x_train_pca_gabor_supumap[:, 1], c=y_train, cmap='Spectral', s=5)\n",
    "plt.colorbar()\n",
    "plt.title(\"Supervised UMAP Clustering of Training Data\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Compute the Silhouette Score\n",
    "sil_score = silhouette_score(x_train_pca_gabor_supumap, y_train)\n",
    "print(f\"Silhouette Score: {sil_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying UMAP + Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# Apply KMeans to the UMAP-reduced data\n",
    "n_clusters = len(np.unique(y_train))  # Number of clusters = number of unique labels\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "predicted_labels = kmeans.fit_predict(x_train_pca_gabor_supumap)\n",
    "\n",
    "# Calculate Silhouette Score\n",
    "sil_score = silhouette_score(x_train_pca_gabor_supumap, predicted_labels)\n",
    "print(f\"Silhouette Score (with KMeans): {sil_score:.2f}\")\n",
    "\n",
    "# Calculate Davies-Bouldin Index\n",
    "db_score = davies_bouldin_score(x_train_pca_gabor_supumap, predicted_labels)\n",
    "print(f\"Davies-Bouldin Index (with KMeans): {db_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari = adjusted_rand_score(y_train, predicted_labels)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.2f}\")\n",
    "\n",
    "nmi = normalized_mutual_info_score(y_train, predicted_labels)\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create Gabor Kernels (Reused from Training)\n",
    "gabor_kernels = create_gabor_kernels()\n",
    "\n",
    "# Step 2: Apply Gabor Filters to x_test\n",
    "x_test_gabor = apply_gabor_filters(x_test, gabor_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply the Pre-Trained PCA Model\n",
    "x_test_pca_gabor = pca.transform(x_test_gabor)  # Use the already fitted PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Apply the Pre-Trained UMAP Model\n",
    "x_test_umap = reducer.transform(x_test_pca_gabor)  # Use the already fitted UMAP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Predict Clusters with the Pre-Trained KMeans Model\n",
    "y_test_predicted = kmeans.predict(x_test_umap)\n",
    "\n",
    "# Step 6: Evaluate the Results\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n",
    "\n",
    "# Adjusted Rand Index (ARI)\n",
    "ari_score = adjusted_rand_score(y_test, y_test_predicted)\n",
    "print(f\"Adjusted Rand Index (ARI) on Test Data: {ari_score:.2f}\")\n",
    "\n",
    "# Normalized Mutual Information (NMI)\n",
    "nmi_score = normalized_mutual_info_score(y_test, y_test_predicted)\n",
    "print(f\"Normalized Mutual Information (NMI) on Test Data: {nmi_score:.2f}\")\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette = silhouette_score(x_test_umap, y_test_predicted)\n",
    "print(f\"Silhouette Score on Test Data: {silhouette:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Visualize the UMAP Results on Test Data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(\n",
    "    x_test_umap[:, 0], \n",
    "    x_test_umap[:, 1], \n",
    "    c=y_test_predicted, \n",
    "    cmap=\"tab10\", \n",
    "    s=5, \n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title(\"Supervised UMAP Projection of Test Data with Predicted Clusters\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "plt.colorbar(label=\"Predicted Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding SMOTE to the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Gabor features if not already flattened (SMOTE requires 2D input)\n",
    "x_train_flat = x_train_gabor\n",
    "\n",
    "# Apply SMOTE to the Gabor features and labels\n",
    "smote = SMOTE(random_state=42)\n",
    "x_train_smote, y_train_smote = smote.fit_resample(x_train_flat, y_train)\n",
    "\n",
    "print(f\"Original train shape: {x_train_gabor.shape}\")\n",
    "print(f\"SMOTE train shape: {x_train_smote.shape}, {y_train_smote.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data after SMOTE\n",
    "x_train_smote = scaler.fit_transform(x_train_smote)\n",
    "\n",
    "# Apply PCA to the SMOTE-balanced data\n",
    "pca = PCA(n_components=0.85, random_state=42)\n",
    "x_train_pca_smote = pca.fit_transform(x_train_smote)\n",
    "\n",
    "print(f\"Reduced train shape after PCA: {x_train_pca_smote.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP\n",
    "umap_model = umap.UMAP(n_neighbors=50, min_dist=0.1, n_components=2, random_state=42)\n",
    "x_train_umap_smote = umap_model.fit_transform(x_train_pca_smote)\n",
    "\n",
    "print(f\"UMAP reduced train shape: {x_train_umap_smote.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Silhouette Score\n",
    "sil_score = silhouette_score(x_train_umap_smote, y_train_smote)\n",
    "print(f\"Silhouette Score: {sil_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Silhouette Score + ARI + Accuracy + knn cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_components=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pca_c2= np.load(\"x_train_pca_c2.npy\")\n",
    "x_test_pca_c2= np.load(\"x_test_pca_c2.npy\")\n",
    "y_test_pred_pca_c2= np.load(\"y_test_pred_pca_c2.npy\")  # Save SVM predictions\n",
    "cv_scores_pca= np.load(\"cv_scores_pca.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "x_train_pca_c2 = pca.fit_transform(x_train_standardized1)\n",
    "x_test_pca_c2 = pca.transform(x_test_standardized1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pca_c2= np.load(\"x_train_pca_c2.npy\")\n",
    "x_test_pca_c2= np.load(\"x_test_pca_c2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_pca_c2 = np.vstack([x_train_pca_c2, x_test_pca_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_pca_c2 = kmeans.fit_predict(x_full_pca_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_pca_c2 = adjusted_rand_score(y_full, cluster_labels_pca_c2)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_pca_c2}\")\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette_pca_c2 = silhouette_score(x_full_pca_c2, cluster_labels_pca_c2)\n",
    "print(silhouette_pca_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracies_pca_c2 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_pca_c2, y_train)\n",
    "    knn_accuracy = knn.score(x_test_pca_c2, y_test)\n",
    "    knn_accuracies_pca_c2[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_pca_c2, y_train)\n",
    "y_test_pred_pca_c2= svm_clf.predict(x_test_pca_c2)\n",
    "svm_accuracy_pca_c2 = accuracy_score(y_test, y_test_pred_pca_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN from HERE\n",
    "\n",
    "# 10-Fold Cross-Validation Accuracy\n",
    "cv_scores_pca_c2 = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_pca_c2, y_train, cv=10)\n",
    "cv_accuracy_pca_c2 = cv_scores_pca_c2.mean()\n",
    "cv_std_pca_c2 = cv_scores_pca_c2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for PCA\n",
    "results_pca_c2 = {\n",
    "    'ARI': ari_pca_c2,\n",
    "    'Silhouette Score': silhouette_pca_c2,\n",
    "    'SVM Accuracy': svm_accuracy_pca_c2,\n",
    "    'k-NN Accuracy': knn_accuracies_pca_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_pca_c2, cv_std_pca_c2)\n",
    "}\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(results_pca_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_pca_c2.npy\", x_train_pca_c2)  # pca-reduced training data\n",
    "np.save(\"x_test_pca_c2.npy\", x_test_pca_c2)    # pca-reduced test data\n",
    "np.save(\"y_test_pred_pca_c2.npy\", y_test_pred_pca_c2)  # SVM predictions\n",
    "np.save(\"cv_scores_pca_c2.npy\", cv_scores_pca_c2)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracies_pca_c2.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracies_pca_c2, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_pca_c2_serializable = convert_to_serializable({\n",
    "    'ARI': ari_pca_c2,\n",
    "    'Silhouette Score': silhouette_pca_c2,\n",
    "    'SVM Accuracy': svm_accuracy_pca_c2,\n",
    "    'k-NN Accuracy': knn_accuracies_pca_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_pca_c2, cv_std_pca_c2)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"pca_c2_results.json\", \"w\") as file:\n",
    "    json.dump(results_pca_c2_serializable, file, indent=4)\n",
    "\n",
    "print(\"PCA results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_pca_c2 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(x_train_pca_c2, y_train).predict(x_test_pca_c2)) # second argument is y_test_pred_pca\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette_pca_c2 = silhouette_score(x_test_pca_c2, KNeighborsClassifier(n_neighbors=1).fit(x_train_pca_c2, y_train).predict(x_test_pca_c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Silhouette and ARI with Kmeans**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering on the t-SNE embeddings\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(x_train_pca_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "ari_pca_c2_kmeans = adjusted_rand_score(y_train, cluster_labels) # second argument is y_test_pred_pca\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette_pca_c2_kmeans = silhouette_score(x_train_pca_c2,cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_pca_c2, y_train)\n",
    "y_test_pred_pca_c2 = svm_clf.predict(x_test_pca_c2)\n",
    "svm_accuracy_pca = accuracy_score(y_test, y_test_pred_pca_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracies_pca = {}\n",
    "for k in [1, 5, 10]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_pca_c2, y_train)\n",
    "    knn_accuracy = knn.score(x_test_pca_c2, y_test)\n",
    "    knn_accuracies_pca[k] = knn_accuracy\n",
    "\n",
    "# 10-Fold Cross-Validation Accuracy\n",
    "cv_scores_pca = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_pca_c2, y_train, cv=10)\n",
    "cv_accuracy_pca = cv_scores_pca.mean()\n",
    "cv_std_pca = cv_scores_pca.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for PCA\n",
    "results_pca = {\n",
    "    'ARI': ari_pca_c2,\n",
    "    'Silhouette Score': silhouette_pca_c2,\n",
    "    'SVM Accuracy': svm_accuracy_pca,\n",
    "    'k-NN Accuracy': knn_accuracies_pca,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_pca, cv_std_pca)\n",
    "}\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(results_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2D projection with cluster labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=x_train_pca_c2[:, 0], y=x_train_pca_c2[:, 1], hue=y_train, palette='tab10', s=10, legend='full')\n",
    "plt.title(\"2D Scatter Plot of PCA-reduced Data\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save intermediate data (PCA embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_pca_c2.npy\", x_train_pca_c2)\n",
    "np.save(\"x_test_pca_c2.npy\", x_test_pca_c2)\n",
    "np.save(\"y_test_pred_pca_c2.npy\", y_test_pred_pca_c2)  # Save SVM predictions\n",
    "np.save(\"cv_scores_pca.npy\", cv_scores_pca)      # Save cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies\n",
    "with open(\"knn_accuracies_pca.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracies_pca, file, indent=4)\n",
    "\n",
    "# Save PCA Results\n",
    "results_pca_c2 = {\n",
    "    'ARI': ari_pca_c2,\n",
    "    'Silhouette Score': silhouette_pca_c2,\n",
    "    'SVM Accuracy': svm_accuracy_pca,\n",
    "    'k-NN Accuracy': knn_accuracies_pca,\n",
    "    '10-Fold CV Accuracy': {\n",
    "        'Mean': cv_accuracy_pca,\n",
    "        'StdDev': cv_std_pca\n",
    "    },\n",
    "    'Filepaths': {\n",
    "        'x_train_pca_c2': \"x_train_pca_c2.npy\",\n",
    "        'x_test_pca_c2': \"x_test_pca_C2.npy\",\n",
    "        'y_test_pred_pca_c2': \"y_test_pred_pca_c2.npy\",\n",
    "        'cv_scores_pca': \"cv_scores_pca.npy\",\n",
    "        'knn_accuracies_pca': \"knn_accuracies_pca.json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open(\"pca_c2_results.json\", \"w\") as file:\n",
    "    json.dump(results_pca_c2, file, indent=4)\n",
    "\n",
    "print(\"PCA components=2 results and all intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for UMAP\n",
    "results_pca_c2 = {\n",
    "    'ARI': ari_pca_c2,\n",
    "    'Silhouette Score': silhouette_pca_c2,\n",
    "    'SVM Accuracy': svm_accuracy_pca,\n",
    "    'k-NN Accuracy': knn_accuracies_pca,\n",
    "    '10-Fold CV Accuracy': {\n",
    "        'Mean': cv_accuracy_pca,\n",
    "        'StdDev': cv_std_pca\n",
    "}}\n",
    "print(\"PCA Results:\")\n",
    "print(results_pca_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply PCA\n",
    "pca = PCA(0.95)\n",
    "x_train_pca_95 = pca.fit_transform(x_train_standardized1)\n",
    "x_test_pca_95 = pca.transform(x_test_standardized1)\n",
    "\n",
    "np.save(\"x_train_pca_95.npy\", x_train_pca_95)\n",
    "np.save(\"x_test_pca_95.npy\", x_test_pca_95)\n",
    "\n",
    "print(f\"Original number of features: {x_train_standardized1.shape[1]}\")\n",
    "print(f\"Reduced number of features: {x_train_pca_95.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pca_95= np.load(\"x_train_pca_95.npy\")\n",
    "x_test_pca_95= np.load(\"x_test_pca_95.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_pca95 = kmeans.fit_predict(x_test_pca_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_pca_95 = adjusted_rand_score(y_test, cluster_labels_pca95)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_pca_95}\")\n",
    "# ari_pca_95 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(x_train_pca_95, y_train).predict(x_test_pca_95))\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette_pca_95 = silhouette_score(x_test_pca_95, cluster_labels_pca95)\n",
    "print(silhouette_pca_95)\n",
    "# silhouette_pca_95 = silhouette_score(x_test_pca_95, KNeighborsClassifier(n_neighbors=1).fit(x_train_pca_95, y_train).predict(x_test_pca_95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_pca_95, y_train)\n",
    "y_test_pred_pca_95= svm_clf.predict(x_test_pca_95)\n",
    "svm_accuracy_pca = accuracy_score(y_test, y_test_pred_pca_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"svm_accuracy_pca.npy\", svm_accuracy_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_accuracy_pca= np.load('svm_accuracy_pca.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracies_pca = {}\n",
    "for k in [1, 5, 10]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_pca_95, y_train)\n",
    "    knn_accuracy = knn.score(x_test_pca_95, y_test)\n",
    "    knn_accuracies_pca[k] = knn_accuracy\n",
    "\n",
    "# for k in [100, 200, 400]:\n",
    "# 'k-NN Accuracy': {100: 0.9143, 200: 0.8993, 400: 0.8809}\n",
    "\n",
    "# # 10-Fold Cross-Validation Accuracy\n",
    "# cv_scores_pca = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_pca_95, y_train, cv=10)\n",
    "cv_accuracy_pca = cv_scores_pca.mean()\n",
    "cv_std_pca = cv_scores_pca.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_pca= np.load(\"cv_scores_pca.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for PCA\n",
    "results_pca = {\n",
    "    'ARI': ari_pca_95,\n",
    "    'Silhouette Score': silhouette_pca_95,\n",
    "    'SVM Accuracy': svm_accuracy_pca,\n",
    "    'k-NN Accuracy': knn_accuracies_pca,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_pca, cv_std_pca)\n",
    "}\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(results_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA n_components=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=50)\n",
    "x_train_pca_c50 = pca.fit_transform(x_train_standardized1)\n",
    "x_test_pca_c50 = pca.transform(x_test_standardized1)\n",
    "\n",
    "np.save(\"x_train_pca_c50.npy\", x_train_pca_c50)\n",
    "np.save(\"x_test_pca_c50.npy\", x_test_pca_c50)\n",
    "\n",
    "print(f\"Original number of features: {x_train_standardized1.shape[1]}\")\n",
    "print(f\"Reduced number of features: {x_train_pca_c50.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pca_c50= np.load(\"x_train_pca_c50.npy\")\n",
    "x_test_pca_c50= np.load(\"x_test_pca_c50.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_pca_c50 = np.vstack([x_train_pca_c50, x_test_pca_c50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_pca50 = kmeans.fit_predict(x_full_pca_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_pca_c50 = adjusted_rand_score(y_full, cluster_labels_pca50)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_pca_c50}\")\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette_pca_c50 = silhouette_score(x_full_pca_c50, cluster_labels_pca50)\n",
    "print(silhouette_pca_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracies_pca_c50 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_pca_c50, y_train)\n",
    "    knn_accuracy = knn.score(x_test_pca_c50, y_test)\n",
    "    knn_accuracies_pca_c50[k] = knn_accuracy\n",
    "\n",
    "# for k in [100, 200, 400]:\n",
    "# 'k-NN Accuracy': {100: 0.9143, 200: 0.8993, 400: 0.8809}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_pca_c50, y_train)\n",
    "y_test_pred_pca_c50= svm_clf.predict(x_test_pca_c50)\n",
    "svm_accuracy_pca_c50 = accuracy_score(y_test, y_test_pred_pca_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-Fold Cross-Validation Accuracy\n",
    "cv_scores_pca_c50 = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_pca_c50, y_train, cv=10)\n",
    "cv_accuracy_pca_c50 = cv_scores_pca_c50.mean()\n",
    "cv_std_pca_c50 = cv_scores_pca_c50.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for PCA\n",
    "results_pca_c50 = {\n",
    "    'ARI': ari_pca_c50,\n",
    "    'Silhouette Score': silhouette_pca_c50,\n",
    "    'SVM Accuracy': svm_accuracy_pca_c50,\n",
    "    'k-NN Accuracy': knn_accuracies_pca_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_pca_c50, cv_std_pca_c50)\n",
    "}\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(results_pca_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_pca_c50.npy\", x_train_pca_c50)  # pca-reduced training data\n",
    "np.save(\"x_test_pca_c50.npy\", x_test_pca_c50)    # pca-reduced test data\n",
    "np.save(\"y_test_pred_pca_c50.npy\", y_test_pred_pca_c50)  # SVM predictions\n",
    "np.save(\"cv_scores_pca_c50.npy\", cv_scores_pca_c50)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracies_pca_c50.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracies_pca_c50, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_pca_c50_serializable = convert_to_serializable({\n",
    "    'ARI': ari_pca_c50,\n",
    "    'Silhouette Score': silhouette_pca_c50,\n",
    "    'SVM Accuracy': svm_accuracy_pca_c50,\n",
    "    'k-NN Accuracy': knn_accuracies_pca_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_pca_c50, cv_std_pca_c50)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"pca_c50_results.json\", \"w\") as file:\n",
    "    json.dump(results_pca_c50_serializable, file, indent=4)\n",
    "\n",
    "print(\"PCA results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply PCA\n",
    "pca = PCA(0.95)\n",
    "x_train_pca_95 = pca.fit_transform(x_train_standardized1)\n",
    "x_test_pca_95 = pca.transform(x_test_standardized1)\n",
    "\n",
    "np.save(\"x_train_pca_95.npy\", x_train_pca_95)\n",
    "np.save(\"x_test_pca_95.npy\", x_test_pca_95)\n",
    "\n",
    "print(f\"Original number of features: {x_train_standardized1.shape[1]}\")\n",
    "print(f\"Reduced number of features: {x_train_pca_95.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pca_95= np.load(\"x_train_pca_95.npy\")\n",
    "x_test_pca_95= np.load(\"x_test_pca_95.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_pca95 = kmeans.fit_predict(x_test_pca_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_pca_95 = adjusted_rand_score(y_test, cluster_labels_pca95)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_pca_95}\")\n",
    "# ari_pca_95 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(x_train_pca_95, y_train).predict(x_test_pca_95))\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette_pca_95 = silhouette_score(x_test_pca_95, cluster_labels_pca95)\n",
    "print(silhouette_pca_95)\n",
    "# silhouette_pca_95 = silhouette_score(x_test_pca_95, KNeighborsClassifier(n_neighbors=1).fit(x_train_pca_95, y_train).predict(x_test_pca_95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_pca_95, y_train)\n",
    "y_test_pred_pca_95= svm_clf.predict(x_test_pca_95)\n",
    "svm_accuracy_pca = accuracy_score(y_test, y_test_pred_pca_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"svm_accuracy_pca.npy\", svm_accuracy_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_accuracy_pca= np.load('svm_accuracy_pca.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracies_pca = {}\n",
    "for k in [1, 5, 10]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_pca_95, y_train)\n",
    "    knn_accuracy = knn.score(x_test_pca_95, y_test)\n",
    "    knn_accuracies_pca[k] = knn_accuracy\n",
    "\n",
    "# for k in [100, 200, 400]:\n",
    "# 'k-NN Accuracy': {100: 0.9143, 200: 0.8993, 400: 0.8809}\n",
    "\n",
    "# # 10-Fold Cross-Validation Accuracy\n",
    "# cv_scores_pca = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_pca_95, y_train, cv=10)\n",
    "cv_accuracy_pca = cv_scores_pca.mean()\n",
    "cv_std_pca = cv_scores_pca.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_pca= np.load(\"cv_scores_pca.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for PCA\n",
    "results_pca = {\n",
    "    'ARI': ari_pca_95,\n",
    "    'Silhouette Score': silhouette_pca_95,\n",
    "    'SVM Accuracy': svm_accuracy_pca,\n",
    "    'k-NN Accuracy': knn_accuracies_pca,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_pca, cv_std_pca)\n",
    "}\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(results_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2D projection with cluster labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=x_train_pca_95[:, 0], y=x_train_pca_95[:, 1], hue=y_train, palette='tab10', s=10, legend='full')\n",
    "plt.title(\"2D Scatter Plot of PCA-reduced Data\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save intermediate data (PCA embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_pca_95.npy\", x_train_pca_95)\n",
    "np.save(\"x_test_pca_95.npy\", x_test_pca_95)\n",
    "np.save(\"y_test_pred_pca_95.npy\", y_test_pred_pca_95)  # Save SVM predictions\n",
    "np.save(\"cv_scores_pca.npy\", cv_scores_pca)      # Save cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies\n",
    "with open(\"knn_accuracies_pca.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracies_pca, file, indent=4)\n",
    "\n",
    "# Save PCA Results\n",
    "results_pca_95 = {\n",
    "    'ARI': ari_pca_95,\n",
    "    'Silhouette Score': silhouette_pca_95,\n",
    "    'SVM Accuracy': svm_accuracy_pca,\n",
    "    'k-NN Accuracy': knn_accuracies_pca,\n",
    "    '10-Fold CV Accuracy': {\n",
    "        'Mean': cv_accuracy_pca,\n",
    "        'StdDev': cv_std_pca\n",
    "    },\n",
    "    'Filepaths': {\n",
    "        'x_train_pca_95': \"x_train_pca_95.npy\",\n",
    "        'x_test_pca_95': \"x_test_pca_95.npy\",\n",
    "        'y_test_pred_pca_95': \"y_test_pred_pca_95.npy\",\n",
    "        'cv_scores_pca': \"cv_scores_pca.npy\",\n",
    "        'knn_accuracies_pca': \"knn_accuracies_pca.json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open(\"pca_95_results.json\", \"w\") as file:\n",
    "    json.dump(results_pca_95, file, indent=4)\n",
    "\n",
    "print(\"PCA 95% results and all intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative of applying Kmeans for ARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_pca95 = kmeans.fit_predict(x_test_pca_95)\n",
    "\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_pca_95 = adjusted_rand_score(y_test, cluster_labels_pca95)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_pca_95}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_pca_95_k = silhouette_score(x_test_pca_95, cluster_labels_pca95)\n",
    "print(silhouette_pca_95_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Std - train - components=2 - perp=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=15)\n",
    "x_train_std_tsne_c2 = tsne.fit_transform(x_train_standardized1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kmeans Clustering evaluations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering on the t-SNE embeddings\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(x_train_std_tsne_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ARI between true labels and cluster assignments\n",
    "ari_tsne_std = adjusted_rand_score(y_train, cluster_labels)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_tsne_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Silhouette Score for the clustering\n",
    "silhouette_tsne_std = silhouette_score(x_train_std_tsne_c2, cluster_labels)\n",
    "print(f\"Silhouette Score: {silhouette_tsne_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_std_tsne_c2, y_train)\n",
    "y_test_pred_tsne = svm_clf.predict(x_test_std_tsne_c2)\n",
    "svm_accuracy_tsne = accuracy_score(y_test, y_test_pred_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-NN Accuracy for varying k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracies_tsne = {}\n",
    "for k in [1, 5, 10]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_tsne_c2, y_train)\n",
    "    knn_accuracy = knn.score(x_test_tsne_c2, y_test)\n",
    "    knn_accuracies_tsne[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 10-Fold Cross-Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-Fold Cross-Validation Accuracy\n",
    "cv_scores_tsne = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_tsne_c2, y_train, cv=10)\n",
    "cv_accuracy_tsne = cv_scores_tsne.mean()\n",
    "cv_std_tsne = cv_scores_tsne.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Std - fullset - n_components=2 - perp=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and testing sets for t-SNE (unsupervised embedding)\n",
    "x_full = np.vstack([x_train_standardized1, x_test_standardized1])  # Combine normalized train and test data\n",
    "y_full = np.hstack([y_train, y_test])  # Combine train and test labels\n",
    "\n",
    "# Apply t-SNE with optimized parameters\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=15)\n",
    "x_full_tsne = tsne.fit_transform(x_full)  # Fit on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_full_tsne.npy', x_full_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means on t-SNE embeddings\n",
    "kmeans_tsne = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits\n",
    "cluster_labels_full_tsne = kmeans.fit_predict(x_full_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering Metrics: ARI and Silhouette Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Adjusted Rand Index (ARI)\n",
    "ari_full_tsne = adjusted_rand_score(y_full, cluster_labels_full_tsne)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_full_tsne}\")\n",
    "\n",
    "# Compute Silhouette Score\n",
    "silhouette_full_tsne = silhouette_score(x_full_tsne, cluster_labels_full_tsne)\n",
    "print(f\"Silhouette Score: {silhouette_full_tsne}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_full_accuracy_tsne = {}\n",
    "for k in [1, 5, 10]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_full_tsne, y_full)\n",
    "    knn_accuracy = knn.score(x_full_tsne, y_full)\n",
    "    knn_accuracies_tsne[k] = knn_accuracy\n",
    "\n",
    "# 100: 0.9269714285714286,\n",
    "#  200: 0.9182714285714285,\n",
    "#  400: 0.9113,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on t-SNE embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_full_tsne, y_full)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm = svm_clf.predict(x_full_tsne)\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_tsne = accuracy_score(y_full, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {svm_accuracy_tsne:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_accuracy_full_tsne=svm_accuracy_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_tsne = cross_val_score(svm_clf, x_full_tsne, y_full, cv=10)\n",
    "cv_accuracy_tsne = cv_scores_tsne.mean()\n",
    "cv_std_tsne_full = cv_scores_tsne.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_tsne:.4f}  {cv_std_tsne:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_std_full_tsne=cv_std_tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for full t-SNE\n",
    "results_full_tsne = {\n",
    "    'ARI': ari_full_tsne,\n",
    "    'Silhouette Score': silhouette_full_tsne,\n",
    "    'SVM Accuracy': svm_accuracy_full_tsne,\n",
    "    'k-NN Accuracy': knn_accuracies_tsne,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_tsne, cv_std_tsne)\n",
    "}\n",
    "\n",
    "print(\"t-SNE Results:\")\n",
    "print(results_full_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to JSON-serializable format\n",
    "results_full_tsne = {\n",
    "    'ARI': float(ari_full_tsne),\n",
    "    'Silhouette Score': float(silhouette_full_tsne),\n",
    "    'SVM Accuracy': float(svm_accuracy_full_tsne),\n",
    "    '10-Fold CV Accuracy': {\n",
    "        'Mean': float(cv_accuracy_full_tsne),\n",
    "        'StdDev': float(cv_std_full_tsne)\n",
    "    },\n",
    "    'Filepaths': {\n",
    "        'x_train_tsne_c2': \"x_train_full_tsne.npy\",\n",
    "        'cv_scores_tsne': \"cv_scores_tsne.npy\",\n",
    "        'knn_accuracies_tsne': \"knn_accuracies_tsne.json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open(\"full_tsne_results.json\", \"w\") as file:\n",
    "    json.dump(results_full_tsne, file, indent=4)\n",
    "\n",
    "print(\"full t-SNE results and all intermediate data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm - train - n_components=2 - perp=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=15)\n",
    "x_train_tsne_c2 = tsne.fit_transform(x_train_normalized)\n",
    "x_test_tsne_c2 = tsne.fit_transform(x_test_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"x_train_tsne_c2.npy\", x_train_tsne_c2)\n",
    "np.save(\"x_test_tsne_c2.npy\", x_test_tsne_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tsne_c2 = np.load('x_train_tsne_c2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering on the t-SNE embeddings\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(x_train_tsne_c2)\n",
    "\n",
    "# Compute ARI between true labels and cluster assignments\n",
    "ari_tsne = adjusted_rand_score(y_train, cluster_labels)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_tsne}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train_tsne_c2, y_train)\n",
    "y_test_pred_tsne = knn.predict(x_test_tsne_c2)\n",
    "ari_tsne_c2 = adjusted_rand_score(y_test, y_test_pred_tsne)\n",
    "print(ari_tsne_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train_tsne_c2, y_train)\n",
    "y_test_pred_tsne = knn.predict(x_test_tsne_c2)\n",
    "ari_tsne_c2 = adjusted_rand_score(y_test, y_test_pred_tsne)\n",
    "\n",
    "# Silhouette Score\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train_tsne_c2, y_train)\n",
    "y_test_pred_tsne = knn.predict(x_test_tsne_c2)\n",
    "silhouette_tsne_c2 = silhouette_score(x_test_tsne_c2, y_test_pred_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_tsne_c2, y_train)\n",
    "y_test_pred_tsne = svm_clf.predict(x_test_tsne_c2)\n",
    "svm_accuracy_tsne = accuracy_score(y_test, y_test_pred_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracies_tsne = {}\n",
    "for k in [1, 5, 10]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_tsne_c2, y_train)\n",
    "    knn_accuracy = knn.score(x_test_tsne_c2, y_test)\n",
    "    knn_accuracies_tsne[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-Fold Cross-Validation Accuracy\n",
    "cv_scores_tsne = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_tsne_c2, y_train, cv=10)\n",
    "cv_accuracy_tsne = cv_scores_tsne.mean()\n",
    "cv_std_tsne = cv_scores_tsne.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for t-SNE\n",
    "results_tsne = {\n",
    "    'ARI': ari_tsne_c2,\n",
    "    'Silhouette Score': silhouette_tsne_c2,\n",
    "    'SVM Accuracy': svm_accuracy_tsne,\n",
    "    'k-NN Accuracy': knn_accuracies_tsne,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_tsne, cv_std_tsne)\n",
    "}\n",
    "\n",
    "print(\"t-SNE Results:\")\n",
    "print(results_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to JSON-serializable format\n",
    "results_tsne_c2 = {\n",
    "    'ARI': float(ari_tsne_c2),\n",
    "    'Silhouette Score': float(silhouette_tsne_c2),\n",
    "    'SVM Accuracy': float(svm_accuracy_tsne),\n",
    "    '10-Fold CV Accuracy': {\n",
    "        'Mean': float(cv_accuracy_tsne),\n",
    "        'StdDev': float(cv_std_tsne)\n",
    "    },\n",
    "    'Filepaths': {\n",
    "        'x_train_tsne_c2': \"x_train_tsne_c2.npy\",\n",
    "        'x_test_tsne_c2': \"x_test_tsne_c2.npy\",\n",
    "        'y_test_pred_tsne_c2': \"y_test_pred_tsne_c2.npy\",\n",
    "        'cv_scores_tsne': \"cv_scores_tsne.npy\",\n",
    "        'knn_accuracies_tsne': \"knn_accuracies_tsne.json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open(\"tsne_c2_results.json\", \"w\") as file:\n",
    "    json.dump(results_tsne_c2, file, indent=4)\n",
    "\n",
    "print(\"tsne components=2 results and all intermediate data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data (tsne embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_tsne_c2.npy\", x_train_tsne_c2)\n",
    "np.save(\"x_test_tsne_c2.npy\", x_test_tsne_c2)\n",
    "np.save(\"y_test_pred_tsne_c2.npy\", y_test_pred_tsne)  # Save SVM predictions\n",
    "np.save(\"cv_scores_tsne.npy\", cv_scores_tsne)      # Save cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tsne_c2=np.load(\"x_train_tsne_c2.npy\")\n",
    "x_test_tsne_c2= np.load(\"x_test_tsne_c2.npy\")\n",
    "y_test_pred_tsne= np.load(\"y_test_pred_tsne_c2.npy\")  # Save SVM predictions\n",
    "cv_scores_tsne= np.load(\"cv_scores_tsne.npy\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE n_components=2 (not possible with 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and testing sets for t-SNE (unsupervised embedding)\n",
    "x_full = np.vstack([x_train_standardized1, x_test_standardized1])  # Combine normalized train and test data\n",
    "y_full = np.hstack([y_train, y_test])  # Combine train and test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=15)\n",
    "x_full_tsne = tsne.fit_transform(x_train_standardized1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_tsne= np.load('x_full_tsne.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the embeddings back into train and test sets\n",
    "x_train_tsne_c2 = x_full_tsne[:x_train_standardized1.shape[0], :]  # Train embeddings\n",
    "x_test_tsne_c2 = x_full_tsne[x_train_standardized1.shape[0]:, :]  # Test embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data (tsne embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_tsne_c2.npy\", x_train_tsne_c2)\n",
    "np.save(\"x_test_tsne_c2.npy\", x_test_tsne_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the TSNE-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_tsne_c2 = kmeans.fit_predict(x_full_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_tsne_c2 = adjusted_rand_score(y_full, cluster_labels_tsne_c2)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_tsne_c2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_tsne_c2 = silhouette_score(x_full_tsne, cluster_labels_tsne_c2)\n",
    "print(silhouette_tsne_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_tsne_c2 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_tsne_c2, y_train)\n",
    "    knn_accuracy = knn.score(x_test_tsne_c2, y_test)\n",
    "    knn_accuracy_tsne_c2[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on umap embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_tsne_c2, y_train)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm_test = svm_clf.predict(x_test_tsne_c2)  # Predict on test embeddings\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_tsne_c2 = accuracy_score(y_test, y_pred_svm_test)\n",
    "print(f\"SVM Accuracy (Test): {svm_accuracy_tsne_c2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_tsne_c2 = cross_val_score(svm_clf, x_train_tsne_c2, y_train, cv=10)\n",
    "cv_accuracy_tsne_c2 = cv_scores_tsne_c2.mean()\n",
    "cv_std_tsne_c2 = cv_scores_tsne_c2.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_tsne_c2:.4f}  {cv_std_tsne_c2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for umap\n",
    "results_tsne_c2 = {\n",
    "    'ARI': ari_tsne_c2,\n",
    "    'Silhouette Score': silhouette_tsne_c2,\n",
    "    'SVM Accuracy': svm_accuracy_tsne_c2,\n",
    "    'k-NN Accuracy': knn_accuracy_tsne_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_tsne_c2, cv_std_tsne_c2)\n",
    "}\n",
    "\n",
    "print(\"umap Results:\")\n",
    "print(results_tsne_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_tsne_c2.npy\", x_train_tsne_c2)  # umap-reduced training data\n",
    "np.save(\"x_test_tsne_c2.npy\", x_test_tsne_c2)    # umap-reduced test data\n",
    "np.save(\"y_test_pred_tsne_c2.npy\", y_pred_svm_test)  # SVM predictions\n",
    "np.save(\"cv_scores_tsne_c2.npy\", cv_scores_tsne_c2)      # Cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_tsne_c2.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_tsne_c2, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_tsne_c2_serializable = convert_to_serializable({\n",
    "    'ARI': ari_tsne_c2,\n",
    "    'Silhouette Score': silhouette_tsne_c2,\n",
    "    'SVM Accuracy': svm_accuracy_tsne_c2,\n",
    "    'k-NN Accuracy': knn_accuracy_tsne_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_tsne_c2, cv_std_tsne_c2)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"tsne_c50_results.json\", \"w\") as file:\n",
    "    json.dump(results_tsne_c2_serializable, file, indent=4)\n",
    "\n",
    "print(\"tsne results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISOMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISOMAP n_components=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_mnist_consistent(x_data, y_labels, sample_fraction=0.35):\n",
    "    \"\"\"\n",
    "    Downsample the dataset consistently, returning indices to ensure\n",
    "    the same points are selected in both spaces.\n",
    "    \"\"\"\n",
    "    sampled_indices = []\n",
    "    unique_labels = np.unique(y_labels)\n",
    "    for label in unique_labels:\n",
    "        # Select indices for the current label\n",
    "        label_indices = np.where(y_labels == label)[0]\n",
    "        # Sample a fraction of points for this label\n",
    "        sampled_indices_label = resample(\n",
    "            label_indices, n_samples=int(len(label_indices) * sample_fraction), replace=False, random_state=42\n",
    "        )\n",
    "        sampled_indices.extend(sampled_indices_label)\n",
    "    return np.array(sampled_indices)\n",
    "\n",
    "# Downsample training data\n",
    "sampled_indices_train = downsample_mnist_consistent(x_train_standardized1, y_train, sample_fraction=0.35)\n",
    "x_train_sampled = x_train_standardized1[sampled_indices_train]\n",
    "y_train_sampled = y_train[sampled_indices_train]\n",
    "\n",
    "# Downsample test data\n",
    "sampled_indices_test = downsample_mnist_consistent(x_test_standardized1, y_test, sample_fraction=0.35)\n",
    "x_test_sampled = x_test_standardized1[sampled_indices_test]\n",
    "y_test_sampled = y_test[sampled_indices_test]\n",
    "\n",
    "print(f\"Training set reduced to {len(x_train_sampled)} samples.\")\n",
    "print(f\"Test set reduced to {len(x_test_sampled)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sampled indices\n",
    "np.save(\"sampled_indices_train.npy\", sampled_indices_train)\n",
    "np.save(\"sampled_indices_test.npy\", sampled_indices_test)\n",
    "\n",
    "# Save the downsampled dataset\n",
    "np.save(\"x_train_sampled.npy\", x_train_sampled)\n",
    "np.save(\"y_train_sampled.npy\", y_train_sampled)\n",
    "np.save(\"x_test_sampled.npy\", x_test_sampled)\n",
    "np.save(\"y_test_sampled.npy\", y_test_sampled)\n",
    "\n",
    "print(\"Downsampling saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sampled indices\n",
    "sampled_indices_train= np.load(\"sampled_indices_train.npy\")\n",
    "sampled_indices_test= np.load(\"sampled_indices_test.npy\")\n",
    "\n",
    "# load the downsampled dataset\n",
    "x_train_sampled= np.load(\"x_train_sampled.npy\")\n",
    "y_train_sampled= np.load(\"y_train_sampled.npy\")\n",
    "x_test_sampled= np.load(\"x_test_sampled.npy\")\n",
    "y_test_sampled= np.load(\"y_test_sampled.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Isomap\n",
    "isomap = Isomap(n_components=2, n_neighbors=15)\n",
    "x_train_isomap_c2 = isomap.fit_transform(x_train_sampled)\n",
    "x_test_isomap_c2 = isomap.transform(x_test_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_train_isomap_c2.npy',x_train_isomap_c2)\n",
    "np.save('x_test_isomap_c2.npy',x_test_isomap_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_isomap_c2 = kmeans.fit_predict(x_train_isomap_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_isomap_c2 = adjusted_rand_score(y_train_sampled, cluster_labels_isomap_c2)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_isomap_c2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_isomap_c2 = silhouette_score(x_train_isomap_c2, cluster_labels_isomap_c2)\n",
    "print(silhouette_isomap_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_isomap_c2 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_isomap_c2, y_train_sampled)\n",
    "    knn_accuracy = knn.score(x_train_isomap_c2, y_train_sampled)\n",
    "    knn_accuracy_isomap_c2[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on Isomap embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_isomap_c2, y_train_sampled)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm = svm_clf.predict(x_train_isomap_c2)\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_isomap_c2 = accuracy_score(y_train_sampled, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {svm_accuracy_isomap_c2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_isomap_c2 = cross_val_score(svm_clf, x_train_isomap_c2, y_train_sampled, cv=10)\n",
    "cv_accuracy_isomap_c2 = cv_scores_isomap_c2.mean()\n",
    "cv_std_isomap_c2 = cv_scores_isomap_c2.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_isomap_c2:.4f}  {cv_std_isomap_c2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for Isomap\n",
    "results_isomap_c2 = {\n",
    "    'ARI': ari_isomap_c2,\n",
    "    'Silhouette Score': silhouette_isomap_c2,\n",
    "    'SVM Accuracy': svm_accuracy_isomap_c2,\n",
    "    'k-NN Accuracy': knn_accuracy_isomap_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_isomap_c2, cv_std_isomap_c2)\n",
    "}\n",
    "\n",
    "print(\"Isomap Results:\")\n",
    "print(results_isomap_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_isomap_c2.npy\", x_train_isomap_c2)  # ISOMAP-reduced training data\n",
    "np.save(\"x_test_isomap_c2.npy\", x_test_isomap_c2)    # ISOMAP-reduced test data\n",
    "np.save(\"y_test_pred_isomap_c2.npy\", y_pred_svm)  # SVM predictions\n",
    "np.save(\"cv_scores_isomap_c2.npy\", cv_scores_isomap_c2)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_isomap_c2.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_isomap_c2, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_isomap_c2_serializable = convert_to_serializable({\n",
    "    'ARI': ari_isomap_c2,\n",
    "    'Silhouette Score': silhouette_isomap_c2,\n",
    "    'SVM Accuracy': svm_accuracy_isomap_c2,\n",
    "    'k-NN Accuracy': knn_accuracy_isomap_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_isomap_c2, cv_std_isomap_c2)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"isomap_c2_results.json\", \"w\") as file:\n",
    "    json.dump(results_isomap_c2_serializable, file, indent=4)\n",
    "\n",
    "print(\"ISOMAP results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train_isomap_c2, y_train_sampled)\n",
    "y_test_pred_isomap = knn.predict(x_test_isomap_c2)\n",
    "ari_isomap_c2 = adjusted_rand_score(y_test_sampled, y_test_pred_isomap)\n",
    "\n",
    "# Silhouette Score\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train_isomap_c2, y_train_sampled)\n",
    "y_test_pred_isomap = knn.predict(x_test_isomap_c2)\n",
    "silhouette_isomap_c2 = silhouette_score(x_test_isomap_c2, y_test_pred_isomap)\n",
    "\n",
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_isomap_c2, y_train_sampled)\n",
    "y_test_pred_isomap = svm_clf.predict(x_test_isomap_c2)\n",
    "svm_accuracy_isomap = accuracy_score(y_test_sampled, y_test_pred_isomap)\n",
    "\n",
    "# k-NN Accuracy for varying k\n",
    "knn_accuracies_isomap = {}\n",
    "for k in [1, 5, 10]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_isomap_c2, y_train_sampled)\n",
    "    knn_accuracy = knn.score(x_test_isomap, y_test_sampled)\n",
    "    knn_accuracies_isomap[k] = knn_accuracy\n",
    "\n",
    "# 10-Fold Cross-Validation Accuracy\n",
    "cv_scores_isomap = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_isomap_c2, y_train_sampled, cv=10)\n",
    "cv_accuracy_isomap = cv_scores_isomap.mean()\n",
    "cv_std_isomap = cv_scores_isomap.std()\n",
    "\n",
    "# Results for Isomap\n",
    "results_isomap = {\n",
    "    'ARI': ari_isomap_c2,\n",
    "    'Silhouette Score': silhouette_isomap_c2,\n",
    "    'SVM Accuracy': svm_accuracy_isomap,\n",
    "    'k-NN Accuracy': knn_accuracies_isomap,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_isomap, cv_std_isomap)\n",
    "}\n",
    "\n",
    "print(\"Isomap Results:\")\n",
    "print(results_isomap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data (tsne embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_isomap_c2.npy\", x_train_isomap_c2)\n",
    "np.save(\"x_test_isomap_c2.npy\", x_test_isomap_c2)\n",
    "np.save(\"y_test_pred_isomap_c2.npy\", y_test_pred_isomap)  # Save SVM predictions\n",
    "np.save(\"cv_scores_isomap.npy\", cv_scores_isomap)      # Save cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_isomap_c2.npy\", x_train_isomap_c2)  # ISOMAP-reduced training data\n",
    "np.save(\"x_test_isomap_c2.npy\", x_test_isomap_c2)    # ISOMAP-reduced test data\n",
    "np.save(\"y_test_pred_isomap_c2.npy\", y_test_pred_isomap)  # SVM predictions\n",
    "np.save(\"cv_scores_isomap_c2.npy\", cv_scores_isomap)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracies_isomap_c2.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracies_isomap, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_isomap_c2_serializable = convert_to_serializable({\n",
    "    'ARI': ari_isomap_c2,\n",
    "    'Silhouette Score': silhouette_isomap_c2,\n",
    "    'SVM Accuracy': svm_accuracy_isomap,\n",
    "    'k-NN Accuracy': knn_accuracies_isomap,\n",
    "    '10-Fold CV Accuracy': {\n",
    "        'Mean': cv_accuracy_isomap,\n",
    "        'StdDev': cv_std_isomap\n",
    "    }\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"isomap_c2_results.json\", \"w\") as file:\n",
    "    json.dump(results_isomap_c2_serializable, file, indent=4)\n",
    "\n",
    "print(\"ISOMAP results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intermediate data\n",
    "x_train_isomap_c2 = np.load(\"x_train_isomap_c2.npy\")\n",
    "x_test_isomap_c2 = np.load(\"x_test_isomap_c2.npy\")\n",
    "y_test_pred_isomap = np.load(\"y_test_pred_isomap_c2.npy\")\n",
    "cv_scores_isomap = np.load(\"cv_scores_isomap_c2.npy\")\n",
    "\n",
    "# Load k-NN accuracies\n",
    "with open(\"knn_accuracies_isomap_c2.json\", \"r\") as file:\n",
    "    knn_accuracies_isomap = json.load(file)\n",
    "\n",
    "# Load results summary\n",
    "with open(\"isomap_c2_results.json\", \"r\") as file:\n",
    "    results_isomap_c2 = json.load(file)\n",
    "\n",
    "print(\"ISOMAP Results Reloaded:\")\n",
    "print(results_isomap_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISOMAP n_component=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Isomap\n",
    "isomap = Isomap(n_components=50, n_neighbors=15)\n",
    "x_train_isomap_c50 = isomap.fit_transform(x_train_sampled)\n",
    "x_test_isomap_c50 = isomap.transform(x_test_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_isomap_c50 = kmeans.fit_predict(x_train_isomap_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_isomap_c50 = adjusted_rand_score(y_train_sampled, cluster_labels_isomap_c50)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_isomap_c50}\")\n",
    "# ari_isomap_c50 = adjusted_rand_score(y_test, KNeighborsClassifier(n_neighbors=1).fit(x_train_isomap_c50, y_train).predict(x_test_isomap_c50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_isomap_c50 = silhouette_score(x_train_isomap_c50, cluster_labels_isomap_c50)\n",
    "print(silhouette_isomap_c50)\n",
    "# silhouette_isomap_c50 = silhouette_score(x_test_isomap_c50, KNeighborsClassifier(n_neighbors=1).fit(x_train_isomap_c50, y_train).predict(x_test_isomap_c50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_isomap_c50 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_isomap_c50, y_train_sampled)\n",
    "    knn_accuracy = knn.score(x_train_isomap_c50, y_train_sampled)\n",
    "    knn_accuracy_isomap_c50[k] = knn_accuracy\n",
    "\n",
    "# {1: 1.0, 5: 0.9598475827577995, 10: 0.9524172422005239}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_accuracy_isomap_c50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on Isomap embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_isomap_c50, y_train_sampled)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm = svm_clf.predict(x_train_isomap_c50)\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_isomap_c50 = accuracy_score(y_train_sampled, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {svm_accuracy_isomap_c50:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_isomap_c50 = cross_val_score(svm_clf, x_train_isomap_c50, y_train_sampled, cv=10)\n",
    "cv_accuracy_isomap_c50 = cv_scores_isomap_c50.mean()\n",
    "cv_std_isomap_c50 = cv_scores_isomap_c50.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_isomap_c50:.4f}  {cv_std_isomap_c50:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for Isomap\n",
    "results_isomap_c50 = {\n",
    "    'ARI': ari_isomap_c50,\n",
    "    'Silhouette Score': silhouette_isomap_c50,\n",
    "    'SVM Accuracy': svm_accuracy_isomap_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_isomap_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_isomap_c50, cv_std_isomap_c50)\n",
    "}\n",
    "\n",
    "print(\"Isomap Results:\")\n",
    "print(results_isomap_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_isomap_c50.npy\", x_train_isomap_c50)  # ISOMAP-reduced training data\n",
    "np.save(\"x_test_isomap_c50.npy\", x_test_isomap_c50)    # ISOMAP-reduced test data\n",
    "np.save(\"y_test_pred_isomap_c50.npy\", y_pred_svm)  # SVM predictions\n",
    "np.save(\"cv_scores_isomap_c50.npy\", cv_scores_isomap_c50)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_isomap_c50.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_isomap_c50, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_isomap_c50_serializable = convert_to_serializable({\n",
    "    'ARI': ari_isomap_c50,\n",
    "    'Silhouette Score': silhouette_isomap_c50,\n",
    "    'SVM Accuracy': svm_accuracy_isomap_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_isomap_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_isomap_c50, cv_std_isomap_c50)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"isomap_c50_results.json\", \"w\") as file:\n",
    "    json.dump(results_isomap_c50_serializable, file, indent=4)\n",
    "\n",
    "print(\"ISOMAP results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train_isomap_c2, y_train_sampled)\n",
    "y_test_pred_isomap = knn.predict(x_test_isomap_c2)\n",
    "ari_isomap_c2 = adjusted_rand_score(y_test_sampled, y_test_pred_isomap)\n",
    "\n",
    "# Silhouette Score\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train_isomap_c2, y_train_sampled)\n",
    "y_test_pred_isomap = knn.predict(x_test_isomap_c2)\n",
    "silhouette_isomap_c2 = silhouette_score(x_test_isomap_c2, y_test_pred_isomap)\n",
    "\n",
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_isomap_c2, y_train_sampled)\n",
    "y_test_pred_isomap = svm_clf.predict(x_test_isomap_c2)\n",
    "svm_accuracy_isomap = accuracy_score(y_test_sampled, y_test_pred_isomap)\n",
    "\n",
    "# k-NN Accuracy for varying k\n",
    "knn_accuracies_isomap = {}\n",
    "for k in [1, 5, 10]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_isomap_c2, y_train_sampled)\n",
    "    knn_accuracy = knn.score(x_test_isomap, y_test_sampled)\n",
    "    knn_accuracies_isomap[k] = knn_accuracy\n",
    "\n",
    "# 10-Fold Cross-Validation Accuracy\n",
    "cv_scores_isomap = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_isomap_c2, y_train_sampled, cv=10)\n",
    "cv_accuracy_isomap = cv_scores_isomap.mean()\n",
    "cv_std_isomap = cv_scores_isomap.std()\n",
    "\n",
    "# Results for Isomap\n",
    "results_isomap = {\n",
    "    'ARI': ari_isomap_c2,\n",
    "    'Silhouette Score': silhouette_isomap_c2,\n",
    "    'SVM Accuracy': svm_accuracy_isomap,\n",
    "    'k-NN Accuracy': knn_accuracies_isomap,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_isomap, cv_std_isomap)\n",
    "}\n",
    "\n",
    "print(\"Isomap Results:\")\n",
    "print(results_isomap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data (tsne embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_isomap_c2.npy\", x_train_isomap_c2)\n",
    "np.save(\"x_test_isomap_c2.npy\", x_test_isomap_c2)\n",
    "np.save(\"y_test_pred_isomap_c2.npy\", y_test_pred_isomap)  # Save SVM predictions\n",
    "np.save(\"cv_scores_isomap.npy\", cv_scores_isomap)      # Save cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_isomap_c2.npy\", x_train_isomap_c2)  # ISOMAP-reduced training data\n",
    "np.save(\"x_test_isomap_c2.npy\", x_test_isomap_c2)    # ISOMAP-reduced test data\n",
    "np.save(\"y_test_pred_isomap_c2.npy\", y_test_pred_isomap)  # SVM predictions\n",
    "np.save(\"cv_scores_isomap_c2.npy\", cv_scores_isomap)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracies_isomap_c2.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracies_isomap, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_isomap_c2_serializable = convert_to_serializable({\n",
    "    'ARI': ari_isomap_c2,\n",
    "    'Silhouette Score': silhouette_isomap_c2,\n",
    "    'SVM Accuracy': svm_accuracy_isomap,\n",
    "    'k-NN Accuracy': knn_accuracies_isomap,\n",
    "    '10-Fold CV Accuracy': {\n",
    "        'Mean': cv_accuracy_isomap,\n",
    "        'StdDev': cv_std_isomap\n",
    "    }\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"isomap_c2_results.json\", \"w\") as file:\n",
    "    json.dump(results_isomap_c2_serializable, file, indent=4)\n",
    "\n",
    "print(\"ISOMAP results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLE n_components=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LLE\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=15, method='standard')\n",
    "x_train_lle_c2 = lle.fit_transform(x_train_sampled)\n",
    "x_test_lle_c2 = lle.transform(x_test_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the LLE-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_lle_c2 = kmeans.fit_predict(x_test_lle_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_lle_c2= np.load('x_test_lle_c2.npy')\n",
    "x_train_lle_c2= np.load('x_train_lle_c2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_lle_c2 = adjusted_rand_score(y_test_sampled, cluster_labels_lle_c2)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_lle_c2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_lle_c2 = silhouette_score(x_test_lle_c2, cluster_labels_lle_c2)\n",
    "print(silhouette_lle_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_lle_c2 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_lle_c2, y_train_sampled)\n",
    "    knn_accuracy = knn.score(x_train_lle_c2, y_train_sampled)\n",
    "    knn_accuracy_lle_c2[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on lle embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_lle_c2, y_train_sampled)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm = svm_clf.predict(x_train_lle_c2)\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_lle_c2 = accuracy_score(y_train_sampled, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {svm_accuracy_lle_c2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_lle_c2 = cross_val_score(svm_clf, x_train_lle_c2, y_train_sampled, cv=10)\n",
    "cv_accuracy_lle_c2 = cv_scores_lle_c2.mean()\n",
    "cv_std_lle_c2 = cv_scores_lle_c2.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_lle_c2:.4f}  {cv_std_lle_c2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for lle\n",
    "results_lle_c2 = {\n",
    "    'ARI': ari_lle_c2,\n",
    "    'Silhouette Score': silhouette_lle_c2,\n",
    "    'SVM Accuracy': svm_accuracy_lle_c2,\n",
    "    'k-NN Accuracy': knn_accuracy_lle_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_lle_c2, cv_std_lle_c2)\n",
    "}\n",
    "\n",
    "print(\"lle Results:\")\n",
    "print(results_lle_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_lle_c2.npy\", x_train_lle_c2)  # lle-reduced training data\n",
    "np.save(\"x_test_lle_c2.npy\", x_test_lle_c2)    # lle-reduced test data\n",
    "np.save(\"y_test_pred_lle_c2.npy\", y_pred_svm)  # SVM predictions\n",
    "np.save(\"cv_scores_lle_c2.npy\", cv_scores_lle_c2)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_lle_c2.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_lle_c2, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_lle_c2_serializable = convert_to_serializable({\n",
    "    'ARI': ari_lle_c2,\n",
    "    'Silhouette Score': silhouette_lle_c2,\n",
    "    'SVM Accuracy': svm_accuracy_lle_c2,\n",
    "    'k-NN Accuracy': knn_accuracy_lle_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_lle_c2, cv_std_lle_c2)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"lle_c2_results.json\", \"w\") as file:\n",
    "    json.dump(results_lle_c2_serializable, file, indent=4)\n",
    "\n",
    "print(\"lle results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train_lle_c2, y_train_sampled)\n",
    "y_test_pred_lle = knn.predict(x_test_lle_c2)\n",
    "ari_lle_c2 = adjusted_rand_score(y_test_sampled, y_test_pred_lle)\n",
    "\n",
    "# Silhouette Score\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train_lle_c2, y_train_sampled)\n",
    "y_test_pred_lle = knn.predict(x_test_lle_c2)\n",
    "silhouette_lle_c2 = silhouette_score(x_test_lle_c2, y_test_pred_lle)\n",
    "\n",
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_lle_c2, y_train_sampled)\n",
    "y_test_pred_lle_c2 = svm_clf.predict(x_test_lle_c2)\n",
    "svm_accuracy_lle_c2 = accuracy_score(y_test_sampled, y_test_pred_lle_c2)\n",
    "\n",
    "# k-NN Accuracy for varying k\n",
    "knn_accuracies_lle_c2 = {}\n",
    "for k in [1, 5, 10]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_lle_c2, y_train_sampled)\n",
    "    knn_accuracy = knn.score(x_test_lle_c2, y_test_sampled)\n",
    "    knn_accuracies_lle_c2[k] = knn_accuracy\n",
    "\n",
    "# 10-Fold Cross-Validation Accuracy\n",
    "cv_scores_lle_c2 = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_lle_c2, y_train_sampled, cv=10)\n",
    "cv_accuracy_lle_c2 = cv_scores_lle_c2.mean()\n",
    "cv_std_lle_c2 = cv_scores_lle_c2.std()\n",
    "\n",
    "# Results for LLE\n",
    "results_lle_c2 = {\n",
    "    'ARI': ari_lle_c2,\n",
    "    'Silhouette Score': silhouette_lle_c2,\n",
    "    'SVM Accuracy': svm_accuracy_lle_c2,\n",
    "    'k-NN Accuracy': knn_accuracies_lle_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_lle_c2, cv_std_lle_c2)\n",
    "}\n",
    "\n",
    "print(\"LLE Results:\")\n",
    "print(results_lle_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save intermediate data (tsne embeddings and other computationally expensive results)\n",
    "# np.save(\"x_train_lle_c2.npy\", x_train_lle_c2)\n",
    "# np.save(\"x_test_lle_c2.npy\", x_test_lle_c2)\n",
    "# np.save(\"y_test_pred_lle_c2.npy\", y_test_pred_lle)  # Save SVM predictions\n",
    "# np.save(\"cv_scores_lle.npy\", cv_scores_lle)      # Save cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intermediate data\n",
    "x_train_lle_c2 = np.load(\"x_train_lle_c2.npy\")\n",
    "x_test_lle_c2 = np.load(\"x_test_lle_c2.npy\")\n",
    "y_test_pred_lle_c2 = np.load(\"y_test_pred_lle_c2.npy\")\n",
    "cv_scores_lle = np.load(\"cv_scores_lle.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intermediate data\n",
    "x_train_sampled = np.load(\"x_train_sampled.npy\")\n",
    "y_train_sampled = np.load(\"y_train_sampled.npy\")\n",
    "x_test_sampled = np.load(\"x_test_sampled.npy\")\n",
    "y_test_sampled = np.load(\"y_test_sampled.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracies_lle_c2.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracies_lle_c2, file, indent=4)\n",
    "\n",
    "# Save results summary to JSON\n",
    "results_lle_c2 = {\n",
    "    'ARI': ari_lle_c2,\n",
    "    'Silhouette Score': silhouette_lle_c2,\n",
    "    'SVM Accuracy': svm_accuracy_lle_c2,\n",
    "    'k-NN Accuracy': knn_accuracies_lle_c2,\n",
    "    '10-Fold CV Accuracy': {\n",
    "        'Mean': cv_accuracy_lle_c2,\n",
    "        'StdDev': cv_std_lle_c2\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"lle_c2_results.json\", \"w\") as file:\n",
    "    json.dump(results_lle_c2, file, indent=4)\n",
    "\n",
    "print(\"LLE results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intermediate data\n",
    "x_train_lle_c2 = np.load(\"x_train_lle_c2.npy\")\n",
    "x_test_lle_c2 = np.load(\"x_test_lle_c2.npy\")\n",
    "y_test_pred_lle_c2 = np.load(\"y_test_pred_lle_c2.npy\")\n",
    "cv_scores_lle = np.load(\"cv_scores_lle.npy\")\n",
    "\n",
    "# Load k-NN accuracies\n",
    "with open(\"knn_accuracies_lle_c2.json\", \"r\") as file:\n",
    "    knn_accuracies_lle_c2 = json.load(file)\n",
    "\n",
    "# Load results summary\n",
    "with open(\"lle_c2_results.json\", \"r\") as file:\n",
    "    lle_c2_results = json.load(file)\n",
    "\n",
    "print(\"LLE Results Reloaded:\")\n",
    "print(lle_c2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLE n_component=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LLE\n",
    "lle = LocallyLinearEmbedding(n_components=50, n_neighbors=15, method='standard')\n",
    "x_train_lle_c50 = lle.fit_transform(x_train_sampled)\n",
    "x_test_lle_c50 = lle.transform(x_test_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_lle_c50= np.load('x_train_lle_c50.npy')\n",
    "x_test_lle_c50= np.load('x_test_lle_c50.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_lle_c50 = np.vstack([x_train_lle_c50, x_test_lle_c50])\n",
    "y_full_lle_c50 = np.hstack([y_train_sampled, y_test_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_lle_c50 = kmeans.fit_predict(x_full_lle_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_lle_c50 = adjusted_rand_score(y_full_lle_c50, cluster_labels_lle_c50)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_lle_c50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_lle_c50 = silhouette_score(x_full_lle_c50, cluster_labels_lle_c50)\n",
    "print(silhouette_lle_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_lle_c50 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_lle_c50, y_train_sampled)\n",
    "    knn_accuracy = knn.score(x_train_lle_c50, y_train_sampled)\n",
    "    knn_accuracy_lle_c50[k] = knn_accuracy\n",
    "\n",
    "# {1: 1.0, 5: 0.9598475827577995, 10: 0.9524172422005239}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_accuracy_lle_c50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on lle embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_lle_c50, y_train_sampled)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm = svm_clf.predict(x_train_lle_c50)\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_lle_c50 = accuracy_score(y_train_sampled, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {svm_accuracy_lle_c50:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_lle_c50 = cross_val_score(svm_clf, x_train_lle_c50, y_train_sampled, cv=10)\n",
    "cv_accuracy_lle_c50 = cv_scores_lle_c50.mean()\n",
    "cv_std_lle_c50 = cv_scores_lle_c50.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_lle_c50:.4f}  {cv_std_lle_c50:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for lle\n",
    "results_lle_c50 = {\n",
    "    'ARI': ari_lle_c50,\n",
    "    'Silhouette Score': silhouette_lle_c50,\n",
    "    'SVM Accuracy': svm_accuracy_lle_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_lle_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_lle_c50, cv_std_lle_c50)\n",
    "}\n",
    "\n",
    "print(\"lle Results:\")\n",
    "print(results_lle_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_lle_c50.npy\", x_train_lle_c50)  # lle-reduced training data\n",
    "np.save(\"x_test_lle_c50.npy\", x_test_lle_c50)    # lle-reduced test data\n",
    "np.save(\"y_test_pred_lle_c50.npy\", y_pred_svm)  # SVM predictions\n",
    "np.save(\"cv_scores_lle_c50.npy\", cv_scores_lle_c50)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_lle_c50.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_lle_c50, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_lle_c50_serializable = convert_to_serializable({\n",
    "    'ARI': ari_lle_c50,\n",
    "    'Silhouette Score': silhouette_lle_c50,\n",
    "    'SVM Accuracy': svm_accuracy_lle_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_lle_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_lle_c50, cv_std_lle_c50)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"lle_c50_results.json\", \"w\") as file:\n",
    "    json.dump(results_lle_c50_serializable, file, indent=4)\n",
    "\n",
    "print(\"lle results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_components=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train_umap_c2, y_train)\n",
    "y_test_pred_umap = knn.predict(x_test_umap_c2)\n",
    "ari_umap_c2 = adjusted_rand_score(y_test, y_test_pred_umap)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train_umap_c2, y_train)\n",
    "y_test_pred_umap = knn.predict(x_test_umap_c2)\n",
    "silhouette_umap_c2 = silhouette_score(x_test_umap_c2, y_test_pred_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from JSON file\n",
    "with open(\"umap_c2_results.json\", \"r\") as file:\n",
    "    results_umap_c2 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_umap_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP\n",
    "umap = UMAP(n_components=2, n_neighbors=15, random_state=42)\n",
    "x_train_umap_c2_std = umap.fit_transform(x_train_standardized1)\n",
    "x_test_umap_c2_std = umap.transform(x_test_standardized1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data (umap embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_umap_c2_std.npy\", x_train_umap_c2_std)\n",
    "np.save(\"x_test_umap_c2_std.npy\", x_test_umap_c2_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_umap_c2_std = np.vstack([x_train_umap_c2_std, x_test_umap_c2_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_umap_c2_std = kmeans.fit_predict(x_full_umap_c2_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_umap_c2_std = adjusted_rand_score(y_full, cluster_labels_umap_c2_std)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_umap_c2_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_umap_c2_std = silhouette_score(x_full_umap_c2_std, cluster_labels_umap_c2_std)\n",
    "print(silhouette_umap_c2_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_umap_c2_std = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_umap_c2_std, y_train)\n",
    "    knn_accuracy = knn.score(x_test_umap_c2_std, y_test)\n",
    "    knn_accuracy_umap_c2_std[k] = knn_accuracy\n",
    "\n",
    "# {1: 1.0, 5: 0.9598475827577995, 10: 0.9524172422005239}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on umap embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_umap_c2_std, y_train)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm_test = svm_clf.predict(x_test_umap_c2_std)  # Predict on test embeddings\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_umap_c2_std = accuracy_score(y_test, y_pred_svm_test)\n",
    "print(f\"SVM Accuracy (Test): {svm_accuracy_umap_c2_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_umap_c2_std = cross_val_score(svm_clf, x_train_umap_c2_std, y_train, cv=10)\n",
    "cv_accuracy_umap_c2_std = cv_scores_umap_c2_std.mean()\n",
    "cv_std_umap_c2_std = cv_scores_umap_c2_std.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_umap_c2_std:.4f}  {cv_std_umap_c2_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for umap\n",
    "results_umap_c2_std = {\n",
    "    'ARI': ari_umap_c2_std,\n",
    "    'Silhouette Score': silhouette_umap_c2_std,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c2_std,\n",
    "    'k-NN Accuracy': knn_accuracy_umap_c2_std,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c2_std, cv_std_umap_c2_std)\n",
    "}\n",
    "\n",
    "print(\"umap Results:\")\n",
    "print(results_umap_c2_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "from umap import UMAP\n",
    "\n",
    "# Apply UMAP\n",
    "umap = UMAP(n_components=2, n_neighbors=15, random_state=42)\n",
    "x_train_umap_c2 = umap.fit_transform(x_train_normalized)\n",
    "x_test_umap_c2 = umap.transform(x_test_normalized)\n",
    "\n",
    "# ARI\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train_umap_c2, y_train)\n",
    "y_test_pred_umap = knn.predict(x_test_umap_c2)\n",
    "ari_umap_c2 = adjusted_rand_score(y_test, y_test_pred_umap)\n",
    "\n",
    "# Silhouette Score\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train_umap_c2, y_train)\n",
    "y_test_pred_umap = knn.predict(x_test_umap_c2)\n",
    "silhouette_umap_c2 = silhouette_score(x_test_umap_c2, y_test_pred_umap)\n",
    "\n",
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_umap_c2, y_train)\n",
    "y_test_pred_umap_c2 = svm_clf.predict(x_test_umap_c2)\n",
    "svm_accuracy_umap_c2 = accuracy_score(y_test, y_test_pred_umap_c2)\n",
    "\n",
    "# k-NN Accuracy for varying k\n",
    "knn_accuracies_umap_c2 = {}\n",
    "for k in [1, 5, 10]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_umap_c2, y_train)\n",
    "    knn_accuracy = knn.score(x_test_umap_c2, y_test)\n",
    "    knn_accuracies_umap_c2[k] = knn_accuracy\n",
    "\n",
    "# 10-Fold Cross-Validation Accuracy\n",
    "cv_scores_umap_c2 = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_umap_c2, y_train, cv=10)\n",
    "cv_accuracy_umap_c2 = cv_scores_umap_c2.mean()\n",
    "cv_std_umap_c2 = cv_scores_umap_c2.std()\n",
    "\n",
    "# Results for UMAP\n",
    "results_umap_c2 = {\n",
    "    'ARI': ari_umap_c2,\n",
    "    'Silhouette Score': silhouette_umap_c2,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c2,\n",
    "    'k-NN Accuracy': knn_accuracies_umap_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c2, cv_std_umap_c2)\n",
    "}\n",
    "\n",
    "print(\"UMAP Results:\")\n",
    "print(results_umap_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data (tsne embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_umap_c2.npy\", x_train_umap_c2)\n",
    "np.save(\"x_test_umap_c2.npy\", x_test_umap_c2)\n",
    "np.save(\"y_test_pred_umap_c2.npy\", y_test_pred_umap)  # Save SVM predictions\n",
    "np.save(\"cv_scores_umap.npy\", cv_scores_umap)      # Save cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intermediate data\n",
    "x_train_umap_c2 = np.load(\"x_train_umap_c2.npy\")\n",
    "x_test_umap_c2 = np.load(\"x_test_umap_c2.npy\")\n",
    "y_test_pred_umap_c2 = np.load(\"y_test_pred_umap_c2.npy\")\n",
    "cv_scores_umap = np.load(\"cv_scores_umap.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # k-NN Accuracy for varying k\n",
    "# knn_accuracies_umap_c2 = {}\n",
    "# for k in [1, 5, 10]:\n",
    "#     knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#     knn.fit(x_train_umap_c2, y_train)\n",
    "#     knn_accuracy = knn.score(x_test_umap_c2, y_test)\n",
    "#     knn_accuracies_umap_c2[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 10-Fold Cross-Validation Accuracy\n",
    "# cv_scores_umap_c2 = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_umap_c2, y_train, cv=10)\n",
    "# cv_accuracy_umap_c2 = cv_scores_umap_c2.mean()\n",
    "# cv_std_umap_c2 = cv_scores_umap_c2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ARI\n",
    "# ari_umap_c2 = adjusted_rand_score(y_train, KNeighborsClassifier(n_neighbors=1).fit(x_train_umap_c2, y_train).predict(x_train_umap_c2))\n",
    "\n",
    "# # Silhouette Score\n",
    "# silhouette_umap_c2 = silhouette_score(x_train_umap_c2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SVM Accuracy\n",
    "# svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "# svm_clf.fit(x_train_umap_c2, y_train)\n",
    "# y_test_pred_umap_c2 = svm_clf.predict(x_test_umap_c2)\n",
    "# svm_accuracy_umap_c2 = accuracy_score(y_test, y_test_pred_umap_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for UMAP\n",
    "results_umap_c2 = {\n",
    "    'ARI': ari_umap_c2,\n",
    "    'Silhouette Score': silhouette_umap_c2,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c2,\n",
    "    'k-NN Accuracy': knn_accuracies_umap_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c2, cv_std_umap_c2)\n",
    "}\n",
    "\n",
    "print(\"UMAP Results:\")\n",
    "print(results_umap_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert NumPy types to native Python types\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):  # Handle NumPy floats\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):  # Handle NumPy integers\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_umap_c2_serializable = convert_to_serializable({\n",
    "    'ARI': ari_umap_c2,\n",
    "    'Silhouette Score': silhouette_umap_c2,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c2,\n",
    "    'k-NN Accuracy': knn_accuracies_umap_c2,\n",
    "    '10-Fold CV Accuracy': {\n",
    "        'Mean': cv_accuracy_umap_c2,\n",
    "        'StdDev': cv_std_umap_c2\n",
    "    }\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"umap_c2_results.json\", \"w\") as file:\n",
    "    json.dump(results_umap_c2_serializable, file, indent=4)\n",
    "\n",
    "print(\"UMAP results saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_components=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "from umap import UMAP\n",
    "\n",
    "# Apply UMAP\n",
    "umap = UMAP(n_components=50, n_neighbors=15, random_state=42)\n",
    "x_train_umap_c50 = umap.fit_transform(x_train_standardized1)\n",
    "x_test_umap_c50 = umap.transform(x_test_standardized1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data (umap embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_umap_c50.npy\", x_train_umap_c50)\n",
    "np.save(\"x_test_umap_c50.npy\", x_test_umap_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_umap_c50 = np.vstack([x_train_umap_c50, x_test_umap_c50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full = np.hstack([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_umap_c50 = kmeans.fit_predict(x_full_umap_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_umap_c50 = adjusted_rand_score(y_full, cluster_labels_umap_c50)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_umap_c50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_umap_c50 = silhouette_score(x_full_umap_c50, cluster_labels_umap_c50)\n",
    "print(silhouette_umap_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_umap_c50 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_umap_c50, y_train)\n",
    "    knn_accuracy = knn.score(x_test_umap_c50, y_test)\n",
    "    knn_accuracy_umap_c50[k] = knn_accuracy\n",
    "\n",
    "# {1: 1.0, 5: 0.9598475827577995, 10: 0.9524172422005239}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on umap embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_umap_c50, y_train)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm_test = svm_clf.predict(x_test_umap_c50)  # Predict on test embeddings\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_umap_c50 = accuracy_score(y_test, y_pred_svm_test)\n",
    "print(f\"SVM Accuracy (Test): {svm_accuracy_umap_c50:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_umap_c50= np.load('cv_scores_umap_c50.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_umap_c50 = cross_val_score(svm_clf, x_train_umap_c50, y_train, cv=10)\n",
    "cv_accuracy_umap_c50 = cv_scores_umap_c50.mean()\n",
    "cv_std_umap_c50 = cv_scores_umap_c50.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_umap_c50:.4f}  {cv_std_umap_c50:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for umap\n",
    "results_umap_c50 = {\n",
    "    'ARI': ari_umap_c50,\n",
    "    'Silhouette Score': silhouette_umap_c50,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_umap_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c50, cv_std_umap_c50)\n",
    "}\n",
    "\n",
    "print(\"umap Results:\")\n",
    "print(results_umap_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_umap_c50.npy\", x_train_umap_c50)  # umap-reduced training data\n",
    "np.save(\"x_test_umap_c50.npy\", x_test_umap_c50)    # umap-reduced test data\n",
    "np.save(\"y_test_pred_umap_c50.npy\", y_pred_svm)  # SVM predictions\n",
    "np.save(\"cv_scores_umap_c50.npy\", cv_scores_umap_c50)      # Cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "x_train_umap_c50= np.load(\"x_train_umap_c50.npy\") \n",
    "x_test_umap_c50= np.load(\"x_test_umap_c50.npy\") \n",
    "y_pred_svm= np.load(\"y_test_pred_umap_c50.npy\") \n",
    "cv_scores_umap_c50= np.load(\"cv_scores_umap_c50.npy\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_umap_c50.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_umap_c50, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_umap_c50_serializable = convert_to_serializable({\n",
    "    'ARI': ari_umap_c50,\n",
    "    'Silhouette Score': silhouette_umap_c50,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_umap_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c50, cv_std_umap_c50)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"umap_c50_results.json\", \"w\") as file:\n",
    "    json.dump(results_umap_c50_serializable, file, indent=4)\n",
    "\n",
    "print(\"umap results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from JSON file\n",
    "with open(\"umap_c50_results.json\", \"r\") as file:\n",
    "    results_umap_c50 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_umap_c50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_components=50 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "from umap import UMAP\n",
    "\n",
    "# Apply UMAP\n",
    "umap = UMAP(n_components=50, n_neighbors=15, random_state=42)\n",
    "x_train_umap_c50_norm = umap.fit_transform(x_train_normalized)\n",
    "x_test_umap_c50_norm = umap.transform(x_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data (umap embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_umap_c50_norm.npy\", x_train_umap_c50_norm)\n",
    "np.save(\"x_test_umap_c50_norm.npy\", x_test_umap_c50_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_umap_c50_norm = np.vstack([x_train_umap_c50_norm, x_test_umap_c50_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full = np.hstack([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_umap_c50_norm = kmeans.fit_predict(x_full_umap_c50_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_umap_c50_norm = adjusted_rand_score(y_full, cluster_labels_umap_c50_norm)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_umap_c50_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_umap_c50_norm = silhouette_score(x_full_umap_c50_norm, cluster_labels_umap_c50_norm)\n",
    "print(silhouette_umap_c50_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_umap_c50_norm = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_umap_c50_norm, y_train)\n",
    "    knn_accuracy = knn.score(x_test_umap_c50_norm, y_test)\n",
    "    knn_accuracy_umap_c50_norm[k] = knn_accuracy\n",
    "\n",
    "# {1: 1.0, 5: 0.9598475827577995, 10: 0.9524172422005239}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on umap embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_umap_c50_norm, y_train)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm_test = svm_clf.predict(x_test_umap_c50_norm)  # Predict on test embeddings\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_umap_c50_norm = accuracy_score(y_test, y_pred_svm_test)\n",
    "print(f\"SVM Accuracy (Test): {svm_accuracy_umap_c50_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('cv_scores_umap_c50_norm.npy', cv_scores_umap_c50_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_umap_c50_norm= np.load('cv_scores_umap_c50_norm.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_umap_c50_norm = cross_val_score(svm_clf, x_train_umap_c50_norm, y_train, cv=10)\n",
    "cv_accuracy_umap_c50_norm = cv_scores_umap_c50_norm.mean()\n",
    "cv_std_umap_c50_norm = cv_scores_umap_c50_norm.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_umap_c50_norm:.4f}  {cv_std_umap_c50_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for umap\n",
    "results_umap_c50_norm = {\n",
    "    'ARI': ari_umap_c50_norm,\n",
    "    'Silhouette Score': silhouette_umap_c50_norm,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c50_norm,\n",
    "    'k-NN Accuracy': knn_accuracy_umap_c50_norm,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c50_norm, cv_std_umap_c50_norm)\n",
    "}\n",
    "\n",
    "print(\"umap Results:\")\n",
    "print(results_umap_c50_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_umap_c50_norm.npy\", x_train_umap_c50_norm)  # umap-reduced training data\n",
    "np.save(\"x_test_umap_c50_norm.npy\", x_test_umap_c50_norm)    # umap-reduced test data\n",
    "np.save(\"y_test_pred_umap_c50_norm.npy\", y_pred_svm)  # SVM predictions\n",
    "np.save(\"cv_scores_umap_c50_norm.npy\", cv_scores_umap_c50_norm)      # Cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "x_train_umap_c50_norm= np.load(\"x_train_umap_c50_norm.npy\") \n",
    "x_test_umap_c50_norm= np.load(\"x_test_umap_c50_norm.npy\") \n",
    "y_pred_svm= np.load(\"y_test_pred_umap_c50_norm.npy\") \n",
    "cv_scores_umap_c50_norm= np.load(\"cv_scores_umap_c50_norm.npy\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_umap_c50_norm.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_umap_c50_norm, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_umap_c50_norm_serializable = convert_to_serializable({\n",
    "    'ARI': ari_umap_c50_norm,\n",
    "    'Silhouette Score': silhouette_umap_c50_norm,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c50_norm,\n",
    "    'k-NN Accuracy': knn_accuracy_umap_c50_norm,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c50_norm, cv_std_umap_c50_norm)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"umap_c50_norm_results.json\", \"w\") as file:\n",
    "    json.dump(results_umap_c50_norm_serializable, file, indent=4)\n",
    "\n",
    "print(\"umap results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from JSON file\n",
    "with open(\"umap_c50_norm_results.json\", \"r\") as file:\n",
    "    results_umap_c50_norm = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_umap__c2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Visualize the UMAP Results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_train_umap_c50_norm[:, 0], x_train_umap_c50_norm[:, 1], c=y_train, cmap=\"tab10\", s=5, alpha=0.8)\n",
    "plt.title(\"UMAP Projection of MNIST Dataset\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "plt.colorbar(label=\"MNIST Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP n_components=2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "from umap import UMAP\n",
    "\n",
    "# Apply UMAP\n",
    "umap = UMAP(n_components=2, n_neighbors=15, random_state=42)\n",
    "x_train_umap_c2_norm = umap.fit_transform(x_train_normalized)\n",
    "x_test_umap_c2_norm = umap.transform(x_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data (umap embeddings and other computationally expensive results)\n",
    "np.save(\"x_train_umap_c2_norm.npy\", x_train_umap_c2_norm)\n",
    "np.save(\"x_test_umap_c2_norm.npy\", x_test_umap_c2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full_umap_c2_norm = np.vstack([x_train_umap_c2_norm, x_test_umap_c2_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full = np.hstack([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_umap_c2_norm = kmeans.fit_predict(x_full_umap_c2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_umap_c2_norm = adjusted_rand_score(y_full, cluster_labels_umap_c2_norm)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_umap_c2_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_umap_c2_norm = silhouette_score(x_full_umap_c2_norm, cluster_labels_umap_c2_norm)\n",
    "print(silhouette_umap_c2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracy_umap_c2_norm = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_umap_c2_norm, y_train)\n",
    "    knn_accuracy = knn.score(x_test_umap_c2_norm, y_test)\n",
    "    knn_accuracy_umap_c2_norm[k] = knn_accuracy\n",
    "\n",
    "# {1: 1.0, 5: 0.9598475827577995, 10: 0.9524172422005239}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on umap embeddings\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_umap_c2_norm, y_train)\n",
    "\n",
    "# Predict on the same embeddings\n",
    "y_pred_svm_test = svm_clf.predict(x_test_umap_c2_norm)  # Predict on test embeddings\n",
    "\n",
    "# Compute SVM accuracy\n",
    "svm_accuracy_umap_c2_norm = accuracy_score(y_test, y_pred_svm_test)\n",
    "print(f\"SVM Accuracy (Test): {svm_accuracy_umap_c2_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('cv_scores_umap_c2_norm.npy', cv_scores_umap_c2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores_umap_c2_norm = cross_val_score(svm_clf, x_train_umap_c2_norm, y_train, cv=10)\n",
    "cv_accuracy_umap_c2_norm = cv_scores_umap_c2_norm.mean()\n",
    "cv_std_umap_c2_norm = cv_scores_umap_c2_norm.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_umap_c2_norm:.4f}  {cv_std_umap_c2_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for umap\n",
    "results_umap_c2_norm = {\n",
    "    'ARI': ari_umap_c2_norm,\n",
    "    'Silhouette Score': silhouette_umap_c2_norm,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c2_norm,\n",
    "    'k-NN Accuracy': knn_accuracy_umap_c2_norm,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c2_norm, cv_std_umap_c2_norm)\n",
    "}\n",
    "\n",
    "print(\"umap Results:\")\n",
    "print(results_umap_c2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_umap_c2_norm.npy\", x_train_umap_c2_norm)  # umap-reduced training data\n",
    "np.save(\"x_test_umap_c2_norm.npy\", x_test_umap_c2_norm)    # umap-reduced test data\n",
    "np.save(\"y_test_pred_umap_c2_norm.npy\", y_pred_svm_test)  # SVM predictions\n",
    "np.save(\"cv_scores_umap_c2_norm.npy\", cv_scores_umap_c2_norm)      # Cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "x_train_umap_c2_norm= np.load(\"x_train_umap_c2_norm.npy\") \n",
    "x_test_umap_c2_norm= np.load(\"x_test_umap_c2_norm.npy\") \n",
    "y_pred_svm= np.load(\"y_test_pred_umap_c2_norm.npy\") \n",
    "cv_scores_umap_c2_norm= np.load(\"cv_scores_umap_c2_norm.npy\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_umap_c2_norm.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_umap_c2_norm, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_umap_c2_norm_serializable = convert_to_serializable({\n",
    "    'ARI': ari_umap_c2_norm,\n",
    "    'Silhouette Score': silhouette_umap_c2_norm,\n",
    "    'SVM Accuracy': svm_accuracy_umap_c2_norm,\n",
    "    'k-NN Accuracy': knn_accuracy_umap_c2_norm,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_umap_c2_norm, cv_std_umap_c2_norm)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"umap_c2_norm_results.json\", \"w\") as file:\n",
    "    json.dump(results_umap_c2_norm_serializable, file, indent=4)\n",
    "\n",
    "print(\"umap results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from JSON file\n",
    "with open(\"umap_c2_norm_results.json\", \"r\") as file:\n",
    "    results_umap_c2_norm = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_umap_c2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Visualize the UMAP Results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_train_umap_c2_norm[:, 0], x_train_umap_c2_norm[:, 1], c=y_train, cmap=\"tab10\", s=5, alpha=0.8)\n",
    "plt.title(\"UMAP Projection of MNIST Dataset\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "plt.colorbar(label=\"MNIST Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS n_components= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_mnist_consistent(x_data, y_labels, sample_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Downsample the dataset consistently, returning indices to ensure\n",
    "    the same points are selected in both spaces.\n",
    "    \"\"\"\n",
    "    sampled_indices = []\n",
    "    unique_labels = np.unique(y_labels)\n",
    "    for label in unique_labels:\n",
    "        # Select indices for the current label\n",
    "        label_indices = np.where(y_labels == label)[0]\n",
    "        # Sample a fraction of points for this label\n",
    "        sampled_indices_label = resample(\n",
    "            label_indices, n_samples=int(len(label_indices) * sample_fraction), replace=False, random_state=42\n",
    "        )\n",
    "        sampled_indices.extend(sampled_indices_label)\n",
    "    return np.array(sampled_indices)\n",
    "\n",
    "# Downsample training data\n",
    "sampled_indices_train_mds = downsample_mnist_consistent(x_train_standardized1, y_train, sample_fraction=0.1)\n",
    "x_train_sampled_mds = x_train_standardized1[sampled_indices_train_mds]\n",
    "y_train_sampled_mds = y_train[sampled_indices_train_mds]\n",
    "\n",
    "# Downsample test data\n",
    "sampled_indices_test_mds = downsample_mnist_consistent(x_test_standardized1, y_test, sample_fraction=0.1)\n",
    "x_test_sampled_mds = x_test_standardized1[sampled_indices_test_mds]\n",
    "y_test_sampled_mds = y_test[sampled_indices_test_mds]\n",
    "\n",
    "print(f\"Training set reduced to {len(x_train_sampled_mds)} samples.\")\n",
    "print(f\"Test set reduced to {len(x_test_sampled_mds)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sampled indices\n",
    "np.save(\"sampled_indices_train_mds.npy\", sampled_indices_train_mds)\n",
    "np.save(\"sampled_indices_test_mds.npy\", sampled_indices_test_mds)\n",
    "\n",
    "# Save the downsampled dataset\n",
    "np.save(\"x_train_sampled_mds.npy\", x_train_sampled_mds)\n",
    "np.save(\"y_train_sampled_mds.npy\", y_train_sampled_mds)\n",
    "np.save(\"x_test_sampled_mds.npy\", x_test_sampled_mds)\n",
    "np.save(\"y_test_sampled_mds.npy\", y_test_sampled_mds)\n",
    "\n",
    "print(\"Downsampling saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sampled indices\n",
    "sampled_indices_train_mds= np.load(\"sampled_indices_train_mds.npy\")\n",
    "sampled_indices_test_mds= np.load(\"sampled_indices_test_mds.npy\")\n",
    "\n",
    "# Load downsampled dataset\n",
    "x_train_sampled_mds= np.load(\"x_train_sampled_mds.npy\")\n",
    "y_train_sampled_mds= np.load(\"y_train_sampled_mds.npy\")\n",
    "x_test_sampled_mds= np.load(\"x_test_sampled_mds.npy\")\n",
    "y_test_sampled_mds= np.load(\"y_test_sampled_mds.npy\")\n",
    "\n",
    "# Load \n",
    "x_train_mds_c2= np.load(\"x_train_mds_c2.npy\")\n",
    "x_test_mds_c2= np.load(\"x_test_mds_c2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MDS\n",
    "mds = MDS(n_components=2, random_state=42, n_jobs=-1)\n",
    "x_train_mds_c2 = mds.fit_transform(x_train_sampled_mds)\n",
    "x_test_mds_c2 = mds.fit_transform(x_test_sampled_mds)  # MDS needs to be run separately for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the downsampled dataset\n",
    "np.save(\"x_train_mds_c2.npy\", x_train_mds_c2)\n",
    "np.save(\"x_test_mds_c2.npy\", x_test_mds_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train_mds_c2, y_train_sampled_mds)\n",
    "y_test_pred_mds = knn.predict(x_test_mds_c2)\n",
    "ari_mds_c2 = adjusted_rand_score(y_test_sampled_mds, y_test_pred_mds)\n",
    "\n",
    "# Silhouette Score\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train_mds_c2, y_train_sampled_mds)\n",
    "y_test_pred_mds = knn.predict(x_test_mds_c2)\n",
    "silhouette_mds_c2 = silhouette_score(x_test_mds_c2, y_test_pred_mds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Accuracy\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_mds_c2, y_train_sampled_mds)\n",
    "y_test_pred_mds_c2 = svm_clf.predict(x_test_mds_c2)\n",
    "svm_accuracy_mds_c2 = accuracy_score(y_test_sampled_mds, y_test_pred_mds_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Accuracy for varying k\n",
    "knn_accuracies_mds_c2 = {}\n",
    "for k in [1, 5, 10]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_mds_c2, y_train_sampled_mds)\n",
    "    knn_accuracy = knn.score(x_test_mds_c2, y_test_sampled_mds)\n",
    "    knn_accuracies_mds_c2[k] = knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-Fold Cross-Validation Accuracy\n",
    "cv_scores_mds_c2 = cross_val_score(SVC(kernel='rbf', random_state=42), x_train_mds_c2, y_train_sampled_mds, cv=10)\n",
    "cv_accuracy_mds_c2 = cv_scores_mds_c2.mean()\n",
    "cv_std_mds_c2 = cv_scores_mds_c2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for MDS\n",
    "results_mds_c2 = {\n",
    "    'ARI': ari_mds_c2,\n",
    "    'Silhouette Score': silhouette_mds_c2,\n",
    "    'SVM Accuracy': svm_accuracy_mds_c2,\n",
    "    'k-NN Accuracy': knn_accuracies_mds_c2,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_mds_c2, cv_std_mds_c2)\n",
    "}\n",
    "\n",
    "print(\"MDS Results:\")\n",
    "print(results_mds_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Save intermediate data\n",
    "np.save(\"x_train_mds_c2.npy\", x_train_mds_c2)  # MDS-reduced training data\n",
    "np.save(\"x_test_mds_c2.npy\", x_test_mds_c2)    # MDS-reduced test data\n",
    "np.save(\"y_test_pred_mds_c2.npy\", y_test_pred_mds_c2)  # SVM predictions\n",
    "np.save(\"cv_scores_mds_c2.npy\", cv_scores_mds_c2)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracies_mds_c2.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracies_mds_c2, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_mds_c2_serializable = convert_to_serializable({\n",
    "    'ARI': ari_mds_c2,\n",
    "    'Silhouette Score': silhouette_mds_c2,\n",
    "    'SVM Accuracy': svm_accuracy_mds_c2,\n",
    "    'k-NN Accuracy': knn_accuracies_mds_c2,\n",
    "    '10-Fold CV Accuracy': {\n",
    "        'Mean': cv_accuracy_mds_c2,\n",
    "        'StdDev': cv_std_mds_c2\n",
    "    }\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"mds_c2_results.json\", \"w\") as file:\n",
    "    json.dump(results_mds_c2_serializable, file, indent=4)\n",
    "\n",
    "print(\"MDS results and intermediate data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2D projection with cluster labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=x_train_mds_c2[:, 0], y=x_train_mds_c2[:, 1], hue=y_train_sampled_mds, palette='tab10', s=10, legend='full')\n",
    "plt.title(\"2D Scatter Plot of PCA-reduced Data\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS n_components= 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and test sets\n",
    "x_full_mds = np.vstack([x_train_sampled_mds, x_test_sampled_mds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MDS once\n",
    "mds = MDS(n_components=50, random_state=42, n_jobs=-1)\n",
    "x_full_mds_c50 = mds.fit_transform(x_full_mds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the embeddings back into train and test sets\n",
    "x_train_mds_c50 = x_full_mds_c50[:len(y_train_sampled_mds)]\n",
    "x_test_mds_c50 = x_full_mds_c50[len(y_train_sampled_mds):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_full_mds_c50.npy', x_full_mds_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_train_mds_c50.npy', x_train_mds_c50)\n",
    "np.save('x_test_mds_c50.npy', x_test_mds_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on the PCA-reduced test data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)  # 10 clusters for MNIST digits (0-9)\n",
    "cluster_labels_mds_c50 = kmeans.fit_predict(x_test_mds_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI\n",
    "# Compute ARI between true labels and cluster labels\n",
    "ari_mds_c50 = adjusted_rand_score(y_test_sampled_mds, cluster_labels_mds_c50)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_mds_c50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "silhouette_mds_c50 = silhouette_score(x_test_mds_c50, cluster_labels_mds_c50)\n",
    "print(silhouette_mds_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_accuracy_mds_c50 = {}\n",
    "for k in [100, 200, 400]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train_mds_c50, y_train_sampled_mds)\n",
    "    knn_accuracy = knn.score(x_test_mds_c50, y_test_sampled_mds)\n",
    "    knn_accuracy_mds_c50[k] = knn_accuracy\n",
    "\n",
    "print(f\"k-NN Accuracy: {knn_accuracy_mds_c50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(x_train_mds_c50, y_train_sampled_mds)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_svm = svm_clf.predict(x_test_mds_c50)\n",
    "\n",
    "# Compute accuracy\n",
    "svm_accuracy_mds_c50 = accuracy_score(y_test_sampled_mds, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {svm_accuracy_mds_c50:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 10-fold CV\n",
    "cv_scores_mds_c50 = cross_val_score(svm_clf, x_train_mds_c50, y_train_sampled_mds, cv=10)\n",
    "cv_accuracy_mds_c50 = cv_scores_mds_c50.mean()\n",
    "cv_std_mds_c50 = cv_scores_mds_c50.std()\n",
    "\n",
    "print(f\"10-Fold CV Accuracy (SVM): {cv_accuracy_mds_c50:.4f}  {cv_std_mds_c50:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for mds\n",
    "results_mds_c50 = {\n",
    "    'ARI': ari_mds_c50,\n",
    "    'Silhouette Score': silhouette_mds_c50,\n",
    "    'SVM Accuracy': svm_accuracy_mds_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_mds_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_mds_c50, cv_std_mds_c50)\n",
    "}\n",
    "\n",
    "print(\"mds Results:\")\n",
    "print(results_mds_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate data\n",
    "np.save(\"x_train_mds_c50.npy\", x_train_mds_c50)  # mds-reduced training data\n",
    "np.save(\"x_test_mds_c50.npy\", x_test_mds_c50)    # mds-reduced test data\n",
    "np.save(\"y_test_pred_mds_c50.npy\", y_pred_svm)  # SVM predictions\n",
    "np.save(\"cv_scores_mds_c50.npy\", cv_scores_mds_c50)      # Cross-validation scores\n",
    "\n",
    "# Save k-NN accuracies to JSON\n",
    "with open(\"knn_accuracy_mds_c50.json\", \"w\") as file:\n",
    "    json.dump(knn_accuracy_mds_c50, file, indent=4)\n",
    "\n",
    "# Helper function to convert to JSON-serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "results_mds_c50_serializable = convert_to_serializable({\n",
    "    'ARI': ari_mds_c50,\n",
    "    'Silhouette Score': silhouette_mds_c50,\n",
    "    'SVM Accuracy': svm_accuracy_mds_c50,\n",
    "    'k-NN Accuracy': knn_accuracy_mds_c50,\n",
    "    '10-Fold CV Accuracy': (cv_accuracy_mds_c50, cv_std_mds_c50)\n",
    "})\n",
    "\n",
    "# Save results summary to JSON\n",
    "with open(\"mds_c50_results.json\", \"w\") as file:\n",
    "    json.dump(results_mds_c50_serializable, file, indent=4)\n",
    "\n",
    "print(\"mds results and intermediate data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings and cluster labels\n",
    "x_train_pca_c2 = np.load(\"x_train_pca_c2.npy\")\n",
    "x_test_pca_c2 = np.load(\"x_test_pca_c2.npy\")\n",
    "\n",
    "x_train_tsne_c2 = np.load(\"x_train_tsne_c2.npy\")\n",
    "x_test_tsne_c2 = np.load(\"x_test_tsne_c2.npy\")\n",
    "\n",
    "x_train_isomap_c2 = np.load(\"x_train_isomap_c2.npy\")\n",
    "x_test_isomap_c2 = np.load(\"x_test_isomap_c2.npy\")\n",
    "y_test_sampled = np.load(\"y_test_sampled.npy\")\n",
    "y_train_sampled = np.load(\"y_train_sampled.npy\")\n",
    "\n",
    "x_train_lle_c2 = np.load(\"x_train_lle_c2.npy\")\n",
    "x_test_lle_c2 = np.load(\"x_test_lle_c2.npy\")\n",
    "\n",
    "x_train_umap = np.load('x_train_umap_c2.npy')\n",
    "x_test_umap_c2_norm= np.load('x_test_umap_c2_norm.npy')\n",
    "\n",
    "x_train_mds_c2 = np.load(\"x_train_mds_c2.npy\")\n",
    "x_test_mds_c2 = np.load(\"x_test_mds_c2.npy\")\n",
    "y_train_sampled_mds = np.load(\"y_train_sampled_mds.npy\")\n",
    "y_test_sampled_mds = np.load(\"y_test_sampled_mds.npy\")\n",
    "\n",
    "# Use test embeddings and labels for visualization\n",
    "methods = {\n",
    "    'PCA': (x_test_pca_c2, y_test),\n",
    "    'Isomap': (x_test_isomap_c2, y_test_sampled),\n",
    "    'LLE': (x_test_lle_c2, y_test_sampled),\n",
    "    'MDS': (x_test_mds_c2, y_test_sampled_mds),\n",
    "    't-SNE': (x_test_tsne_c2, y_test),\n",
    "    'UMAP': (x_test_umap_c2_norm, y_test)\n",
    "}\n",
    "\n",
    "# Create a grid of subplots with two columns and three rows\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))  # Two columns, three rows\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)  # Adjust spacing between plots\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define the label names (digits 0-9)\n",
    "label_names = [f\"Digit {i}\" for i in range(10)]\n",
    "\n",
    "for ax, (method, (embedding, labels)) in zip(axes, methods.items()):\n",
    "    scatter = sns.scatterplot(\n",
    "        x=embedding[:, 0], \n",
    "        y=embedding[:, 1], \n",
    "        hue=labels.astype(str),  # Ensure labels are strings\n",
    "        palette='tab10',  # Use a standard tab10 palette for each plot\n",
    "        s=40,  # Larger markers for better visibility\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f'{method} Embeddings', fontsize=14, pad=10, loc='center')  # Larger font size\n",
    "\n",
    "    # Hide x and y axis ticks and labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    # Set equal aspect ratio for symmetry\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Add a legend for each plot\n",
    "    handles, labels = scatter.get_legend_handles_labels()\n",
    "    ax.legend(\n",
    "        handles=handles, labels=label_names, title=\"Cluster\", fontsize=10, loc='upper right',\n",
    "        frameon=True, edgecolor=\"black\"\n",
    "    )\n",
    "\n",
    "# Ensure the layout updates properly\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show all plots\n",
    "plt.show()\n",
    "\n",
    "# Verification: Ensure colors and digits are correctly matched\n",
    "print(\"\\nVerifying colors and digits for each plot:\")\n",
    "for method, (embedding, labels) in methods.items():\n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    \n",
    "    # Generate scatter plot to extract handles and colors\n",
    "    scatter = sns.scatterplot(\n",
    "        x=embedding[:, 0],\n",
    "        y=embedding[:, 1],\n",
    "        hue=labels.astype(str),\n",
    "        palette='tab10',\n",
    "        s=10,\n",
    "        legend=True  # Ensure legend is generated\n",
    "    )\n",
    "    legend = scatter.get_legend()\n",
    "    handles = legend.legendHandles  # Get handles from the legend\n",
    "    plt.close()  # Close the plot since we only need the handles\n",
    "\n",
    "    # Ensure there are exactly 10 handles for digits 0-9\n",
    "    if len(handles) != 10:\n",
    "        print(f\"Error: Expected 10 clusters but got {len(handles)} for {method}.\")\n",
    "        continue\n",
    "\n",
    "    # Check colors for each digit\n",
    "    for digit in range(10):\n",
    "        # Extract color from the plot handle and normalize to (R, G, B)\n",
    "        color_in_plot = tuple(handles[digit].get_facecolor()[0][:3])  # Normalize to a tuple\n",
    "        expected_color = tuple(sns.color_palette('tab10', 10)[digit])  # Also as a tuple\n",
    "        \n",
    "        # Compare RGB components\n",
    "        match = color_in_plot == expected_color\n",
    "        print(\n",
    "            f\"Digit {digit}: Color in plot {color_in_plot} | Expected color {expected_color} | Match: {match}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"x_train_umap_c50.npy\", x_train_umap_c50)\n",
    "np.save(\"x_test_umap_c50.npy\", x_test_umap_c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings and cluster labels\n",
    "x_train_pca_c50 = np.load(\"x_train_pca_c50.npy\")\n",
    "x_test_pca_c50 = np.load(\"x_test_pca_c50.npy\")\n",
    "\n",
    "x_train_tsne_c2 = np.load(\"x_train_tsne_c2.npy\")\n",
    "x_test_tsne_c2 = np.load(\"x_test_tsne_c2.npy\")\n",
    "\n",
    "x_train_isomap_c50 = np.load(\"x_train_isomap_c50.npy\")\n",
    "x_test_isomap_c50 = np.load(\"x_test_isomap_c50.npy\")\n",
    "y_test_sampled = np.load(\"y_test_sampled.npy\")\n",
    "y_train_sampled = np.load(\"y_train_sampled.npy\")\n",
    "\n",
    "x_train_lle_c50 = np.load(\"x_train_lle_c50.npy\")\n",
    "x_test_lle_c50 = np.load(\"x_test_lle_c50.npy\")\n",
    "\n",
    "x_train_umap_c50 = np.load('x_train_umap_c50.npy')\n",
    "x_test_umap_c50 = np.load('x_test_umap_c50.npy')\n",
    "\n",
    "\n",
    "\n",
    "x_train_mds_c50 = np.load(\"x_train_mds_c50.npy\")\n",
    "x_test_mds_c50 = np.load(\"x_test_mds_c50.npy\")\n",
    "y_train_sampled_mds = np.load(\"y_train_sampled_mds.npy\")\n",
    "y_test_sampled_mds = np.load(\"y_test_sampled_mds.npy\")\n",
    "\n",
    "# Use test embeddings and labels for visualization\n",
    "methods = {\n",
    "    'PCA': (x_test_pca_c50, y_test),\n",
    "    'Isomap': (x_test_isomap_c50, y_test_sampled),\n",
    "    'LLE': (x_test_lle_c50, y_test_sampled),\n",
    "    'MDS': (x_test_mds_c50, y_test_sampled_mds),\n",
    "    't-SNE': (x_test_tsne_c2, y_test),\n",
    "    'UMAP': (x_test_umap_c50, y_test)\n",
    "}\n",
    "\n",
    "# Create a grid of subplots with two columns and three rows\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))  # Two columns, three rows\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)  # Adjust spacing between plots\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define the label names (digits 0-9)\n",
    "label_names = [f\"Digit {i}\" for i in range(10)]\n",
    "\n",
    "for ax, (method, (embedding, labels)) in zip(axes, methods.items()):\n",
    "    scatter = sns.scatterplot(\n",
    "        x=embedding[:, 0], \n",
    "        y=embedding[:, 1], \n",
    "        hue=labels.astype(str),  # Ensure labels are strings\n",
    "        palette='tab10',  # Use a standard tab10 palette for each plot\n",
    "        s=40,  # Larger markers for better visibility\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f'{method} Embeddings', fontsize=14, pad=10, loc='center')  # Larger font size\n",
    "\n",
    "    # Hide x and y axis ticks and labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    # Set equal aspect ratio for symmetry\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Add a legend for each plot\n",
    "    handles, labels = scatter.get_legend_handles_labels()\n",
    "    ax.legend(\n",
    "        handles=handles, labels=label_names, title=\"Cluster\", fontsize=10, loc='upper right',\n",
    "        frameon=True, edgecolor=\"black\"\n",
    "    )\n",
    "\n",
    "# Ensure the layout updates properly\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show all plots\n",
    "plt.show()\n",
    "\n",
    "# Verification: Ensure colors and digits are correctly matched\n",
    "print(\"\\nVerifying colors and digits for each plot:\")\n",
    "for method, (embedding, labels) in methods.items():\n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    \n",
    "    # Generate scatter plot to extract handles and colors\n",
    "    scatter = sns.scatterplot(\n",
    "        x=embedding[:, 0],\n",
    "        y=embedding[:, 1],\n",
    "        hue=labels.astype(str),\n",
    "        palette='tab10',\n",
    "        s=10,\n",
    "        legend=True  # Ensure legend is generated\n",
    "    )\n",
    "    legend = scatter.get_legend()\n",
    "    handles = legend.legendHandles  # Get handles from the legend\n",
    "    plt.close()  # Close the plot since we only need the handles\n",
    "\n",
    "    # Ensure there are exactly 10 handles for digits 0-9\n",
    "    if len(handles) != 10:\n",
    "        print(f\"Error: Expected 10 clusters but got {len(handles)} for {method}.\")\n",
    "        continue\n",
    "\n",
    "    # Check colors for each digit\n",
    "    for digit in range(10):\n",
    "        # Extract color from the plot handle and normalize to (R, G, B)\n",
    "        color_in_plot = tuple(handles[digit].get_facecolor()[0][:3])  # Normalize to a tuple\n",
    "        expected_color = tuple(sns.color_palette('tab10', 10)[digit])  # Also as a tuple\n",
    "        \n",
    "        # Compare RGB components\n",
    "        match = color_in_plot == expected_color\n",
    "        print(\n",
    "            f\"Digit {digit}: Color in plot {color_in_plot} | Expected color {expected_color} | Match: {match}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
